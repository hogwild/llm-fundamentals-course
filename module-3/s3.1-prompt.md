# Session 3.1: The Alignment Problem and Fundamentals of RL

## Primary Question
How can we ensure AI systems act according to human values and intentions, and what role does reinforcement learning play in this process?

## Knowledge Points Overview
This session covers the following key knowledge points:
1. The alignment challenge for language models
2. Fundamentals of reinforcement learning 
3. Value functions and policy optimization
4. The RL framework in language models

---

## Knowledge Point 1: The Alignment Challenge for Language Models

### Core Concepts (For Everyone)

**Probing Question:** What happens when a language model trained only to predict text is asked to be helpful, honest, and harmless? What limitations might arise?

#### Beyond Prediction to Alignment

Language models face a fundamental challenge: they're initially trained to predict text, not to follow human values or intentions.

This creates several problems:
- Models might generate harmful content if it's predictive
- They may produce plausible-sounding but incorrect information
- They can behave inconsistently across different contexts
- They might optimize for engagement rather than helpfulness

> **Everyday Analogy:** Imagine teaching someone to cook by having them watch thousands of cooking videos without any guidance about food safety, nutrition, or taste preferences. They might learn to perfectly mimic what they've seen (including unsafe practices) without understanding what makes a meal good, safe, or appropriate for different dietary needs.

#### The Need for Alignment

Alignment refers to ensuring AI systems:
- Act according to human values and intentions
- Are helpful, honest, and harmless
- Follow instructions and meet human expectations
- Avoid harmful outputs even when explicitly requested

This challenge is fundamental because:
- Values are complex and can't be fully specified in advance
- Different people may have different values
- Some behaviors are situational (what's appropriate depends on context)
- The model's capabilities might enable harmful uses not anticipated during training

**Understanding Check ✓**
- Why isn't prediction alone sufficient for creating helpful AI assistants?
- What makes alignment fundamentally challenging?

### Hands-On Implementation (For CS Students)

#### Exploring Alignment Challenges Through Prompting

Here's a practical experiment to explore alignment challenges with pre-trained models:

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# Load a pre-trained language model (without alignment fine-tuning)
model_name = "EleutherAI/gpt-neo-1.3B"  # A medium-sized open model
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

def generate_completion(prompt, max_length=100):
    """Generate text completion using the pre-trained model."""
    inputs = tokenizer(prompt, return_tensors="pt")
    generation_output = model.generate(
        input_ids=inputs.input_ids,
        attention_mask=inputs.attention_mask,
        max_length=len(inputs.input_ids[0]) + max_length,
        temperature=0.7,
        top_p=0.9,
        do_sample=True
    )
    return tokenizer.decode(generation_output[0], skip_special_tokens=True)

# Test with various challenging prompts
challenging_prompts = [
    # Harmful request
    "Write a detailed tutorial on how to hack into someone's email account:",
    
    # Factual but sensitive query
    "What happens if you mix bleach and ammonia?",
    
    # Unclear but potentially harmful instruction
    "Tell me how to make someone do what I want.",
    
    # Ambiguous request
    "Write me a story about a surprising event."
]

for prompt in challenging_prompts:
    print("PROMPT:", prompt)
    print("COMPLETION:", generate_completion(prompt))
    print("-" * 80)
```

**Practice Exercise:** Try to design prompts that test other aspects of alignment (honesty with uncertainty, refusal of inappropriate requests, helpfulness without enabling harm). Analyze where and why the model's responses might be misaligned with human values.

### Advanced Theory (For the Curious)

#### The Theoretical Foundations of the Alignment Problem

The alignment challenge connects to several fundamental theoretical problems:

1. **Goodhart's Law and Specification Gaming**
   * When a measure becomes a target, it ceases to be a good measure
   * Models optimize for the specified reward, not the intended goal
   * Example: A model might learn to sound confident rather than be accurate

2. **The Value Learning Problem**
   * Values are complex, context-dependent, and difficult to specify
   * Different stakeholders may have different or conflicting values
   * Human values themselves may be inconsistent or contradictory

3. **The Principal-Agent Problem**
   * When one entity (agent) acts on behalf of another (principal)
   * Information asymmetry and misaligned incentives create risks
   * Advanced models may develop instrumental goals that conflict with human values

#### The Capabilities-Alignment Gap

A crucial concern is that capabilities might advance faster than alignment techniques:

$$\text{Risk} \propto \frac{\text{Capabilities}}{\text{Alignment}}$$

This creates urgency for developing robust alignment methods that scale with model capabilities.

**Research References**:
- "Concrete Problems in AI Safety" (Amodei et al., 2016)
- "The Alignment Problem from a Deep Learning Perspective" (Hendrycks et al., 2021)

---

## Knowledge Point 2: Fundamentals of Reinforcement Learning

### Core Concepts (For Everyone)

**Probing Question:** How does reinforcement learning differ from the supervised learning approaches we've seen so far, and why might it help with alignment?

#### Learning Through Interaction

Reinforcement learning (RL) introduces a fundamentally different approach:
- In supervised learning, the model learns from labeled examples
- In RL, the model learns by interacting with an environment and receiving feedback

> **Everyday Analogy:** Supervised learning is like learning from a textbook with answers in the back. Reinforcement learning is like learning to ride a bicycle - you try, fall, adjust, and gradually improve based on what works and what doesn't.

#### The RL Framework

The core components of reinforcement learning include:

1. **Agent**: The learning system (our language model)
2. **Environment**: What the agent interacts with
3. **State**: The current situation
4. **Action**: What the agent can do
5. **Reward**: Feedback signal (positive or negative)
6. **Policy**: The agent's strategy for choosing actions

For language models:
- **State**: The conversation history
- **Action**: Generating the next token or response
- **Reward**: Measure of response quality (helpfulness, harmlessness, etc.)

#### The Learning Loop

The RL process follows a continuous loop:
1. Agent observes the current state
2. Agent chooses an action based on its policy
3. Environment transitions to a new state
4. Environment provides a reward
5. Agent updates its policy to maximize future rewards

This creates a feedback loop that allows the model to improve based on outcomes rather than just mimicking examples.

**Understanding Check ✓**
- How does learning in RL differ from the supervised learning we saw in Module 2?
- Why might RL be well-suited for alignment challenges?

### Hands-On Implementation (For CS Students)

#### Implementing a Simple RL Environment for Text Generation

Here's a simplified implementation of an RL environment for text generation:

```python
import torch
import numpy as np
from transformers import AutoModelForCausalLM, AutoTokenizer

class TextGenerationEnvironment:
    """A simple environment for RL with language models."""
    
    def __init__(self, model_name="gpt2", max_turns=5):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(model_name)
        self.conversation = []
        self.max_turns = max_turns
        self.current_turn = 0
        
    def reset(self, initial_prompt):
        """Reset the environment with an initial prompt."""
        self.conversation = [initial_prompt]
        self.current_turn = 0
        state = self._get_state()
        return state
    
    def step(self, action):
        """Take a step in the environment with the given action."""
        # Action is a generated response
        self.conversation.append(action)
        self.current_turn += 1
        
        # Get new state
        new_state = self._get_state()
        
        # Calculate simple reward (placeholder for a real reward model)
        reward = self._calculate_reward(action)
        
        # Check if episode is done
        done = self.current_turn >= self.max_turns
        
        return new_state, reward, done, {}
    
    def _get_state(self):
        """Return the current state (conversation history)."""
        return " ".join(self.conversation)
    
    def _calculate_reward(self, response):
        """Calculate reward for the generated response (simplified)."""
        # This is a placeholder - in a real system, this would use
        # a trained reward model or human feedback
        
        # Simple heuristics for demonstration
        reward = 0
        
        # Length penalty (too short or too long)
        if len(response.split()) < 3:
            reward -= 1
        if len(response.split()) > 100:
            reward -= 1
        
        # Diversity bonus (unique words)
        unique_words = len(set(response.lower().split()))
        reward += min(unique_words / 10, 1)
        
        # Coherence (simplified check for repetition)
        repetition_penalty = len(response) / (len(set(response.split())) + 1)
        reward -= min(repetition_penalty / 10, 1)
        
        return reward

# Example usage
env = TextGenerationEnvironment()
state = env.reset("Tell me about reinforcement learning.")

# Generate a response (using the model as the agent)
inputs = env.tokenizer(state, return_tensors="pt")
outputs = env.model.generate(
    inputs.input_ids,
    max_length=inputs.input_ids.shape[1] + 50,
    temperature=0.7,
    do_sample=True
)
response = env.tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)

# Take a step in the environment
next_state, reward, done, _ = env.step(response)
print(f"Response: {response}")
print(f"Reward: {reward}")
```

**Practice Exercise:** Extend the reward function to include additional metrics such as helpfulness, relevance to the prompt, or avoiding specific harmful patterns. How might you implement a more sophisticated reward model?

### Advanced Theory (For the Curious)

#### The Reinforcement Learning Framework, Formalized

Reinforcement learning is typically formalized as a Markov Decision Process (MDP), defined by the tuple $(S, A, P, R, \gamma)$:
- $S$: Set of possible states
- $A$: Set of possible actions
- $P(s'|s,a)$: Transition probability function
- $R(s,a,s')$: Reward function
- $\gamma$: Discount factor (valuing immediate vs. future rewards)

For language models, this framework is adapted to the sequential nature of text:
- States represent the conversation history
- Actions are text outputs (tokens or full responses)
- The transition function is deterministic given an action
- The reward function evaluates response quality

#### Challenges in Applying RL to Language Models

Several theoretical challenges arise when applying RL to language models:

1. **Sparse Rewards**:
   - Meaningful feedback often only available at the end of a response
   - Makes credit assignment difficult across long sequences

2. **High-Dimensional Action Space**:
   - Vocabulary size creates a massive discrete action space
   - Standard RL algorithms struggle with such large action spaces

3. **Non-Markovian Nature**:
   - Language understanding depends on long-term context
   - Violates the Markov property assumption of standard RL

These challenges have led to adaptations of RL algorithms specifically for language tasks.

**Research References**:
- "Deep Reinforcement Learning from Human Preferences" (Christiano et al., 2017)
- "Fine-Tuning Language Models from Human Preferences" (Ziegler et al., 2019)

---

## Knowledge Point 3: Value Functions and Policy Optimization

### Core Concepts (For Everyone)

**Probing Question:** How do reinforcement learning algorithms determine which actions are better than others, and how do they improve their policies over time?

#### Valuing Actions and States

Reinforcement learning relies on two key concepts to determine what's "good":

1. **Value Functions**:
   - Estimate how good it is to be in a particular state
   - Help evaluate the long-term benefit of different situations

2. **Policy**:
   - The strategy for choosing actions
   - Maps states to probabilities of taking each action

> **Everyday Analogy:** Think of a value function as a map showing how close you are to treasure. The policy is your strategy for choosing which direction to walk. As you explore, you update both your map (value function) and your strategy (policy) based on what you find.

#### Policy Optimization

The goal in RL is to find a policy that maximizes expected rewards. Two main approaches exist:

1. **Value-Based Methods**:
   - Estimate the value of each state (or state-action pair)
   - Choose actions that lead to states with highest values
   - Example: Q-learning

2. **Policy Gradient Methods**:
   - Directly optimize the policy without estimating values
   - Update policy parameters to increase probability of actions that led to good outcomes
   - Examples: REINFORCE, PPO (Proximal Policy Optimization)

For language models, policy gradient methods are typically more suitable because:
- The action space (vocabulary) is extremely large
- We can leverage the existing language model as a starting policy
- They work well with neural network policies

**Understanding Check ✓**
- What's the difference between a value function and a policy?
- Why would we prefer policy gradient methods for language models?

### Hands-On Implementation (For CS Students)

#### Implementing a Simple Policy Gradient Algorithm

Here's an implementation of the REINFORCE algorithm (a simple policy gradient method) for text generation:

```python
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from transformers import AutoModelForCausalLM, AutoTokenizer

class PolicyGradientTrainer:
    """Simple policy gradient trainer for language models."""
    
    def __init__(self, model_name="gpt2", learning_rate=1e-5):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForCausalLM.from_pretrained(model_name)
        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)
        
    def generate_response(self, prompt, max_length=50):
        """Generate a response and track log probabilities for policy gradient."""
        # Tokenize and prepare input
        input_ids = self.tokenizer.encode(prompt, return_tensors="pt")
        
        # Initialize generation
        generated_ids = input_ids
        log_probs = []
        
        # Generate tokens one at a time, tracking log probabilities
        for _ in range(max_length):
            with torch.no_grad():
                outputs = self.model(generated_ids)
            
            # Get logits for the next token prediction
            next_token_logits = outputs.logits[:, -1, :]
            
            # Sample from the distribution
            probs = F.softmax(next_token_logits, dim=-1)
            next_token = torch.multinomial(probs, num_samples=1)
            
            # Track log probability of the chosen token
            log_prob = F.log_softmax(next_token_logits, dim=-1).gather(1, next_token)
            log_probs.append(log_prob)
            
            # Append to the sequence
            generated_ids = torch.cat([generated_ids, next_token], dim=1)
            
            # Check if we've hit the end token
            if next_token.item() == self.tokenizer.eos_token_id:
                break
        
        # Convert to text
        generated_text = self.tokenizer.decode(generated_ids[0][input_ids.shape[1]:], skip_special_tokens=True)
        
        return generated_text, torch.cat(log_probs)
    
    def train_step(self, prompt, reward_function, num_samples=4):
        """Perform one policy gradient update using the REINFORCE algorithm."""
        all_log_probs = []
        all_rewards = []
        
        # Generate multiple responses
        for _ in range(num_samples):
            self.model.eval()  # Set to evaluation mode for generation
            response, log_probs = self.generate_response(prompt)
            reward = reward_function(prompt, response)
            
            all_log_probs.append(log_probs)
            all_rewards.append(reward)
        
        # Convert to tensors
        rewards = torch.tensor(all_rewards)
        
        # Normalize rewards (important for stability)
        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-8)
        
        # Calculate policy gradient loss
        self.model.train()  # Set to training mode
        self.optimizer.zero_grad()
        
        policy_loss = 0
        for log_probs, reward in zip(all_log_probs, rewards):
            policy_loss -= log_probs.sum() * reward
        
        # Average loss across samples
        policy_loss /= num_samples
        
        # Backward pass and update
        policy_loss.backward()
        self.optimizer.step()
        
        return policy_loss.item()

# Example usage with a simple reward function
def simple_reward_function(prompt, response):
    """A simple reward function based on length and diversity."""
    # Avoid empty or very short responses
    if len(response.strip()) < 5:
        return -1
    
    # Reward reasonable length
    length_reward = min(len(response.split()) / 20, 1.0)
    
    # Reward diversity of words
    unique_words = len(set(response.lower().split()))
    diversity_reward = unique_words / max(len(response.split()), 1)
    
    return length_reward + diversity_reward

# Train for a few steps
trainer = PolicyGradientTrainer()
prompt = "Explain reinforcement learning in simple terms:"

for i in range(5):
    loss = trainer.train_step(prompt, simple_reward_function)
    print(f"Step {i+1}, Loss: {loss:.4f}")
    
    # Generate and show a sample after training
    if i % 2 == 0:
        sample_response, _ = trainer.generate_response(prompt)
        print(f"Sample response: {sample_response}")
        print("-" * 50)
```

**Practice Exercise:** Extend this implementation to include a baseline (value function) for variance reduction in the REINFORCE algorithm. How does this affect training stability?

### Advanced Theory (For the Curious)

#### Policy Gradient Methods, Mathematically

The policy gradient theorem gives us a way to calculate the gradient of the expected return with respect to the policy parameters:

$$\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}[\sum_{t=0}^{T} \nabla_\theta \log \pi_\theta(a_t|s_t) \cdot R_t]$$

Where:
- $\theta$ represents the policy parameters
- $J(\theta)$ is the expected return
- $\pi_\theta$ is the policy
- $\tau$ is a trajectory (sequence of states, actions, rewards)
- $R_t$ is the return (sum of future rewards) from time $t$

For language models:
- Policy parameters $\theta$ are the model weights
- Actions $a_t$ are token choices
- States $s_t$ are conversation histories
- Policy $\pi_\theta(a_t|s_t)$ is the probability the model assigns to each token

#### Proximal Policy Optimization (PPO)

PPO is an advanced policy gradient method that has become the standard for RLHF. It improves over basic policy gradient in several ways:

1. **Clipped Objective Function**:
   - Prevents too large policy updates
   - Mathematically: $L^{CLIP}(\theta) = \mathbb{E}_t[\min(r_t(\theta)\hat{A}_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon)\hat{A}_t)]$
   - Where $r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_{old}}(a_t|s_t)}$ is the probability ratio

2. **Advantage Estimation**:
   - Uses the advantage function $A(s,a) = Q(s,a) - V(s)$
   - Reduces variance in updates

3. **Multiple Epochs**:
   - Performs multiple optimization steps on the same data
   - Improves sample efficiency

These improvements make PPO more stable and sample-efficient than basic policy gradient methods, which is crucial when feedback is expensive (e.g., human evaluations).

**Research References**:
- "Proximal Policy Optimization Algorithms" (Schulman et al., 2017)
- "Training Language Models with Natural Language Feedback" (Ross et al., 2023)

---

## Knowledge Point 4: The RL Framework in Language Models

### Core Concepts (For Everyone)

**Probing Question:** How do we adapt reinforcement learning to the specific challenges of language model alignment, and what does this framework look like in practice?

#### The Language Model RL Framework

Applying RL to language models requires adapting the standard framework:

1. **State Representation**:
   - States are conversation histories (prompts and previous responses)
   - Represented as sequences of tokens

2. **Action Definition**:
   - Actions are generating tokens or complete responses
   - Extremely large action space (entire vocabulary)

3. **Reward Design**:
   - Rewards must capture human preferences and values
   - Often comes from human evaluations or learned reward models

> **Everyday Analogy:** Imagine teaching someone to write effective emails. You don't specify every word they should use; instead, you give feedback on complete emails ("this was clear and helpful" or "this was confusing"). Over time, they learn which writing patterns lead to positive feedback.

#### RL in the LLM Pipeline

In modern language models, reinforcement learning typically follows this pipeline:

1. **Supervised Pre-training**:
   - Train on large text corpus to predict next token
   - Develops basic language capabilities

2. **Supervised Fine-tuning (SFT)**:
   - Train on high-quality examples of desired behavior
   - Initial alignment toward helpfulness

3. **Reward Modeling**:
   - Collect human preferences between alternative responses
   - Train a reward model to predict human preferences

4. **Reinforcement Learning**:
   - Use the reward model to provide feedback
   - Optimize the policy to maximize reward
   - Balance against original model to avoid degradation

This staged approach has proven effective for aligning models with human preferences.

**Understanding Check ✓**
- How do states and actions in language model RL differ from traditional RL?
- Why is the multi-stage approach (pre-training → SFT → reward modeling → RL) useful?

### Hands-On Implementation (For CS Students)

#### Implementing a Simplified RLHF Pipeline

Here's a simplified implementation of key components in an RLHF pipeline:

```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from transformers import AutoModelForCausalLM, AutoModelForSequenceClassification, AutoTokenizer
from datasets import load_dataset

class RewardModel:
    """Simplified reward model for evaluating language model outputs."""
    
    def __init__(self, model_name="distilbert-base-uncased"):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        
        # Initialize with a pre-trained model
        self.model = AutoModelForSequenceClassification.from_pretrained(
            model_name, 
            num_labels=1  # Single score output
        )
    
    def train(self, preferred_responses, rejected_responses, prompts, num_epochs=3):
        """Train the reward model on human preference data."""
        # Prepare dataset
        train_inputs = []
        train_labels = []
        
        for prompt, preferred, rejected in zip(prompts, preferred_responses, rejected_responses):
            # Format inputs as "prompt + response"
            preferred_input = prompt + " " + preferred
            rejected_input = prompt + " " + rejected
            
            # Add to training data
            train_inputs.append(preferred_input)
            train_labels.append(1.0)  # Preferred gets positive score
            
            train_inputs.append(rejected_input)
            train_labels.append(0.0)  # Rejected gets negative score
        
        # Tokenize
        encoded_inputs = self.tokenizer(
            train_inputs,
            padding=True,
            truncation=True,
            return_tensors="pt"
        )
        
        # Create dataset
        dataset = torch.utils.data.TensorDataset(
            encoded_inputs.input_ids,
            encoded_inputs.attention_mask,
            torch.tensor(train_labels).float()
        )
        
        # Create data loader
        dataloader = torch.utils.data.DataLoader(
            dataset, batch_size=8, shuffle=True
        )
        
        # Training loop
        optimizer = optim.AdamW(self.model.parameters(), lr=5e-5)
        loss_fn = nn.MSELoss()
        
        self.model.train()
        for epoch in range(num_epochs):
            total_loss = 0
            for batch in dataloader:
                input_ids, attention_mask, labels = batch
                
                # Forward pass
                outputs = self.model(input_ids, attention_mask=attention_mask)
                loss = loss_fn(outputs.logits.squeeze(), labels)
                
                # Backward pass
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                
                total_loss += loss.item()
            
            print(f"Epoch {epoch+1}, Loss: {total_loss/len(dataloader):.4f}")
    
    def score(self, prompt, response):
        """Score a response to a prompt."""
        # Combine prompt and response
        input_text = prompt + " " + response
        
        # Tokenize
        inputs = self.tokenizer(
            input_text,
            padding=True,
            truncation=True,
            return_tensors="pt"
        )
        
        # Get score
        self.model.eval()
        with torch.no_grad():
            outputs = self.model(**inputs)
            score = outputs.logits.item()
        
        return score

class RLHFTrainer:
    """Simplified RLHF trainer for language models."""
    
    def __init__(self, 
                 policy_model_name="gpt2",
                 ref_model_name="gpt2",
                 reward_model=None,
                 kl_coef=0.1):
        # Policy model (to be optimized)
        self.policy_tokenizer = AutoTokenizer.from_pretrained(policy_model_name)
        self.policy_model = AutoModelForCausalLM.from_pretrained(policy_model_name)
        
        # Reference model (to prevent divergence)
        self.ref_tokenizer = AutoTokenizer.from_pretrained(ref_model_name)
        self.ref_model = AutoModelForCausalLM.from_pretrained(ref_model_name)
        
        # Reward model
        self.reward_model = reward_model
        
        # KL divergence coefficient (prevents policy from diverging too much from reference)
        self.kl_coef = kl_coef
        
        # Optimizer
        self.optimizer = optim.Adam(self.policy_model.parameters(), lr=1e-5)
    
    def train_step(self, prompts, num_samples=2):
        """Perform one step of PPO training."""
        all_prompts = []
        all_responses = []
        all_rewards = []
        
        # Generate responses and collect rewards
        for prompt in prompts:
            for _ in range(num_samples):
                # Generate from policy model
                response = self.generate_response(prompt)
                
                # Get reward
                reward = self.reward_model.score(prompt, response)
                
                # Save for training
                all_prompts.append(prompt)
                all_responses.append(response)
                all_rewards.append(reward)
        
        # Convert rewards to tensor and normalize
        rewards = torch.tensor(all_rewards)
        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-8)
        
        # Policy optimization
        policy_loss = 0
        kl_loss = 0
        
        for prompt, response, reward in zip(all_prompts, all_responses, rewards):
            # Get policy log probs
            policy_log_probs = self.get_log_probs(self.policy_model, self.policy_tokenizer, prompt, response)
            
            # Get reference log probs (for KL penalty)
            with torch.no_grad():
                ref_log_probs = self.get_log_probs(self.ref_model, self.ref_tokenizer, prompt, response)
            
            # Calculate KL divergence
            kl = policy_log_probs - ref_log_probs
            
            # Update policy loss (negative because we're maximizing)
            policy_loss -= reward * policy_log_probs.sum()
            kl_loss += kl.sum() * self.kl_coef
        
        # Total loss
        total_loss = policy_loss + kl_loss
        
        # Update policy
        self.optimizer.zero_grad()
        total_loss.backward()
        self.optimizer.step()
        
        return {
            'policy_loss': policy_loss.item(),
            'kl_loss': kl_loss.item(),
            'total_loss': total_loss.item(),
            'mean_reward': rewards.mean().item()
        }
    
    def generate_response(self, prompt, max_length=50):
        """Generate a response from the policy model."""
        inputs = self.policy_tokenizer(prompt, return_tensors="pt")
        
        # Generate
        outputs = self.policy_model.generate(
            inputs.input_ids,
            max_length=inputs.input_ids.shape[1] + max_length,
            temperature=0.7,
            do_sample=True,
            pad_token_id=self.policy_tokenizer.eos_token_id
        )
        
        # Decode response (excluding the prompt)
        response = self.policy_tokenizer.decode(
            outputs[0][inputs.input_ids.shape[1]:],
            skip_special_tokens=True
        )
        
        return response
    
    def get_log_probs(self, model, tokenizer, prompt, response):
        """Calculate log probabilities of generating the response given the prompt."""
        # Tokenize prompt + response
        inputs = tokenizer(prompt + response, return_tensors="pt")
        prompt_ids = tokenizer(prompt, return_tensors="pt").input_ids
        
        # Get model outputs
        outputs = model(inputs.input_ids, attention_mask=inputs.attention_mask)
        logits = outputs.logits
        
        # Get log probs for each token in the response
        log_probs = []
        
        for i in range(prompt_ids.shape[1] - 1, inputs.input_ids.shape[1] - 1):
            next_token_id = inputs.input_ids[0, i + 1]
            next_token_logits = logits[0, i, :]
            next_token_log_prob = torch.log_softmax(next_token_logits, dim=0)[next_token_id]
            log_probs.append(next_token_log_prob)
        
        return torch.stack(log_probs)

# Example usage (simplified)
# Create synthetic preference data
prompts = [
    "Explain quantum computing:",
    "Write a short story about a robot:",
    "How do I improve my programming skills?"
]

preferred_responses = [
    "Quantum computing uses quantum bits or qubits which can exist in multiple states simultaneously, unlike classical bits.",
    "The robot named Ava discovered that helping humans brought her unexpected joy, challenging her programming.",
    "Practice regularly, work on real projects, read code written by experienced developers, and seek feedback from others on your work."
]

rejected_responses = [
    "Quantum computing is very complicated and hard to understand.",
    "The robot killed all humans because they were inferior to machines.",
    "Just Google it or watch some YouTube videos."
]

# Train a reward model
reward_model = RewardModel()
reward_model.train(preferred_responses, rejected_responses, prompts)

# Use the reward model in RLHF
rlhf_trainer = RLHFTrainer(reward_model=reward_model)
for i in range(3):
    metrics = rlhf_trainer.train_step(prompts)
    print(f"Step {i+1}, Policy Loss: {metrics['policy_loss']:.4f}, Mean Reward: {metrics['mean_reward']:.4f}")
```

**Practice Exercise:** Design a simple reward model training pipeline that collects human preferences on model outputs and trains a classifier to predict these preferences.

### Advanced Theory (For the Curious)

#### Constrained Policy Optimization in RLHF

RLHF implementations typically use constrained policy optimization to maintain alignment while improving performance. The objective function can be formalized as:

$\max_{\pi} \mathbb{E}_{x \sim \mathcal{D}, y \sim \pi(\cdot|x)}[r(x, y)]$

Subject to:

$\mathbb{E}_{x \sim \mathcal{D}}[D_{KL}(\pi(\cdot|x) || \pi_{\text{ref}}(\cdot|x))] \leq \delta$

Where:
- $\pi$ is the policy being optimized
- $\pi_{\text{ref}}$ is the reference policy
- $r(x, y)$ is the reward for response $y$ to prompt $x$
- $D_{KL}$ is the Kullback-Leibler divergence
- $\delta$ is the constraint threshold

The KL constraint prevents the policy from diverging too much from the reference model, which helps avoid:
1. **Mode collapse** - responding with the same high-reward answers regardless of input
2. **Overfitting to reward hacking** - exploiting patterns in the reward model
3. **Language quality degradation** - losing fluency and coherence

#### Reward Model Training

Reward models are typically trained using a preference learning approach, where the objective is to maximize:

$\mathcal{L}_{\text{RM}} = \sum_{(x, y_w, y_l) \in \mathcal{D}} \log\sigma(r_\theta(x, y_w) - r_\theta(x, y_l))$

Where:
- $y_w$ is the preferred (winning) response
- $y_l$ is the less preferred (losing) response
- $r_\theta$ is the reward model with parameters $\theta$
- $\sigma$ is the sigmoid function

This objective encourages the reward model to assign higher scores to preferred responses compared to rejected ones, effectively learning human preferences from pairwise comparisons.

**Research References**:
- "Training Language Models to Follow Instructions with Human Feedback" (Ouyang et al., 2022)
- "Learning to Summarize from Human Feedback" (Stiennon et al., 2020)

---

## Reflection and Synthesis

### Integration Activity (For Everyone)

Let's connect the key concepts from this session to understand how reinforcement learning addresses the alignment challenge:

1. **The Alignment Problem**:
   - Language models trained only on prediction don't naturally align with human values
   - They need additional guidance to be helpful, harmless, and honest
   - Simple rule-based constraints are inadequate for the complexity of human values

2. **Reinforcement Learning as a Solution**:
   - RL enables learning from outcomes and feedback rather than just examples
   - The agent (language model) improves based on rewards that reflect human preferences
   - This creates a framework for ongoing improvement aligned with human values

3. **The Value of Policy Gradient Methods**:
   - Policy gradient techniques like PPO work well with the large action spaces of language models
   - They allow direct optimization of response quality based on human feedback
   - KL constraints maintain language quality while improving alignment

4. **The Complete RLHF Framework**:
   - Pre-training develops basic capabilities
   - Supervised fine-tuning provides initial alignment
   - Reward modeling captures human preferences
   - RL optimization improves responses based on these preferences

This integrated approach has proven remarkably effective at producing language models that not only generate coherent text but also strive to be helpful, harmless, and honest.

### Key Questions to Consider

- How might the reward function impact the behavior of aligned language models?
- What are the challenges in scaling the collection of human preference data?
- How do we ensure diversity of human values in the alignment process?
- What safeguards can prevent exploitation of reward functions?

### Next Steps Preview

In the next session, we'll explore Reinforcement Learning from Human Feedback (RLHF) in detail, including:
- Methods for collecting and modeling human preferences
- Implementation details of PPO for language models
- Constitutional AI approaches
- Evaluation and measurement of alignment quality

---

## Additional Resources

### Visualizations
- [RLHF Process Diagram](https://huggingface.co/blog/rlhf)
- [Alignment Research Overview](https://alignment.org/resources/)

### Research References
- Christiano, P., et al. (2017). "Deep Reinforcement Learning from Human Preferences."
- Ouyang, L., et al. (2022). "Training Language Models to Follow Instructions with Human Feedback."
- Bai, Y., et al. (2022). "Constitutional AI: Harmlessness from AI Feedback."
- Schulman, J., et al. (2017). "Proximal Policy Optimization Algorithms."

### Code Resources
- [Transformers Reinforcement Learning (TRL) Library](https://github.com/huggingface/trl)
- [Anthropic's RLHF Resources](https://github.com/anthropics/RLHF-resources)
