# Session 2.1: Tokens and Positions in Transformer Models

## Primary Question
How do modern language models efficiently process text and maintain sequence information?

Through this exploration, you'll discover how transformers efficiently handle the complexity of language through tokenization rather than word-level processing, and how they preserve positional information despite parallel processing.

## Knowledge Points Overview
This session covers the following key knowledge points:
1. From words to tokens: Power laws and subword tokenization
2. Position embeddings: Preserving sequence information
3. Combined inputs: Tokens and positions working together

---

## Knowledge Point 1: From Words to Tokens: Power Laws and Subword Tokenization

### Core Concepts (For Everyone)

**Probing Question**: Early language models used whole words as their basic unit, while modern LLMs use "tokens" instead. What do you think might be the limitations of word-based approaches that led to this shift?

#### Power Laws in Language: The Long Tail Problem

Natural language follows power law distributions - a small number of words appear extremely frequently, while most words appear rarely:

- In English, just 100 common words make up about 50% of all text
- The next 1,000 words account for another 25%
- The remaining 25% is spread across hundreds of thousands of rare words

This pattern, known as Zipf's Law, creates a challenge: How do we efficiently represent both common and rare words?

> **Everyday Example**: Think of your own vocabulary. You use words like "the," "and," and "of" constantly, but might use words like "serendipity" or "ephemeral" only occasionally. A dictionary that only includes common words would miss important meaning, but one that includes every possible word would be impractically large.

#### The Word-Level Dilemma

Word-level models face these key challenges:

- **Vocabulary Size Problem**: Choose between a small vocabulary (missing rare words) or an enormous one (inefficient)
- **Unknown Word Problem**: Replace rare words with `<UNK>` tokens, losing information
- **Variant Problem**: Treat related words like "walk," "walks," and "walking" as completely separate
- **New Words Problem**: Cannot handle words created after training (like "COVID")

#### Subword Tokenization: The Elegant Solution

Subword tokenization aligns with language's natural structure by breaking words into meaningful pieces:

- Common words remain as single tokens: "the" → "the"
- Medium-frequency words split into common parts: "walking" → "walk" + "ing"
- Rare words break into smaller units: "tokenization" → "token" + "ization"
- New words handled compositionally: "COVID19" → "C" + "O" + "V" + "ID" + "19"

This approach is efficient for common words while maintaining coverage for rare ones.

#### Byte Pair Encoding (BPE): How It Works

BPE builds a vocabulary through these steps:
1. Start with individual characters
2. Count pairs that appear most frequently
3. Merge the most common pair into a new token
4. Repeat until desired vocabulary size is reached

**Understanding Check ✓**
- How does subword tokenization handle words it has never seen before?
- Why do common words usually remain as single tokens while rare words get split?
- How does this approach address the "long tail" problem in language?

### Hands-On Implementation (For CS Students)

#### Implementing a Simple BPE Tokenizer

Here's a simplified implementation of BPE in PyTorch:

```python
import torch
from collections import Counter

def train_bpe(text, vocab_size=1000):
    # Start with character-level tokens
    tokens = list(text)
    vocab = set(tokens)
    
    # Track merges
    merges = {}
    
    while len(vocab) < vocab_size:
        # Count frequencies of adjacent pairs
        pairs = Counter()
        for i in range(len(tokens) - 1):
            pair = (tokens[i], tokens[i+1])
            pairs[pair] += 1
            
        # Find most frequent pair
        if not pairs:
            break
        best_pair = max(pairs, key=pairs.get)
        
        # Create new token from merge
        new_token = best_pair[0] + best_pair[1]
        merges[best_pair] = new_token
        vocab.add(new_token)
        
        # Apply the merge to the token list
        i = 0
        while i < len(tokens) - 1:
            if (tokens[i], tokens[i+1]) == best_pair:
                tokens[i] = new_token
                tokens.pop(i+1)
            else:
                i += 1
    
    return vocab, merges

# Exercise: Try implementing tokenization using the trained merges
def tokenize(text, merges):
    tokens = list(text)
    i = 0
    while i < len(tokens) - 1:
        pair = (tokens[i], tokens[i+1])
        if pair in merges:
            tokens[i] = merges[pair]
            tokens.pop(i+1)
        else:
            i += 1
    return tokens
```

**Practice Exercise**: Apply this tokenizer to a sample text and analyze which words remain whole and which get split.

### Advanced Theory (For the Curious)

#### Information Theory Perspective

BPE tokenization can be understood through the lens of information theory:

- Each token represents an optimal "chunk" of information
- The algorithm finds a balance between minimizing vocabulary size and minimizing the number of tokens per text
- This approach approximates the Minimum Description Length (MDL) principle

#### Mathematical Analysis of Power Laws in Language

Zipf's Law states that the frequency of a word is inversely proportional to its rank:

$$f(r) \propto \frac{1}{r^α}$$

Where:
- $f(r)$ is the frequency of the word with rank $r$
- $α$ is close to 1 for natural language

This distribution creates a theoretical minimum for the average description length of text, which BPE approaches.

#### Connection to Modern Tokenizers

While BPE is foundational, advanced tokenizers have evolved:
- WordPiece (used by BERT): Similar to BPE but uses likelihood rather than frequency
- SentencePiece: Language-agnostic tokenization that handles spaces as special tokens
- Unigram (used by T5): Probabilistic approach that starts with a large vocabulary and removes tokens

**Research Reference**: Review "Neural Machine Translation of Rare Words with Subword Units" (Sennrich et al., 2016) for the original BPE application to NLP.

---

## Knowledge Point 2: Position Embeddings: Preserving Sequence Information

### Core Concepts (For Everyone)

**Probing Question**: In the models we've explored so far, position information was implicit based on the order of processing. How might a model that processes all tokens simultaneously maintain information about token position?

#### The Sequence Challenge

Word order matters tremendously in language:
- "The dog chased the cat" means something completely different from "The cat chased the dog"
- Without position information, these sentences would look identical to a model

> **Everyday Analogy**: Imagine reading a book where all the words on a page were scattered randomly instead of in order. Even with all the same words, you couldn't understand the story. Position matters!

#### Evolution of Position Information

Let's trace how different models handle position information:

1. **In Bengio's Neural Language Model** (from Session 1.4):
   - Position was implicit in the architecture
   - Used a fixed-size window (e.g., 3-5 words)
   - Word embeddings were concatenated in order: [word₁, word₂, word₃, ...]
   - Each position had its own set of weights connecting to the hidden layer
   - The weights essentially "remembered" which position each word came from

2. **The Transformer Challenge**:
   - Processes all tokens simultaneously, not one by one
   - Uses the same weights for all positions (parameter sharing)
   - Without additional information, couldn't tell word order

This is why transformers need an explicit way to encode position information.

#### Adding Position Information

Transformers add position information through position embeddings:
- Each position in the sequence gets its own unique vector representation
- These vectors are added to the token embeddings
- The result maintains both "what" (token identity) and "where" (position) information

#### Fixed vs. Learned Position Embeddings

There are two main approaches:
- **Learned Position Embeddings**: The model learns a separate vector for each position during training
- **Fixed Position Embeddings**: Mathematical functions determine position representations

Most transformers use a clever approach called sinusoidal position encodings:
- Each position gets a unique pattern of sine and cosine waves
- Similar positions have similar patterns
- The model can generalize to sequence lengths not seen during training

**Understanding Check ✓**
- How does the transformer's handling of position information differ from Bengio's neural language model?
- Why can't transformers rely on the implicit position information used in earlier models?

### Hands-On Implementation (For CS Students)

#### Implementing Sinusoidal Position Encodings

Here's how to implement the sinusoidal position encodings from the original Transformer paper:

```python
import torch
import numpy as np
import matplotlib.pyplot as plt

def get_sinusoidal_position_encoding(max_seq_len, d_model):
    """Generate sinusoidal position encodings."""
    position = torch.arange(max_seq_len).unsqueeze(1)
    div_term = torch.exp(torch.arange(0, d_model, 2) * -(np.log(10000.0) / d_model))
    
    pos_encoding = torch.zeros(max_seq_len, d_model)
    pos_encoding[:, 0::2] = torch.sin(position * div_term)
    pos_encoding[:, 1::2] = torch.cos(position * div_term)
    
    return pos_encoding

# Create position encodings
seq_len = 100
d_model = 64
pos_encodings = get_sinusoidal_position_encoding(seq_len, d_model)

# Visualize position encodings
plt.figure(figsize=(10, 6))
plt.imshow(pos_encodings.numpy(), cmap='viridis', aspect='auto')
plt.xlabel('Embedding Dimension')
plt.ylabel('Sequence Position')
plt.colorbar()
plt.title('Sinusoidal Position Encodings')
plt.show()

# Exercise: Try adding these position encodings to token embeddings
```

**Practice Exercise**: Experiment with visualizing how the position encodings change for longer sequences.

### Advanced Theory (For the Curious)

#### Alternative Position Encoding Methods

Before transformers, other approaches handled position information in different ways:

1. **Recurrent Neural Networks (RNNs)**:
   - Processed tokens one at a time in sequence
   - Maintained a hidden state that implicitly encoded position
   - LSTMs and GRUs improved this approach to handle longer sequences
   - Sequential processing created computational bottlenecks

2. **Convolutional Neural Networks for Text**:
   - Fixed-window convolutions captured local positional relationships
   - Stacking layers increased the effective receptive field
   - Position still somewhat "hard-wired" into the architecture

#### Mathematical Properties of Sinusoidal Encodings

The sinusoidal position encoding for position $pos$ and dimension $i$ is:

$PE_{(pos, 2i)} = \sin\left(\frac{pos}{10000^{2i/d_{model}}}\right)$
$PE_{(pos, 2i+1)} = \cos\left(\frac{pos}{10000^{2i/d_{model}}}\right)$

This formulation has several important properties:
- It allows the model to attend to relative positions through linear transformations
- The wavelengths form a geometric progression from $2\pi$ to $10000 \cdot 2\pi$
- It enables the model to extrapolate to unseen sequence positions

#### Relative Position Representations

Recent research has explored alternatives that directly model relative positions:
- Shaw et al. (2018) proposed relative position representations in self-attention
- T5 (Raffel et al., 2020) used relative position bias terms
- These approaches can better capture relative distances between tokens

**Research Reference**: "Attention Is All You Need" (Vaswani et al., 2017) introduced the sinusoidal position encodings that are still widely used.

---

## Knowledge Point 3: Combined Inputs: Tokens and Positions Working Together

### Core Concepts (For Everyone)

**Probing Question**: Now that we've explored tokens and positions separately, how do you think these elements might work together to form the input to a transformer?

#### The Integration Problem

To understand language, we need to know both:
- **What** each token is (its meaning)
- **Where** it appears in the sequence (its position)

> **Everyday Analogy**: Think of a recipe. You need both the ingredients (what) and the order of steps (where). Mixing flour and eggs first creates something very different than adding them last!

#### Simple But Powerful Solution: Vector Addition

Transformers combine token and position information through a remarkably simple approach:
- Token embeddings represent the identity of each token
- Position embeddings represent the position in the sequence
- These are combined through simple vector addition:
  
  `input = token_embedding + position_embedding`

This creates a single representation that preserves both types of information and serves as the foundation for all subsequent processing.

#### Why This Works

Vector addition works because:
- Each dimension of the embedding can capture different aspects of both token and position
- The model learns which dimensions should focus on which aspects during training
- It's computationally efficient, requiring only a single addition operation

**Understanding Check ✓**
- Why is adding vectors sufficient to combine token and position information?
- How does this combined representation help the model process language?

### Hands-On Implementation (For CS Students)

#### Implementing Combined Embeddings

Here's how to implement token and position embeddings in PyTorch:

```python
import torch
import torch.nn as nn

class TransformerEmbedding(nn.Module):
    def __init__(self, vocab_size, d_model, max_seq_len):
        super().__init__()
        self.token_embedding = nn.Embedding(vocab_size, d_model)
        self.position_encoding = get_sinusoidal_position_encoding(max_seq_len, d_model)
        self.dropout = nn.Dropout(0.1)
        
    def forward(self, x):
        # Get token embeddings
        token_emb = self.token_embedding(x)  # [batch_size, seq_len, d_model]
        
        # Get position encodings and convert to same device as token embeddings
        pos_enc = self.position_encoding[:x.size(1), :].to(x.device)
        
        # Add token embeddings and position encodings
        combined = token_emb + pos_enc
        
        return self.dropout(combined)

# Example usage
vocab_size = 5000
d_model = 64
max_seq_len = 512
embedding_layer = TransformerEmbedding(vocab_size, d_model, max_seq_len)

# Create a sample batch of token indices
batch_size = 16
seq_len = 10
token_indices = torch.randint(0, vocab_size, (batch_size, seq_len))

# Get combined embeddings
combined_embeddings = embedding_layer(token_indices)
print(f"Combined embedding shape: {combined_embeddings.shape}")
```

**Practice Exercise**: Visualize how token embeddings and position embeddings combine for specific tokens.

### Advanced Theory (For the Curious)

#### Information Flow Analysis

The combined embeddings serve as the input to the transformer's attention layers. Mathematically, we can represent the input at position $t$ as:

$$x_t = E_{token}[w_t] + E_{position}[t]$$

Where:
- $E_{token}$ is the token embedding matrix
- $w_t$ is the token at position $t$
- $E_{position}$ is the position embedding matrix

This representation preserves both semantic and positional information, which is crucial for the model to learn syntactic and semantic relationships.

#### Alternative Combination Methods

While addition is the standard approach, researchers have explored other combination methods:
- Concatenation: Combine by stacking vectors (increases dimensionality)
- Multiplication: Element-wise multiplication of vectors
- Complex embeddings: Using complex numbers to encode different types of information

Recent work suggests that for very deep models, position information can "leak" across layers, leading to innovations in how position is incorporated in advanced architectures like DeepNet and Transformers-XL.

**Research Reference**: "On Position Embeddings in BERT" (Wang & Chen, 2020) explores different position embedding approaches and their effects.

---

## Reflection and Synthesis

### Integration Activity (For Everyone)

Summarize how the concepts from this session work together:

1. **Power Law Solution**: Subword tokenization efficiently handles language's natural distribution, where a few words are very common but most are rare.

2. **Position Preservation**: Position embeddings solve the challenge of maintaining sequence information when processing tokens in parallel.

3. **Combined Representations**: Simple vector addition effectively merges token identity with position information to create rich input representations.

### Key Questions to Consider

- How do these mechanisms differ from the language models we explored in Module 1?
- What advantages do these approaches provide for handling complex language tasks?
- How do these elements prepare us to understand the full transformer architecture?

### Next Steps Preview

In the next session, we'll explore the core innovation of transformer models—attention mechanisms—and how they allow tokens to interact with each other to create context-aware representations. We'll also discover how feed-forward networks function within the transformer architecture.

---

## Additional Resources

### Visualizations
- [Interactive BPE Tokenization Demo](https://huggingface.co/tokenizers/)
- [Transformer Position Encoding Visualization](https://jalammar.github.io/illustrated-transformer/)

### Research References
- Vaswani, A., et al. (2017). "Attention Is All You Need."
- Sennrich, R., Haddow, B., & Birch, A. (2016). "Neural Machine Translation of Rare Words with Subword Units."
- Shaw, P., Uszkoreit, J., & Vaswani, A. (2018). "Self-Attention with Relative Position Representations."

### Code Tutorials
- [Implementing Transformers from Scratch](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)
- [HuggingFace Tokenizers Library](https://github.com/huggingface/tokenizers)
