# Session 2.2: Building a Transformer Block: Attention and Knowledge Storage

## Primary Question
How do transformers combine knowledge storage with contextual awareness to process language effectively?

## Knowledge Points Overview
This session covers the following key knowledge points:
1. Feed-forward networks as knowledge stores
2. The challenge of finding relevant context
3. Self-attention mechanisms
4. Multi-head attention
5. Combining attention with feed-forward networks into transformer blocks

---

## Knowledge Point 1: Feed-Forward Networks as Knowledge Stores

### Core Concepts (For Everyone)

**Probing Question:** How might a neural network store and retrieve factual knowledge that isn't explicitly present in the input text?

#### Beyond Pattern Recognition to Knowledge Storage

Modern language models can complete sentences like:
- "The capital of France is ____." (Paris)
- "Water boils at ____ degrees Celsius." (100)
- "Shakespeare wrote ____." (Hamlet, Macbeth, etc.)

These answers require knowledge beyond the immediate context - the model must have stored this information somewhere. But where?

> **Everyday Analogy**: Think of a library. Books contain factual information, but you need a system to find the right book when you need specific knowledge. Feed-forward networks work similarly in language models.

#### How Knowledge Is Stored

In modern transformers, feed-forward networks (FFNs) act as knowledge stores with a specific structure:

1. **Expansion Layer**: Makes the representation much larger (like opening a book wide)
   - Example: From 768 dimensions to 3072 dimensions in BERT
   
2. **ReLU Activation**: Creates sparsity (only some neurons activate)
   - Like highlighting only the relevant paragraphs in a book
   
3. **Contraction Layer**: Compresses back to the original size
   - Like summarizing what you found into a concise note

This expand-contract architecture functions like an associative memory or key-value store:
- Input patterns (keys) activate specific pathways
- The activated pathways retrieve associated knowledge (values)

**Understanding Check ✓**
- Why might a model need to expand representations before processing them?
- How does the sparse activation pattern help with storing different types of knowledge?

### Hands-On Implementation (For CS Students)

#### Implementing a Feed-Forward Network

Here's how to implement a standard feed-forward network in PyTorch:

```python
import torch
import torch.nn as nn

class FeedForward(nn.Module):
    def __init__(self, d_model, d_ff, dropout=0.1):
        super().__init__()
        # First linear layer expands the dimension
        self.linear1 = nn.Linear(d_model, d_ff)
        # ReLU activation creates sparse patterns
        self.activation = nn.ReLU()
        # Dropout for regularization
        self.dropout = nn.Dropout(dropout)
        # Second linear layer contracts back to model dimension
        self.linear2 = nn.Linear(d_ff, d_model)
        
    def forward(self, x):
        # Expansion
        x = self.linear1(x)
        # Activation creates sparse patterns
        x = self.activation(x)
        # Apply dropout
        x = self.dropout(x)
        # Contraction
        x = self.linear2(x)
        return x

# Example usage
d_model = 64  # Model dimension
d_ff = 256    # Expanded dimension (typically 4x the model dimension)
ff_layer = FeedForward(d_model, d_ff)

# Create a sample input
batch_size = 16
seq_len = 10
x = torch.randn(batch_size, seq_len, d_model)

# Pass through feed-forward network
output = ff_layer(x)
print(f"Input shape: {x.shape}, Output shape: {output.shape}")
```

**Practice Exercise**: Try visualizing the activation patterns in the hidden layer to see how sparsity emerges.

### Advanced Theory (For the Curious)

#### Universal Approximation Theorem

The theoretical foundation for FFNs as knowledge stores comes from the Universal Approximation Theorem (Hornik et al., 1989). This theorem states that a feed-forward network with a single hidden layer of sufficient width can approximate any continuous function to arbitrary precision.

In language models:
- The "function" being approximated maps contexts to relevant facts
- The wider the hidden layer, the more facts the network can store
- This explains why modern LLMs use extremely wide FFNs (often 4x the model dimension)

#### Key-Value Interpretation

Recent research (Geva et al., 2022) has demonstrated that feed-forward networks in transformers can be interpreted as key-value memories:
- Each neuron in the expanded representation corresponds to a specific "key" pattern
- The weights connecting this neuron to the output form the "value" associated with this key
- The ReLU activation ensures that only matching keys retrieve their associated values

This insight explains why larger feed-forward layers (with more neurons) can store more knowledge.

**Research Reference**: "Transformer Feed-Forward Layers Are Key-Value Memories" (Geva et al., 2022) provides empirical evidence for this interpretation.

---

## Knowledge Point 2: The Challenge of Finding Relevant Context

### Core Concepts (For Everyone)

**Probing Question:** If we have token embeddings, position information, and knowledge stored in FFNs, what obstacle remains in processing language effectively?

#### The Context Problem

Consider this sentence:
"The cat, which had been sleeping on the couch in the living room next to the large window overlooking the garden with colorful flowers, suddenly jumped when it heard a noise."

To predict what comes after "jumped," which earlier words matter most?
- "cat" is highly relevant (the subject who jumped)
- "flowers" is less relevant (just part of the background)
- "noise" is highly relevant (the cause of jumping)

> **Everyday Analogy**: Imagine trying to follow a conversation where 10 people are talking at once. You need to focus on the most relevant speakers and filter out the rest. Language models need a similar mechanism to focus on relevant context.

#### Beyond Sequential Processing

Earlier models like RNNs processed text one token at a time, but this approach had limitations:
- Information from distant tokens faded away (the "vanishing gradient" problem)
- Long-range dependencies were difficult to capture
- Processing was inherently sequential and slow

Modern language models need a mechanism to:
- Process all tokens simultaneously (for efficiency)
- Find connections between any tokens regardless of distance
- Weight the importance of different context elements

**Understanding Check ✓**
- Why is finding relevant context challenging in language processing?
- What limitations did sequential processing models have in capturing long-range dependencies?

### Hands-On Implementation (For CS Students)

#### Identifying Context Challenges

Let's explore the challenge of finding relevant context with some examples:

```python
import numpy as np
import matplotlib.pyplot as plt

# Example sentences with dependencies
sentences = [
    "John dropped the glass and it shattered.",
    "The movie that I watched last week with my friends was amazing.",
    "Although it was raining heavily, Sarah decided not to take her umbrella."
]

# Manually identify dependencies
dependencies = [
    [(0, 7), (3, 7)],  # "John" → "shattered", "glass" → "shattered"
    [(1, 12), (9, 1)],  # "movie" → "amazing", "watched" → "movie"
    [(5, 12), (12, 14)]   # "raining" → "decided", "decided" → "umbrella"
]

# Visualize dependencies
for i, (sentence, deps) in enumerate(zip(sentences, dependencies)):
    words = sentence.replace('.', '').replace(',', '').split()
    n = len(words)
    
    # Create dependency matrix
    matrix = np.zeros((n, n))
    for src, tgt in deps:
        matrix[src, tgt] = 1
        matrix[tgt, src] = 1  # Bidirectional for visualization
    
    # Plot
    plt.figure(figsize=(10, 6))
    plt.imshow(matrix, cmap='Blues')
    plt.xticks(range(n), words, rotation=45, ha='right')
    plt.yticks(range(n), words)
    plt.title(f"Dependencies in Sentence {i+1}")
    plt.tight_layout()
    plt.show()
```

**Practice Exercise**: For a given sentence, try identifying all the relevant dependencies that a model would need to capture.

### Advanced Theory (For the Curious)

#### Information Theory of Context

From an information theory perspective, the context problem can be formulated as finding the minimal sufficient context to predict the next token:

$$p(x_t | x_{<t}) \approx p(x_t | x_C)$$

Where:
- $x_t$ is the token to predict
- $x_{<t}$ is all previous tokens
- $x_C$ is a subset of previous tokens that provides sufficient context

The challenge is that this context subset $x_C$ varies based on:
- Syntactic structure
- Semantic relationships
- Document structure
- Task requirements

Traditional fixed-window approaches are insufficient because they can't adapt to these varying context requirements.

**Research Reference**: "Long Short-Term Memory" (Hochreiter & Schmidhuber, 1997) identified the challenge of long-range dependencies that attention ultimately addresses.

---

## Knowledge Point 3: Self-Attention Mechanisms

### Core Concepts (For Everyone)

**Probing Question**: How might a model determine the importance of different words in a sequence when processing each position?

#### The Attention Insight

The key innovation of attention is elegantly simple: for each position in the sequence, compute how much it should "attend to" (or focus on) every other position.

> **Everyday Analogy**: Imagine reading a mystery novel with a flashlight. As you read each word, you can shine your light back to important earlier parts of the story that help you understand the current moment. The attention mechanism is like having a flashlight for each word.

#### How Self-Attention Works

Self-attention involves three main steps:

1. **Create Three Perspectives for Each Token**:
   - **Query (Q)**: What this token is looking for (the questions it asks)
   - **Key (K)**: What this token offers to others (how it answers questions)
   - **Value (V)**: The actual content to be gathered (the information it provides)

2. **Calculate Attention Scores**:
   - For each token's query, compute its compatibility with all keys
   - Higher scores mean higher relevance

3. **Create a Weighted Sum**:
   - Use the scores to create a weighted mixture of values
   - The result is a context-aware representation of each token

#### Visualizing Attention

For a simple example "The cat sat on the mat":
- When processing "sat", high attention might go to "cat" (its subject)
- When processing "mat", high attention might go to "on" (its preposition)
- Every token can attend to all other tokens, regardless of distance

**Understanding Check ✓**
- How does attention solve the challenge of finding relevant context?
- Why is creating three perspectives (Q, K, V) helpful for determining relevance?

### Hands-On Implementation (For CS Students)

#### Implementing Self-Attention

Here's how to implement the self-attention mechanism in PyTorch:

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt

class SelfAttention(nn.Module):
    def __init__(self, d_model):
        super().__init__()
        self.d_model = d_model
        
        # Create projection matrices for Q, K, V
        self.q_linear = nn.Linear(d_model, d_model)
        self.k_linear = nn.Linear(d_model, d_model)
        self.v_linear = nn.Linear(d_model, d_model)
        
    def forward(self, x):
        # x shape: [batch_size, seq_len, d_model]
        batch_size, seq_len, d_model = x.size()
        
        # Create Q, K, V projections
        q = self.q_linear(x)  # [batch_size, seq_len, d_model]
        k = self.k_linear(x)  # [batch_size, seq_len, d_model]
        v = self.v_linear(x)  # [batch_size, seq_len, d_model]
        
        # Compute attention scores
        scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_model).float())
        # scores shape: [batch_size, seq_len, seq_len]
        
        # Apply softmax to get attention weights
        attention_weights = F.softmax(scores, dim=-1)
        # attention_weights shape: [batch_size, seq_len, seq_len]
        
        # Apply attention weights to values
        output = torch.matmul(attention_weights, v)
        # output shape: [batch_size, seq_len, d_model]
        
        return output, attention_weights

# Example usage
d_model = 64
attention = SelfAttention(d_model)

# Create a sample input
batch_size = 2
seq_len = 5
x = torch.randn(batch_size, seq_len, d_model)

# Pass through attention layer
output, weights = attention(x)
print(f"Output shape: {output.shape}")
print(f"Attention weights shape: {weights.shape}")

# Visualize attention for the first example in the batch
plt.figure(figsize=(8, 6))
plt.imshow(weights[0].detach().numpy(), cmap='viridis')
plt.colorbar()
plt.title('Attention Weights')
plt.xlabel('Key positions')
plt.ylabel('Query positions')
plt.show()
```

**Practice Exercise**: Try implementing a masked self-attention layer that prevents attention to future tokens (for autoregressive models).

### Advanced Theory (For the Curious)

#### Mathematical Formulation of Self-Attention

The self-attention mechanism can be precisely formulated as:

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

Where:
- $Q = XW^Q$ (queries)
- $K = XW^K$ (keys)
- $V = XW^V$ (values)
- $X$ is the input matrix containing token representations
- $W^Q, W^K, W^V$ are learned parameter matrices
- $d_k$ is the dimensionality of the keys

The division by $\sqrt{d_k}$ is a scaling factor that prevents the dot products from growing too large in magnitude, which could push the softmax function into regions with extremely small gradients.

#### Attention as Differentiable Dictionary Lookup

From a theoretical perspective, attention can be viewed as a differentiable dictionary lookup:
- Keys act as dictionary keys
- Values act as dictionary values
- Queries search the dictionary
- The softmax creates a fuzzy matching process instead of exact lookup

This interpretation helps explain why attention is so effective at retrieving relevant information from context.

**Research Reference**: "Attention Is All You Need" (Vaswani et al., 2017) introduced the self-attention mechanism that revolutionized natural language processing.

---

## Knowledge Point 4: Multi-Head Attention

### Core Concepts (For Everyone)

**Probing Question**: Why might it be helpful for a model to have multiple different ways of computing attention over the same sequence?

#### The Limitation of Single Attention

Language has many types of relationships:
- Subject-verb relationships ("The dog barks")
- Object relationships ("Threw the ball")
- Grammatical agreement ("These books are...")
- Pronouns and their references ("She said she...")

A single attention mechanism might struggle to capture all these different relationship types simultaneously.

> **Everyday Analogy**: Imagine trying to analyze a painting with just one perspective. You might focus on the colors, or the composition, or the subjects - but it's hard to see all aspects at once. Multi-head attention is like having multiple art critics looking at the same painting, each focusing on different aspects.

#### The Multi-Head Solution

Multi-head attention runs multiple attention operations in parallel:

1. **Multiple Sets of Projections**:
   - Instead of one set of Q/K/V projections, create several sets
   - Each "head" gets its own projection matrices
   
2. **Parallel Processing**:
   - Each head computes attention independently
   - Different heads can specialize in different patterns

3. **Combine Results**:
   - Concatenate the outputs from all heads
   - Project back to the original dimension

#### Specialized Patterns Emerge

In trained models, different attention heads often specialize in different linguistic patterns:
- Some track subject-verb relationships
- Others focus on object relationships
- Some detect entity references (like pronouns and their antecedents)
- Certain heads might focus on positional patterns

**Understanding Check ✓**
- Why is having multiple attention heads better than a single, larger attention mechanism?
- How might different heads specialize in different types of linguistic relationships?

### Hands-On Implementation (For CS Students)

#### Implementing Multi-Head Attention

Here's how to implement multi-head attention in PyTorch:

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.head_dim = d_model // num_heads
        
        assert d_model % num_heads == 0, "d_model must be divisible by num_heads"
        
        # Create projections for all heads at once
        self.q_linear = nn.Linear(d_model, d_model)
        self.k_linear = nn.Linear(d_model, d_model)
        self.v_linear = nn.Linear(d_model, d_model)
        
        # Output projection
        self.out_proj = nn.Linear(d_model, d_model)
        
    def forward(self, x):
        batch_size, seq_len, d_model = x.size()
        
        # Create Q, K, V projections for all heads
        q = self.q_linear(x)  # [batch_size, seq_len, d_model]
        k = self.k_linear(x)  # [batch_size, seq_len, d_model]
        v = self.v_linear(x)  # [batch_size, seq_len, d_model]
        
        # Reshape for multi-head attention
        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        k = k.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        v = v.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        # Now shapes are [batch_size, num_heads, seq_len, head_dim]
        
        # Compute attention scores
        scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.head_dim).float())
        # scores shape: [batch_size, num_heads, seq_len, seq_len]
        
        # Apply softmax to get attention weights
        attention_weights = F.softmax(scores, dim=-1)
        # weights shape: [batch_size, num_heads, seq_len, seq_len]
        
        # Apply attention weights to values
        output = torch.matmul(attention_weights, v)
        # output shape: [batch_size, num_heads, seq_len, head_dim]
        
        # Reshape back to original dimensions
        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)
        # output shape: [batch_size, seq_len, d_model]
        
        # Final projection
        output = self.out_proj(output)
        
        return output, attention_weights

# Example usage
d_model = 64
num_heads = 4
multi_head_attn = MultiHeadAttention(d_model, num_heads)

# Create a sample input
batch_size = 2
seq_len = 5
x = torch.randn(batch_size, seq_len, d_model)

# Pass through multi-head attention
output, weights = multi_head_attn(x)
print(f"Output shape: {output.shape}")
print(f"Attention weights shape: {weights.shape}")
```

**Practice Exercise**: Visualize the attention patterns from different heads and observe how they differ.

### Advanced Theory (For the Curious)

#### Mathematical Formulation of Multi-Head Attention

Multi-head attention is defined as:

$\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W^O$

Where each head is:

$\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$

The parameters for each head are:
- $W_i^Q \in \mathbb{R}^{d_{\text{model}} \times d_k}$
- $W_i^K \in \mathbb{R}^{d_{\text{model}} \times d_k}$
- $W_i^V \in \mathbb{R}^{d_{\text{model}} \times d_v}$
- $W^O \in \mathbb{R}^{hd_v \times d_{\text{model}}}$

Where typically $d_k = d_v = d_{\text{model}}/h$.

#### Attention Head Specialization

Research has shown that different heads specialize in different linguistic phenomena:
- Syntactic heads: Focus on grammatical structure
- Semantic heads: Focus on meaning relationships
- Positional heads: Attend to nearby tokens
- Background heads: Distribute attention broadly

This emergent specialization happens naturally during training and contributes to the model's overall language understanding capabilities.

**Research Reference**: "A Multiscale Visualization of Attention in the Transformer Model" (Vig, 2019) provides visualizations of how different attention heads specialize.

---

## Knowledge Point 5: Combining Attention with Feed-Forward Networks into Transformer Blocks

### Core Concepts (For Everyone)

**Probing Question**: How do all these components come together to form the fundamental unit of transformers?

#### The Transformer Block: Combining Strengths

We now have two powerful mechanisms with complementary strengths:

1. **Multi-Head Self-Attention**:
   - Finds relevant connections across the sequence
   - Creates contextually enriched representations

2. **Feed-Forward Networks**:
   - Store factual knowledge
   - Apply computation to individual positions

The transformer block combines these into a standard unit that serves as the building block of modern language models.

> **Everyday Analogy**: Think of attention as research (finding relevant information from many sources) and the feed-forward network as reasoning (applying knowledge to process that information). The transformer block combines research and reasoning into a single thinking step.

#### The Standard Block Architecture

A standard transformer block arranges these components sequentially:

1. **Layer Normalization** → **Multi-Head Attention** → **Residual Connection**
2. **Layer Normalization** → **Feed-Forward Network** → **Residual Connection**

Additional components provide stability:
- **Layer Normalization**: Standardizes representations to keep training stable
- **Residual Connections**: Add the input to the output of each component (like a shortcut)

#### The Power of This Combination

This arrangement creates a system more powerful than either component alone:
- Attention provides context-aware representations
- FFNs apply knowledge and computation
- Layer normalization maintains stability
- Residual connections help information flow

**Understanding Check ✓**
- Why is the combination of attention and feed-forward networks more powerful than either component alone?
- What role do residual connections play in the transformer block?

### Hands-On Implementation (For CS Students)

#### Implementing a Transformer Block

Here's how to implement a complete transformer block in PyTorch:

```python
import torch
import torch.nn as nn

class TransformerBlock(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super().__init__()
        # Multi-head attention
        self.attention = MultiHeadAttention(d_model, num_heads)
        # Feed-forward network
        self.feed_forward = FeedForward(d_model, d_ff, dropout)
        # Layer normalization
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        # Dropout
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x):
        # Apply first layer norm
        norm_x = self.norm1(x)
        
        # Apply attention
        attn_output, _ = self.attention(norm_x)
        attn_output = self.dropout(attn_output)
        
        # First residual connection
        x = x + attn_output
        
        # Apply second layer norm
        norm_x = self.norm2(x)
        
        # Apply feed-forward network
        ff_output = self.feed_forward(norm_x)
        ff_output = self.dropout(ff_output)
        
        # Second residual connection
        x = x + ff_output
        
        return x

# Example usage
d_model = 64
num_heads = 4
d_ff = 256
transformer_block = TransformerBlock(d_model, num_heads, d_ff)

# Create a sample input
batch_size = 2
seq_len = 5
x = torch.randn(batch_size, seq_len, d_model)

# Pass through transformer block
output = transformer_block(x)
print(f"Output shape: {output.shape}")
```

**Practice Exercise**: Implement a stack of transformer blocks and observe how information flows through the network.

### Advanced Theory (For the Curious)

#### Block Variants and Architecture Choices

While the standard transformer block is powerful, researchers have explored various modifications:

1. **Pre-norm vs. Post-norm**:
   - Pre-norm (described above): Normalize before each component
   - Post-norm: Normalize after each component and residual connection
   - Pre-norm tends to be more stable for training very deep models

2. **Gated Mechanisms**:
   - Some architectures add gating mechanisms to control information flow
   - Similar to LSTMs but applied to transformers

3. **Architecture Efficiency**:
   - Performer, Linformer, and other variants modify attention for computational efficiency
   - Some replace standard attention with approximations that scale better to long sequences

#### Computation Complexity Analysis

The standard transformer block has two computational bottlenecks:

1. **Self-Attention**:
   - Time complexity: O(n²d) where n is sequence length and d is dimension
   - Memory complexity: O(n²) for storing attention weights

2. **Feed-Forward Network**:
   - Time complexity: O(nd²) where d is typically large (4x the model dimension)
   - Memory complexity: O(nd)

For very long sequences, the quadratic scaling of attention becomes prohibitive, leading to research on more efficient attention mechanisms.

**Research Reference**: "Transformers: State-of-the-Art Natural Language Processing" (Wolf et al., 2020) provides a comprehensive overview of transformer architectures and variants.

---

## Reflection and Synthesis

### Integration Activity (For Everyone)

Let's connect the key concepts we've explored in this session:

1. **Knowledge Storage**: Feed-forward networks act as key-value memories, storing factual knowledge using an expand-contract architecture.

2. **Context Finding**: Self-attention directly models relationships between all positions in a sequence, solving the long-range dependency problem.

3. **Multi-Faceted Understanding**: Multi-head attention enables specialized attention patterns for different types of relationships.

4. **The Transformer Block**: By combining multi-head attention with feed-forward networks, transformers create a powerful computational unit that can both find relevant context and apply stored knowledge.

These components work together to create a system that:
- Efficiently processes language in parallel
- Handles long-range dependencies
- Stores and applies factual knowledge
- Adapts to different types of linguistic relationships

### Key Questions to Consider

- How does the transformer architecture address the limitations of previous approaches?
- What specific advantages does self-attention have over sequential processing methods?
- How do the complementary strengths of attention and feed-forward networks create a more powerful system when combined?

### Next Steps Preview

In the next session, we'll explore how transformer blocks are stacked to create deeper networks using residual connections, and we'll examine the complete training process for modern LLMs, from pre-training on raw text to instruction tuning.

---

## Additional Resources

### Visualizations
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- [Attention Visualization Tool](https://github.com/jessevig/bertviz)

### Research References
- Vaswani, A., et al. (2017). "Attention Is All You Need."
- Geva, M., et al. (2022). "Transformer Feed-Forward Layers Are Key-Value Memories."
- Vig, J. (2019). "A Multiscale Visualization of Attention in the Transformer Model."

### Code Tutorials
- [PyTorch Transformer Implementation](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)
- [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)
