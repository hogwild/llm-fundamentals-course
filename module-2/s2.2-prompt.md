# Session 2.2: Building a Transformer Block: Attention and Knowledge Storage

## Primary Question
How do transformers combine knowledge storage with contextual awareness to process language effectively?

## Knowledge Points Overview
This session covers the following key knowledge points:
1. Feed-forward networks as knowledge stores
2. The challenge of finding relevant context
3. Self-attention mechanisms
4. Multi-head attention
5. Combining attention with feed-forward networks into transformer blocks

---

## Knowledge Point 1: Feed-Forward Networks as Knowledge Stores

### Core Concepts (For Everyone)

**Probing Question:** How might a neural network store and retrieve factual knowledge that isn't explicitly present in the input text?

#### Beyond Pattern Recognition to Knowledge Storage

Modern language models can complete sentences like:
- "The capital of France is ____." (Paris)
- "Water boils at ____ degrees Celsius." (100)
- "Shakespeare wrote ____." (Hamlet, Macbeth, etc.)

These answers require knowledge beyond the immediate context - the model must have stored this information somewhere. But where?

> **Everyday Analogy**: Think of a library. Books contain factual information, but you need a system to find the right book when you need specific knowledge. Feed-forward networks work similarly in language models.

#### How Knowledge Is Stored

In modern transformers, feed-forward networks (FFNs) act as knowledge stores with a specific structure:

1. **Expansion Layer**: Makes the representation much larger (like opening a book wide)
   - Example: From 768 dimensions to 3072 dimensions in BERT
   
2. **ReLU Activation**: Creates sparsity (only some neurons activate)
   - Like highlighting only the relevant paragraphs in a book
   
3. **Contraction Layer**: Compresses back to the original size
   - Like summarizing what you found into a concise note

This expand-contract architecture functions like an associative memory or key-value store:
- Input patterns (keys) activate specific pathways in the expansion layer
- The activated pathways retrieve associated knowledge (values) from the contraction layer

**Understanding Check ✓**
- Why might a model need to expand representations before processing them?
- How does the sparse activation pattern help with storing different types of knowledge?
- Before proceeding, can you explain in your own words how FFNs might store factual knowledge?

### Hands-On Implementation (For CS Students)

#### Implementing a Feed-Forward Network

Here's how to implement a standard feed-forward network in PyTorch:

```python
import torch
import torch.nn as nn

class FeedForward(nn.Module):
    def __init__(self, d_model, d_ff, dropout=0.1):
        super().__init__()
        # First linear layer expands the dimension (KEY)
        self.linear1 = nn.Linear(d_model, d_ff)
        # ReLU activation creates sparse patterns
        self.activation = nn.ReLU()
        # Dropout for regularization
        self.dropout = nn.Dropout(dropout)
        # Second linear layer contracts back to model dimension (VALUE)
        self.linear2 = nn.Linear(d_ff, d_model)
        
    def forward(self, x):
        # Expansion - works as KEY matching
        x = self.linear1(x)
        # Activation creates sparse patterns - only some keys "match"
        x = self.activation(x)
        # Apply dropout
        x = self.dropout(x)
        # Contraction - retrieves VALUES associated with activated keys
        x = self.linear2(x)
        return x

# Example usage
d_model = 64  # Model dimension
d_ff = 256    # Expanded dimension (typically 4x the model dimension)
ff_layer = FeedForward(d_model, d_ff)

# Create a sample input
batch_size = 16
seq_len = 10
x = torch.randn(batch_size, seq_len, d_model)

# Pass through feed-forward network
output = ff_layer(x)
print(f"Input shape: {x.shape}, Output shape: {output.shape}")
```

**Practice Exercise**: Try visualizing the activation patterns in the hidden layer to see how sparsity emerges after the ReLU activation.

### Advanced Theory (For the Curious)

#### Universal Approximation Theorem

The theoretical foundation for why FFNs can function as powerful knowledge stores comes from the Universal Approximation Theorem (Hornik et al., 1989). This fundamental theorem states that a feed-forward network with a single hidden layer of sufficient width can approximate any continuous function to arbitrary precision.

In the context of language models:
- The "function" being approximated maps contexts to relevant facts
- The wider the hidden layer, the more facts the network can store
- This explains why modern LLMs use extremely wide FFNs (often 4x the model dimension)

The theorem essentially guarantees that with enough neurons in the hidden layer, an FFN can learn to store and retrieve virtually any knowledge pattern required during training, provided it has enough capacity.

#### Key-Value Interpretation

Recent research (Geva et al., 2022) has demonstrated that feed-forward networks in transformers can be interpreted as key-value memories:
- Each neuron in the expanded representation corresponds to a specific "key" pattern
- The weights connecting this neuron to the output form the "value" associated with this key
- The ReLU activation ensures that only matching keys retrieve their associated values

Mathematically, we can represent this as:
- First layer (W₁): Maps inputs x to keys K = W₁x
- ReLU: Creates sparsity pattern S = ReLU(K), activating only matching keys
- Second layer (W₂): Retrieves associated values V = S·W₂

This insight explains why larger feed-forward layers (with more neurons) can store more knowledge, and why the first layer acts as key-matching while the second layer provides value retrieval.

**Research References**: 
- "Multilayer feedforward networks are universal approximators" (Hornik et al., 1989)
- "Transformer Feed-Forward Layers Are Key-Value Memories" (Geva et al., 2022)

---

## Knowledge Point 2: The Challenge of Finding Relevant Context

### Core Concepts (For Everyone)

**Probing Question:** If we have token embeddings, position information, and knowledge stored in FFNs, what obstacle remains in processing language effectively?

#### The Context Problem

Consider this sentence:
"The cat, which had been sleeping on the couch in the living room next to the large window overlooking the garden with colorful flowers, suddenly jumped when it heard a noise."

To predict what comes after "jumped," which earlier words matter most?
- "cat" is highly relevant (the subject who jumped)
- "flowers" is less relevant (just part of the background)
- "noise" is highly relevant (the cause of jumping)

> **Everyday Analogy**: Imagine trying to follow a conversation where 10 people are talking at once. You need to focus on the most relevant speakers and filter out the rest. Language models need a similar mechanism to focus on relevant context.

#### The Limitations of Previous Approaches

Let's review how earlier models handled context:

1. **N-gram Models** (Session 1.2):
   - Limited to a small, fixed window of previous words
   - Couldn't capture long-range dependencies at all

2. **Bengio's Neural Language Model** (Session 1.4):
   - Also used a fixed context window
   - Concatenated embeddings from a small number of previous words
   - Position-specific processing locked to window size

3. **Sequential Processing Models**:
   - Process text one token at a time
   - Information from distant tokens fades away
   - Computationally slow for long sequences

What we need is a mechanism to:
- Process all tokens simultaneously (for efficiency)
- Find connections between any tokens regardless of distance
- Weight the importance of different context elements dynamically

**Understanding Check ✓**
- Why is finding relevant context challenging in language processing?
- What limitations did earlier models have in capturing dependencies between distant words?
- Before we proceed, can you identify a real-world example where understanding a word requires context from much earlier in a text?

### Hands-On Implementation (For CS Students)

#### Exploring Context Dependencies

Let's create a visualization tool to explore the challenge of context dependencies:

```python
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

def visualize_context_dependencies(sentences, dependencies):
    """
    Visualize the context dependencies in sentences.
    
    Parameters:
    - sentences: List of sentences
    - dependencies: List of lists of tuples (source_idx, target_idx)
        representing dependencies between words
    """
    for i, (sentence, deps) in enumerate(zip(sentences, dependencies)):
        words = sentence.replace('.', '').replace(',', '').split()
        n = len(words)
        
        # Create dependency matrix
        matrix = np.zeros((n, n))
        for src, tgt in deps:
            matrix[src, tgt] = 1
            matrix[tgt, src] = 1  # Make bidirectional for visualization
        
        # Plot
        plt.figure(figsize=(10, 8))
        sns.heatmap(matrix, cmap='Blues', 
                   xticklabels=words, yticklabels=words)
        plt.title(f"Context Dependencies in Example {i+1}")
        plt.tight_layout()
        plt.show()
        
        # Print dependency explanation
        print(f"Dependencies in '{sentence}':")
        for src, tgt in deps:
            print(f"  * '{words[src]}' → '{words[tgt]}'")
        print()

# Example sentences with dependencies
sentences = [
    "The cat chased the mouse because it was hungry.",
    "The company that was founded in Seattle in 1994 is Amazon.",
    "After studying all night, Maria felt exhausted during her exam."
]

# Dependencies as (source_word_idx, target_word_idx)
dependencies = [
    [(0, 1), (1, 2), (1, 6), (6, 7)],  # "cat" → "chased", "cat" → "hungry", etc.
    [(1, 9), (6, 9), (7, 9)],          # "company" → "Amazon", "Seattle" → "Amazon", etc.
    [(1, 2), (3, 5), (5, 8)]           # "studying" → "night", "Maria" → "exhausted", etc.
]

# Visualize
visualize_context_dependencies(sentences, dependencies)
```

This visualization helps us see that language has many types of dependencies:
- Subject-verb relationships ("cat" → "chased")
- Pronoun references ("it" → "cat")
- Long-distance dependencies ("company" → "Amazon")

**Practice Exercise**: Create your own example sentence with complex dependencies and visualize the relationships. Try to include at least one dependency that spans more than 5 words.

### Advanced Theory (For the Curious)

#### Attention as a Solution: A Preview

The challenges of context modeling ultimately led to the development of attention mechanisms, which we'll explore in the next knowledge point. But first, let's understand the theoretical foundations of the context problem.

#### Information Bottleneck Theory

The context problem can be understood through the lens of information bottleneck theory. This theory suggests that effective representation learning involves:

1. **Compression**: Reducing input to a compact representation
2. **Relevance Preservation**: Keeping information relevant to the task
3. **Bottleneck Management**: Balancing compression against relevance

Fixed-window approaches create a hard bottleneck that arbitrarily restricts context. Sequential models create a "leaky" bottleneck where distant information gradually fades.

#### Statistical Language Model Perspective

From a statistical perspective, a language model estimates:

$p(x_t | x_{<t})$

Where $x_{<t}$ represents all previous tokens. The challenge is approximating this full context with a tractable computation. Previous approaches used different approximations:

- N-grams: $p(x_t | x_{<t}) \approx p(x_t | x_{t-n},...,x_{t-1})$
- Neural LMs: $p(x_t | x_{<t}) \approx f_\theta(E[x_{t-n}],...,E[x_{t-1}])$
- RNNs: $p(x_t | x_{<t}) \approx f_\theta(h_{t-1}, E[x_t])$ where $h_{t-1}$ is a fixed-size state

None of these approaches can optimally balance the full context against computational efficiency.

**Research References**: 
- "A Neural Probabilistic Language Model" (Bengio et al., 2003)
- "On the difficulty of training recurrent neural networks" (Pascanu et al., 2013)

---

## Knowledge Point 3: Self-Attention Mechanisms

### Core Concepts (For Everyone)

**Probing Question**: Now that we understand the challenge of finding relevant context, how might a model determine the importance of different words in a sequence when processing each position?

#### The Attention Solution

The key innovation of attention is elegantly simple yet powerful: for each position in the sequence, compute how much it should "attend to" (or focus on) every other position.

> **Everyday Analogy**: Imagine reading a mystery novel with a flashlight. As you read each word, you can shine your light back to important earlier parts of the story that help you understand the current moment. The attention mechanism is like having a flashlight for each word.

#### How Self-Attention Works

Self-attention involves three main steps:

1. **Create Three Perspectives for Each Token**:
   - **Query (Q)**: What this token is looking for (the questions it asks)
   - **Key (K)**: What this token offers to others (how it answers questions)
   - **Value (V)**: The actual content to be gathered (the information it provides)

2. **Calculate Attention Scores**:
   - For each token's query, compute its compatibility with all keys
   - Higher scores mean higher relevance
   - Convert scores to weights using softmax (ensures all weights sum to 1)

3. **Create a Weighted Sum**:
   - Use the weights to create a weighted mixture of values
   - The result is a context-aware representation of each token

#### Visualizing Attention

For a simple example "The cat sat on the mat":
- When processing "sat", high attention might go to "cat" (its subject)
- When processing "mat", high attention might go to "on" (its preposition)
- Every token can attend to all other tokens, regardless of distance

**Understanding Check ✓**
- How does attention solve the challenge of finding relevant context?
- Why is creating three perspectives (Q, K, V) helpful for determining relevance?
- Before proceeding, can you explain how self-attention would help with a specific example (e.g., resolving "it" in "The cat chased the mouse because it was hungry")?

### Hands-On Implementation (For CS Students)

#### Implementing Self-Attention

Here's how to implement the self-attention mechanism in PyTorch:

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt

class SelfAttention(nn.Module):
    def __init__(self, d_model):
        super().__init__()
        self.d_model = d_model
        
        # Create projection matrices for Q, K, V
        self.q_linear = nn.Linear(d_model, d_model)
        self.k_linear = nn.Linear(d_model, d_model)
        self.v_linear = nn.Linear(d_model, d_model)
        
    def forward(self, x):
        # x shape: [batch_size, seq_len, d_model]
        batch_size, seq_len, d_model = x.size()
        
        # Create Q, K, V projections
        q = self.q_linear(x)  # [batch_size, seq_len, d_model]
        k = self.k_linear(x)  # [batch_size, seq_len, d_model]
        v = self.v_linear(x)  # [batch_size, seq_len, d_model]
        
        # Compute attention scores
        scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(d_model).float())
        # scores shape: [batch_size, seq_len, seq_len]
        
        # Apply softmax to get attention weights
        attention_weights = F.softmax(scores, dim=-1)
        # attention_weights shape: [batch_size, seq_len, seq_len]
        
        # Apply attention weights to values
        output = torch.matmul(attention_weights, v)
        # output shape: [batch_size, seq_len, d_model]
        
        return output, attention_weights

# Example usage
d_model = 64
attention = SelfAttention(d_model)

# Create a sample input
batch_size = 2
seq_len = 5
x = torch.randn(batch_size, seq_len, d_model)

# Pass through attention layer
output, weights = attention(x)
print(f"Output shape: {output.shape}")
print(f"Attention weights shape: {weights.shape}")

# Visualize attention for the first example in the batch
plt.figure(figsize=(8, 6))
plt.imshow(weights[0].detach().numpy(), cmap='viridis')
plt.colorbar()
plt.title('Attention Weights')
plt.xlabel('Key positions')
plt.ylabel('Query positions')
plt.show()
```

**Practice Exercise**: Try implementing a masked self-attention layer that prevents attention to future tokens (for autoregressive models).

### Advanced Theory (For the Curious)

#### Mathematical Formulation of Self-Attention

The self-attention mechanism can be precisely formulated as:

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$

Where:
- $Q = XW^Q$ (queries)
- $K = XW^K$ (keys)
- $V = XW^V$ (values)
- $X$ is the input matrix containing token representations
- $W^Q, W^K, W^V$ are learned parameter matrices
- $d_k$ is the dimensionality of the keys

The division by $\sqrt{d_k}$ is a scaling factor that prevents the dot products from growing too large in magnitude, which could push the softmax function into regions with extremely small gradients.

#### Attention as Differentiable Dictionary Lookup

From a theoretical perspective, attention can be viewed as a differentiable dictionary lookup:
- Keys act as dictionary keys
- Values act as dictionary values
- Queries search the dictionary
- The softmax creates a fuzzy matching process instead of exact lookup

This interpretation helps explain why attention is so effective at retrieving relevant information from context.

**Research Reference**: "Attention Is All You Need" (Vaswani et al., 2017) introduced the self-attention mechanism that revolutionized natural language processing.

---

### Reflection on Self-Attention

Before moving to our next knowledge point, let's take a moment to reflect on what we've learned about self-attention:

1. **Problem Solved**: Self-attention addresses the context challenge by allowing each token to directly attend to all other tokens in the sequence.

2. **Key Mechanism**: The query-key-value framework creates a dynamic, content-based attention system.

3. **Advantages**:
   - Works with sequences of any length
   - Captures long-range dependencies efficiently
   - Processes all tokens in parallel

However, a single attention mechanism has limitations. Consider this sentence: "The trophy doesn't fit in the suitcase because it's too big."

What does "it" refer to? The trophy or the suitcase? Depending on the context, either could make sense. Different types of relationships (syntactic, semantic, referential) might require different attention patterns.

This brings us to our next knowledge point: how can we enable the model to attend to the sequence in multiple different ways simultaneously?

## Knowledge Point 4: Multi-Head Attention

### Core Concepts (For Everyone)

**Probing Question**: Why might it be helpful for a model to have multiple different ways of computing attention over the same sequence?

#### The Limitation of Single Attention

Language has many types of relationships:
- Subject-verb relationships ("The dog barks")
- Object relationships ("Threw the ball")
- Grammatical agreement ("These books are...")
- Pronouns and their references ("She said she...")

A single attention mechanism might struggle to capture all these different relationship types simultaneously.

> **Everyday Analogy**: Imagine trying to analyze a painting with just one perspective. You might focus on the colors, or the composition, or the subjects - but it's hard to see all aspects at once. Multi-head attention is like having multiple art critics looking at the same painting, each focusing on different aspects.

#### The Multi-Head Solution

Multi-head attention runs multiple attention operations in parallel:

1. **Multiple Sets of Projections**:
   - Instead of one set of Q/K/V projections, create several sets
   - Each "head" gets its own projection matrices
   
2. **Parallel Processing**:
   - Each head computes attention independently
   - Different heads can specialize in different patterns

3. **Combine Results**:
   - Concatenate the outputs from all heads
   - Project back to the original dimension

#### Specialized Patterns Emerge

In trained models, different attention heads often specialize in different linguistic patterns:
- Some track subject-verb relationships
- Others focus on object relationships
- Some detect entity references (like pronouns and their antecedents)
- Certain heads might focus on positional patterns

**Understanding Check ✓**
- Why is having multiple attention heads better than a single, larger attention mechanism?
- How might different heads specialize in different types of linguistic relationships?

### Hands-On Implementation (For CS Students)

#### Implementing Multi-Head Attention

Here's how to implement multi-head attention in PyTorch:

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class MultiHeadAttention(nn.Module):
    def __init__(self, d_model, num_heads):
        super().__init__()
        self.d_model = d_model
        self.num_heads = num_heads
        self.head_dim = d_model // num_heads
        
        assert d_model % num_heads == 0, "d_model must be divisible by num_heads"
        
        # Create projections for all heads at once
        self.q_linear = nn.Linear(d_model, d_model)
        self.k_linear = nn.Linear(d_model, d_model)
        self.v_linear = nn.Linear(d_model, d_model)
        
        # Output projection
        self.out_proj = nn.Linear(d_model, d_model)
        
    def forward(self, x):
        batch_size, seq_len, d_model = x.size()
        
        # Create Q, K, V projections for all heads
        q = self.q_linear(x)  # [batch_size, seq_len, d_model]
        k = self.k_linear(x)  # [batch_size, seq_len, d_model]
        v = self.v_linear(x)  # [batch_size, seq_len, d_model]
        
        # Reshape for multi-head attention
        q = q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        k = k.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        v = v.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)
        # Now shapes are [batch_size, num_heads, seq_len, head_dim]
        
        # Compute attention scores
        scores = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.head_dim).float())
        # scores shape: [batch_size, num_heads, seq_len, seq_len]
        
        # Apply softmax to get attention weights
        attention_weights = F.softmax(scores, dim=-1)
        # weights shape: [batch_size, num_heads, seq_len, seq_len]
        
        # Apply attention weights to values
        output = torch.matmul(attention_weights, v)
        # output shape: [batch_size, num_heads, seq_len, head_dim]
        
        # Reshape back to original dimensions
        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)
        # output shape: [batch_size, seq_len, d_model]
        
        # Final projection
        output = self.out_proj(output)
        
        return output, attention_weights

# Example usage
d_model = 64
num_heads = 4
multi_head_attn = MultiHeadAttention(d_model, num_heads)

# Create a sample input
batch_size = 2
seq_len = 5
x = torch.randn(batch_size, seq_len, d_model)

# Pass through multi-head attention
output, weights = multi_head_attn(x)
print(f"Output shape: {output.shape}")
print(f"Attention weights shape: {weights.shape}")
```

**Practice Exercise**: Visualize the attention patterns from different heads and observe how they differ.

### Advanced Theory (For the Curious)

#### Mathematical Formulation of Multi-Head Attention

Multi-head attention is defined as:

$\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W^O$

Where each head is:

$\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$

The parameters for each head are:
- $W_i^Q \in \mathbb{R}^{d_{\text{model}} \times d_k}$
- $W_i^K \in \mathbb{R}^{d_{\text{model}} \times d_k}$
- $W_i^V \in \mathbb{R}^{d_{\text{model}} \times d_v}$
- $W^O \in \mathbb{R}^{hd_v \times d_{\text{model}}}$

Where typically $d_k = d_v = d_{\text{model}}/h$.

#### Attention Head Specialization

Research has shown that different heads specialize in different linguistic phenomena:
- Syntactic heads: Focus on grammatical structure
- Semantic heads: Focus on meaning relationships
- Positional heads: Attend to nearby tokens
- Background heads: Distribute attention broadly

This emergent specialization happens naturally during training and contributes to the model's overall language understanding capabilities.

**Research Reference**: "A Multiscale Visualization of Attention in the Transformer Model" (Vig, 2019) provides visualizations of how different attention heads specialize.

---

## Knowledge Point 5: Combining Attention with Feed-Forward Networks into Transformer Blocks

### Core Concepts (For Everyone)

**Probing Question**: How do all these components come together to form the fundamental unit of transformers?

#### The Transformer Block: Combining Strengths

We now have two powerful mechanisms with complementary strengths:

1. **Multi-Head Self-Attention**:
   - Finds relevant connections across the sequence
   - Creates contextually enriched representations

2. **Feed-Forward Networks**:
   - Store factual knowledge
   - Apply computation to individual positions

The transformer block combines these into a standard unit that serves as the building block of modern language models.

> **Everyday Analogy**: Think of attention as research (finding relevant information from many sources) and the feed-forward network as reasoning (applying knowledge to process that information). The transformer block combines research and reasoning into a single thinking step.

#### The Standard Block Architecture

A standard transformer block arranges these components sequentially:

1. **Layer Normalization** → **Multi-Head Attention** → **Residual Connection**
2. **Layer Normalization** → **Feed-Forward Network** → **Residual Connection**

Additional components provide stability:
- **Layer Normalization**: Standardizes representations to keep training stable
- **Residual Connections**: Add the input to the output of each component (like a shortcut)

#### The Power of This Combination

This arrangement creates a system more powerful than either component alone:
- Attention provides context-aware representations
- FFNs apply knowledge and computation
- Layer normalization maintains stability
- Residual connections help information flow

**Understanding Check ✓**
- Why is the combination of attention and feed-forward networks more powerful than either component alone?
- What role do residual connections play in the transformer block?

### Hands-On Implementation (For CS Students)

#### Implementing a Transformer Block

Here's how to implement a complete transformer block in PyTorch:

```python
import torch
import torch.nn as nn

class TransformerBlock(nn.Module):
    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):
        super().__init__()
        # Multi-head attention
        self.attention = MultiHeadAttention(d_model, num_heads)
        # Feed-forward network
        self.feed_forward = FeedForward(d_model, d_ff, dropout)
        # Layer normalization
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        # Dropout
        self.dropout = nn.Dropout(dropout)
        
    def forward(self, x):
        # Apply first layer norm
        norm_x = self.norm1(x)
        
        # Apply attention
        attn_output, _ = self.attention(norm_x)
        attn_output = self.dropout(attn_output)
        
        # First residual connection
        x = x + attn_output
        
        # Apply second layer norm
        norm_x = self.norm2(x)
        
        # Apply feed-forward network
        ff_output = self.feed_forward(norm_x)
        ff_output = self.dropout(ff_output)
        
        # Second residual connection
        x = x + ff_output
        
        return x

# Example usage
d_model = 64
num_heads = 4
d_ff = 256
transformer_block = TransformerBlock(d_model, num_heads, d_ff)

# Create a sample input
batch_size = 2
seq_len = 5
x = torch.randn(batch_size, seq_len, d_model)

# Pass through transformer block
output = transformer_block(x)
print(f"Output shape: {output.shape}")
```

**Practice Exercise**: Implement a stack of transformer blocks and observe how information flows through the network.

### Advanced Theory (For the Curious)

#### Block Variants and Architecture Choices

While the standard transformer block is powerful, researchers have explored various modifications:

1. **Pre-norm vs. Post-norm**:
   - Pre-norm (described above): Normalize before each component
   - Post-norm: Normalize after each component and residual connection
   - Pre-norm tends to be more stable for training very deep models

2. **Gated Mechanisms**:
   - Some architectures add gating mechanisms to control information flow
   - Similar to LSTMs but applied to transformers

3. **Architecture Efficiency**:
   - Performer, Linformer, and other variants modify attention for computational efficiency
   - Some replace standard attention with approximations that scale better to long sequences

#### Computation Complexity Analysis

The standard transformer block has two computational bottlenecks:

1. **Self-Attention**:
   - Time complexity: O(n²d) where n is sequence length and d is dimension
   - Memory complexity: O(n²) for storing attention weights

2. **Feed-Forward Network**:
   - Time complexity: O(nd²) where d is typically large (4x the model dimension)
   - Memory complexity: O(nd)

For very long sequences, the quadratic scaling of attention becomes prohibitive, leading to research on more efficient attention mechanisms.

**Research Reference**: "Transformers: State-of-the-Art Natural Language Processing" (Wolf et al., 2020) provides a comprehensive overview of transformer architectures and variants.

---

## Reflection and Synthesis

### Integration Activity (For Everyone)

Let's connect the key concepts we've explored in this session:

1. **Knowledge Storage**: Feed-forward networks act as key-value memories, storing factual knowledge using an expand-contract architecture.

2. **Context Finding**: Self-attention directly models relationships between all positions in a sequence, solving the long-range dependency problem.

3. **Multi-Faceted Understanding**: Multi-head attention enables specialized attention patterns for different types of relationships.

4. **The Transformer Block**: By combining multi-head attention with feed-forward networks, transformers create a powerful computational unit that can both find relevant context and apply stored knowledge.

These components work together to create a system that:
- Efficiently processes language in parallel
- Handles long-range dependencies
- Stores and applies factual knowledge
- Adapts to different types of linguistic relationships

### Key Questions to Consider

- How does the transformer architecture address the limitations of previous approaches?
- What specific advantages does self-attention have over sequential processing methods?
- How do the complementary strengths of attention and feed-forward networks create a more powerful system when combined?

### Next Steps Preview

In the next session, we'll explore how transformer blocks are stacked to create deeper networks using residual connections, and we'll examine the complete training process for modern LLMs, from pre-training on raw text to instruction tuning.

---

## Additional Resources

### Visualizations
- [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)
- [Attention Visualization Tool](https://github.com/jessevig/bertviz)

### Research References
- Vaswani, A., et al. (2017). "Attention Is All You Need."
- Geva, M., et al. (2022). "Transformer Feed-Forward Layers Are Key-Value Memories."
- Vig, J. (2019). "A Multiscale Visualization of Attention in the Transformer Model."

### Code Tutorials
- [PyTorch Transformer Implementation](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)
- [The Annotated Transformer](https://nlp.seas.harvard.edu/2018/04/03/attention.html)
