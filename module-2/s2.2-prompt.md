# Session 2.2: Feed-Forward Networks and Attention Mechanisms

## Primary Question
How do transformers combine knowledge storage with contextual awareness to process language effectively?

## Knowledge Points Overview
This session covers the following key knowledge points:
1. Feed-forward networks as knowledge stores
2. The challenge of finding relevant context
3. Self-attention mechanisms
4. Multi-head attention
5. Combining attention with feed-forward networks into transformer blocks

---

## Knowledge Point 1: Feed-Forward Networks as Knowledge Stores

**Probing Question:** How might a neural network store and retrieve factual knowledge that isn't explicitly present in the input text?

### Beyond Pattern Recognition to Knowledge Storage

In our previous session, we explored how tokens are embedded and position information is encoded. These representations allow us to process sequences, but to generate meaningful responses, a model needs access to factual knowledge.

Consider these examples:
- "The capital of France is ____."
- "Water boils at ____ degrees Celsius."
- "Shakespeare wrote ____."

Answering these requires knowledge beyond the immediate context. While early language models like Bengio's Neural Language Model could process text sequences, they lacked an efficient mechanism for storing and retrieving large amounts of factual knowledge.

### Feed-Forward Networks as Key-Value Stores

Modern transformer-based LLMs store knowledge primarily in feed-forward networks (FFNs). These networks typically have a specific structure:

1. **Expansion Layer**: Projects inputs into a much higher-dimensional space
   - Example: From 768 dimensions to 3072 dimensions in BERT
   
2. **ReLU Activation**: Creates sparse activation patterns
   - Only some neurons activate for any given input
   
3. **Contraction Layer**: Projects back to the model's hidden dimension
   - Compresses the activated patterns into a dense representation

This expand-contract architecture enables FFNs to function as associative memories:
- The first layer and ReLU activation effectively select "keys"
- The second layer retrieves associated "values"

### Universal Approximation Theorem

The theoretical foundation for how FFNs can store knowledge comes from the Universal Approximation Theorem (Hornik et al., 1989), which states that feedforward networks with a single hidden layer and sufficient width can approximate any continuous function to arbitrary precision.

In the context of language models:
- The "function" being approximated is the mapping from contexts to relevant facts
- The wider the hidden layer, the more facts the network can store
- Modern LLMs use extremely wide FFNs for this reason (up to 4x the model dimension)

### Understanding Check ✓
- How does the expand-contract structure of FFNs enable knowledge storage?
- Why isn't this factual knowledge simply stored in the token embeddings?

**Navigation Options**:
1. Proceed to Knowledge Point 2
2. Explore more about knowledge storage in FFNs
3. See visualization of FFN knowledge activation
4. Request practice exercises

---

## Knowledge Point 2: The Challenge of Finding Relevant Context

**Probing Question:** If we have token embeddings, position information, and stored knowledge in FFNs, what obstacle remains in processing language effectively?

### From Neural Language Models to Transformers

With the components we've discussed so far, we could build a language model similar to Bengio's Neural Language Model (which we explored in Module 1). This model would:
1. Embed tokens into vectors
2. Encode their positions
3. Process each position with an FFN to apply knowledge

However, such a model would face a significant limitation: **it would have no efficient way to determine which parts of the context are most relevant when processing each token**.

Consider this example:
"The cat, which had been sleeping on the couch in the living room next to the large window overlooking the garden with colorful flowers, suddenly jumped when it heard a noise."

- For predicting what follows "jumped," which earlier words matter most?
- How should a model determine that "cat" is more relevant than "flowers"?
- What if relevant information appears many tokens earlier in a very long document?

Traditional approaches like RNNs process text sequentially, causing information to fade as the distance increases. This creates challenges when handling long-range dependencies - critical relationships between words that are far apart in the text.

### Interactive Activity

Let's identify what information is needed to predict the blank in these sentences:

1. "John dropped the glass and it ____."
2. "After studying all night, Maria felt ____ during her exam."
3. "The company that was founded in Seattle in 1994 and specializes in online retail sales is ____."

For each example, try to identify:
- Which specific words provide the most important clues
- How far these clues are from the blank
- What type of relationship exists between these clues and the blank

### Understanding Check ✓
- Why might fixed-window approaches (like looking at only the previous 5 words) be insufficient?
- How does the distance between related words create challenges for language models?

**Navigation Options**:
1. Proceed to Knowledge Point 3
2. Explore more examples of context challenges
3. Request practice exercises

---

## Knowledge Point 3: Self-Attention Mechanisms

**Probing Question**: How might a model determine the importance of different words in a sequence when processing each position?

### The Key Insight of Attention

The key innovation of attention is to create a mechanism that directly models relationships between all positions in a sequence. Rather than relying on fixed windows or sequential processing, attention computes a set of weights that determine how much each position should influence the representation of the current token.

Let's examine the self-attention mechanism in detail:

1. **Query, Key, Value Concept**: 
   - Each token position generates three different vectors:
     - **Query (Q)**: What the current position is looking for
     - **Key (K)**: What each position offers to others
     - **Value (V)**: The actual content at each position

2. **Computing Attention Weights**:
   - For each position's query, we compute its compatibility with all keys
   - Mathematically: $\text{Score}(Q_i, K_j) = \frac{Q_i \cdot K_j}{\sqrt{d_k}}$ 
   - The division by $\sqrt{d_k}$ (where $d_k$ is the dimension of the keys) helps stabilize gradients

3. **Softmax Normalization**:
   - Convert scores to probabilities using the softmax function
   - This ensures all attention weights sum to 1

4. **Weighted Aggregation**:
   - Combine all values using these weights
   - Output: $\text{Attention}(Q, K, V) = \text{softmax}(\frac{QK^T}{\sqrt{d_k}})V$

### Visual Exploration

Let's visualize attention in a simple example:
"The cat sat on the mat."

When processing "sat," the attention mechanism might give:
- High weight to "cat" (subject-verb relationship)
- Lower weights to "the" and "on"
- Minimal weights to distant or less relevant tokens

### Understanding Check ✓
- How does attention solve the challenge of capturing long-range dependencies?
- What determines which tokens receive higher attention weights when processing a specific position?

**Navigation Options**:
1. Proceed to Knowledge Point 4
2. Explore the attention mechanism in more detail
3. See visualization of attention weights
4. Request practice exercises

---

## Knowledge Point 4: Multi-Head Attention

**Probing Question**: Why might it be helpful for a model to have multiple different ways of computing attention over the same sequence?

### Beyond Single Attention

While basic attention allows the model to weigh different positions, language has many types of relationships: syntactic, semantic, reference-based, and more. A single attention mechanism might struggle to capture all these relationship types simultaneously.

Multi-head attention addresses this by running multiple attention operations in parallel, each potentially focusing on different relationship patterns:

1. **Multiple Projections**:
   - Instead of a single set of Q/K/V projections, we create multiple sets
   - Each "head" has its own projection matrices $W^Q_i$, $W^K_i$, and $W^V_i$
   
2. **Parallel Computation**:
   - Each head computes attention independently
   - This allows different heads to specialize in different patterns

3. **Combining Outputs**:
   - The outputs from all heads are concatenated
   - A final projection matrix combines these outputs

The full multi-head attention is calculated as:
$$\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \text{head}_2, ..., \text{head}_h)W^O$$

where each head is:
$$\text{head}_i = \text{Attention}(QW^Q_i, KW^K_i, VW^V_i)$$

### Specialized Attention Patterns

Research has shown that different attention heads in trained models tend to specialize in different linguistic patterns:

- Some heads track subject-verb relationships
- Others focus on object relationships
- Some detect entity references (like pronouns and their antecedents)
- Certain heads might focus on positional patterns

This specialization emerges naturally during training as each head optimizes for different aspects of the prediction task.

### Understanding Check ✓
- How does having multiple attention heads help capture different types of relationships?
- Why would concatenating the outputs from different heads provide a richer representation than a single attention mechanism?

**Navigation Options**:
1. Proceed to Knowledge Point 5
2. Explore attention head specialization
3. See visualization of multiple attention heads
4. Request practice exercises

---

## Knowledge Point 5: Combining Attention with Feed-Forward Networks into Transformer Blocks

**Probing Question**: How do all these components come together to form the fundamental unit of transformers?

### The Transformer Block

We now have two powerful mechanisms with complementary strengths:
1. **Feed-forward networks (FFNs)** that can store knowledge and act as key-value memories
2. **Multi-head self-attention** that can find relevant relationships across the sequence

The transformer architecture combines these into a standard unit called a "transformer block" or "transformer layer," which serves as the fundamental building block that gets repeated throughout the model.

### The Standard Transformer Block

A standard transformer block arranges these components sequentially:

1. **Multi-Head Attention Layer**:
   - Finds relevant connections across the sequence
   - Creates contextually enriched representations
   
2. **Feed-Forward Network Layer**:
   - Processes each position's representation independently
   - Applies stored knowledge and performs computation
   - Uses the expand-contract architecture we discussed earlier:
     $$\text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2$$

3. **Additional Critical Components**:
   - **Layer Normalization**: Stabilizes representations by normalizing their statistics
   - **Residual Connections**: Adds input directly to the output of each sub-layer

### Processing Flow Within a Block

The flow through a transformer block typically follows this pattern:
1. Apply layer normalization to the input
2. Process through multi-head attention
3. Add the original input (residual connection)
4. Apply layer normalization again
5. Process through the feed-forward network
6. Add the pre-FFN result (another residual connection)

Mathematically:
$$\begin{align}
x' &= \text{LayerNorm}(x) \\
x'' &= x + \text{MultiHeadAttention}(x') \\
x''' &= \text{LayerNorm}(x'') \\
\text{output} &= x'' + \text{FFN}(x''')
\end{align}$$

### The Power of This Combination

This combination creates a system more powerful than either component alone:

1. **Attention provides context**, determining which tokens are relevant
2. **FFNs provide knowledge and computation**, applying factual information
3. **Residual connections** enable information to flow easily through the network
4. **Layer normalization** keeps activations stable throughout training

Together, these components address the fundamental challenges of language modeling:
- Finding relevant context across long sequences
- Storing and retrieving factual knowledge
- Maintaining stable training dynamics

### Scaling Up: Stacking Blocks

The full transformer architecture consists of multiple blocks stacked on top of each other:
- GPT-3: 96 transformer blocks
- BERT-Large: 24 transformer blocks
- T5-Large: 24 transformer blocks

Each additional layer allows the model to perform more complex transformations, progressively refining its representations through repeated applications of attention and FFNs.

### Understanding Check ✓
- Why is the combination of attention and feed-forward networks more powerful than either component alone?
- What role do residual connections play in enabling deeper transformer networks?

**Navigation Options**:
1. Proceed to Reflection and Synthesis
2. Explore different transformer block variations
3. See visualization of stacked transformer blocks
4. Request practice exercises

---

## Reflection and Synthesis

**Integration Activity**: Let's connect the key concepts we've explored in this session:

1. **Knowledge Storage**: Feed-forward networks act as key-value memories, storing factual knowledge using the expand-contract architecture supported by the Universal Approximation Theorem.

2. **Neural LMs vs. Transformers**: Bengio's neural language model, which we studied in Module 1, provided a foundation for language modeling but lacked an effective way to model dependencies across long sequences.

3. **Contextual Awareness**: Self-attention directly models relationships between all positions in a sequence, solving the long-range dependency problem by computing dynamic, content-based weights.

4. **Multi-Faceted Understanding**: Multi-head attention enables parallel processing of different relationship types, allowing specialized attention patterns to emerge during training.

5. **The Transformer Block**: By combining multi-head attention with feed-forward networks, transformers create a powerful computational unit that can both find relevant context and apply stored knowledge.

**Key Questions to Consider**:
- How does the transformer architecture address the limitations of previous approaches to language modeling?
- What specific advantages does self-attention have over sequential processing methods like RNNs?
- How do the complementary strengths of attention and feed-forward networks create a more powerful system when combined?

**Next Steps Preview**: In the next session, we'll explore how transformer blocks are stacked to create deeper networks, and we'll examine the complete training process for modern LLMs, from pre-training on raw text to instruction tuning.

---

## Additional Resources

### Visualizations
- FFN activation patterns
- Attention weight heatmaps
- Information flow diagrams through transformer blocks

### Research References
- Hornik, K., Stinchcombe, M., & White, H. (1989). "Multilayer feedforward networks are universal approximators." Neural Networks, 2(5), 359-366.
- Vaswani, A., et al. (2017). "Attention Is All You Need." NeurIPS.
- He, K., et al. (2016). "Deep Residual Learning for Image Recognition." CVPR.
- Geva, M., et al. (2022). "Transformer Feed-Forward Layers Are Key-Value Memories." EMNLP.
