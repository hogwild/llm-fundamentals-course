# Session 2.1: Tokens and Positions in Transformer Models

## Primary Question
How do modern language models efficiently process text and maintain sequence information?

Through this exploration, you'll discover how modern transformers efficiently handle the complexity of language through tokenization rather than word-level processing, and how they preserve positional information despite parallel processing.

## Knowledge Points Overview
This session covers the following key knowledge points:
1. From words to tokens: Power laws and subword tokenization
2. Position embeddings: Preserving sequence information
3. Combined inputs: Tokens and positions working together

---

## Knowledge Point 1: From Words to Tokens: Power Laws and Subword Tokenization

**Probing Question**: Early language models used whole words as their basic unit, while modern LLMs use "tokens" instead. What do you think might be the limitations of word-based approaches that led to this shift?

### Power Laws and the Long Tail of Language

- **Key Insight**: Natural language follows power law distributions - a small number of words occur extremely frequently, while a vast number of words appear rarely
- **Exploration Activity**: Consider these distributions:
  * In English, the 100 most common words comprise about 50% of all text
  * The next 1,000 words might account for another 25%
  * The remaining 25% is distributed across hundreds of thousands of rare words
  * This pattern is known as Zipf's Law in linguistics

- **Visual Reference**: [Optional] Examine a graph comparing normal distributions vs. power law distributions in language.

- **Power Law Properties**:
  * Small number of very common items (the "head")
  * Vast number of rare items (the "long tail")
  * Mathematically: frequency ∝ rank^(-α)
  * Self-similar at different scales (fractal-like behavior)

- **Examples Beyond Words**:
  * City populations (few megacities, many small towns)
  * Website popularity (few sites get most traffic)
  * Social media influence (few accounts have millions of followers)

- **Beyond Simple Words**:
  * Knowledge complexity follows similar distributions
  * Simple facts appear frequently in text
  * Complex, specialized knowledge appears rarely
  * Domain-specific terminology follows its own power laws

### The Vocabulary Problem

- **Word-Level Model Challenges**:
  * **Dilemma**: Choose between limited vocabulary (missing rare words) or enormous vocabulary (inefficient)
  * **Unknown Word Problem**: Rare words replaced with `<UNK>` token, losing information
  * **Variant Problem**: "walk," "walks," "walking" treated as completely separate entities
  * **Creation Problem**: New words constantly emerging ("COVID," "doomscrolling")
  * **Compound Problem**: German and some other languages create compound words freely

- **Interactive Activity**: Consider how these words would be handled in a word-level model with a 50,000-word vocabulary:
  * Common: "the," "of," "and" (easily included)
  * Medium: "antidote," "blockchain" (might be included)
  * Rare: "electrocephalogram," "teleconferencing" (likely excluded)
  * New: "COVID19," "NFT" (definitely excluded)
  * Variants: "walking," "walks," "walker" (treated as separate vocabulary entries)

- **Visual Reference**: [Optional] View a diagram showing how the long tail of language creates inefficiencies in word-level models.

### Subword Tokenization: A Power Law Solution

- **Key Insight**: Subword tokenization aligns with the natural power law distribution of language
- **Byte Pair Encoding (BPE) Process**:
  * Start with character-level tokenization
  * Count frequency of adjacent pairs
  * Merge most frequent pair into a new token
  * Repeat until desired vocabulary size is reached
  * Result: common words are single tokens, rare words are multiple subword tokens

- **BPE Advantages**:
  * Common words remain efficient (single token)
  * Rare words broken into common subword components
  * New words handled compositionally
  * Morphologically related words share tokens
  * Optimal balance of vocabulary size and coverage

- **Interactive Demonstration**: Let's see how BPE would tokenize across the frequency spectrum:
  * Common words: "the", "of", "and" → single tokens
  * Medium frequency: "walking" → "walk" + "ing"
  * Rare words: "antidisestablishmentarianism" → "anti" + "dis" + "establish" + "ment" + "arian" + "ism"
  * Never-before-seen words: "COVID19" → "C" + "O" + "V" + "ID" + "19"

- **Visual Reference**: [Optional] Look at a visualization of BPE in action, showing how tokens are merged iteratively based on frequency.

### Understanding Check ✓
- How does subword tokenization naturally align with the power law distribution of language?
- Why might splitting rare or complex words into subword units help the model handle new words it hasn't seen before?
- How does BPE address both efficiency (for common words) and coverage (for rare words)?

**Navigation Options**:
1. Proceed to Knowledge Point 2
2. Explore more tokenization examples
3. See visualization of BPE process
4. Request practice exercises

---

## Knowledge Point 2: Position Embeddings: Preserving Sequence Information

**Probing Question**: In the models we've explored so far, position information was implicit based on the order of processing. How might a model that processes all tokens simultaneously maintain information about token position?

### The Sequence Challenge
- **Key Insight**: Transformers process all tokens in parallel, requiring explicit position information
- **Exploration Activity**: Consider what happens when word order changes:
  * "The dog chased the cat" vs. "The cat chased the dog"
  * Same words, different meaning based solely on position
  * "Time flies like an arrow" vs. "An arrow flies like time"
  * Syntactic ambiguity resolved through position

- **The Problem**:
  * Without position information, "The dog chased the cat" would be indistinguishable from "The cat chased the dog"
  * Order matters for syntax, semantics, reference resolution, and more
  * Solution: add explicit position information to each token

- **Visual Reference**: [Optional] View a diagram showing how position needs to be explicitly encoded when processing tokens in parallel.

### Position Encoding Approaches

- **Absolute Position Encodings**:
  * Each position in the sequence gets a unique vector representation
  * Position 1 → [0.1, 0.7, -0.3, ...]
  * Position 2 → [0.2, 0.5, -0.1, ...]
  * And so on...

- **Learned vs. Fixed Position Embeddings**:
  * **Learned**: Model updates position representations during training
  * **Fixed**: Mathematical functions determine position representations

- **Focus on Sinusoidal Encodings**:
  * Original transformer approach (Vaswani et al., 2017)
  * Use sine and cosine functions of different frequencies
  * Position in even dimensions: sin(pos/10000^(2i/d))
  * Position in odd dimensions: cos(pos/10000^(2i/d))

- **Sinusoidal Encoding Properties**:
  * Creates unique pattern for each position
  * Preserves relative distances (similar positions have similar encodings)
  * Generalizes to sequence lengths not seen during training
  * Enables efficient calculation of relative positions

- **Visual Reference**: [Optional] Examine a heatmap visualization of sinusoidal position encodings, showing how each position gets a unique fingerprint and how patterns emerge across the embedding dimensions.

### Understanding Check ✓
- Why is explicit position information necessary when processing tokens in parallel?
- How do sinusoidal functions help create unique representations for different positions?
- What advantages might sinusoidal encodings have over simple learned position embeddings?

**Navigation Options**:
1. Proceed to Knowledge Point 3
2. Explore more about position encodings
3. See visualization of position encoding patterns
4. Request practice exercises

---

## Knowledge Point 3: Combined Inputs: Tokens and Positions Working Together

**Probing Question**: Now that we've explored tokens and positions separately, how do you think these elements might work together to form the input to a transformer?

### Integrating Multiple Information Sources
- **Key Insight**: Transformer inputs combine token identity and position information through a simple but powerful mechanism
- **Exploration Activity**: Consider the information needed to understand each token in this sentence:
  * "After visiting Paris, she returned to her home in London"
  * What does each token mean? (token identity)
  * Where does it appear in the sequence? (position)

- **The Integration Problem**:
  * Need to represent both what a token is and where it appears
  * Must preserve both types of information without loss
  * Should be computationally efficient
  * Must enable parallel processing

- **Solution: Vector Addition**
  * Token embeddings: Represent the identity of each token
  * Position embeddings: Represent the position in the sequence
  * Combined through simple vector addition: input = token_embedding + position_embedding

- **Mathematical Representation**:
  * For a token t at position p: x_t,p = E_t + P_p
  * Where E_t is the token embedding
  * P_p is the position embedding
  * x_t,p is the combined input representation

- **Properties of Combined Representations**:
  * Simple and computationally efficient
  * Both token and position information influence each dimension
  * Preserves the fundamental structure of both embeddings
  * Establishes the foundation for subsequent processing steps

- **Visual Reference**: [Optional] View a diagram showing how token embeddings and position embeddings are combined through vector addition to create the input to the transformer.

### Understanding Check ✓
- Why might simple vector addition be sufficient to combine token and position information?
- How does this combined representation enable the parallel processing that will be discussed in future sessions?
- What advantages might this combined representation have over the approaches we studied in Module 1?

**Navigation Options**:
1. Proceed to Reflection
2. Explore more about representation combination
3. See visualization of information integration
4. Request practice exercises

---

## Reflection and Synthesis

**Integration Activity**: Summarize how the concepts from this session work together to enable modern transformer architectures to process language effectively:

1. How do subword tokens address the challenges posed by power law distributions in language?
2. How do transformers preserve sequential information when processing tokens in parallel?
3. How do token and position information combine to create the foundation for transformer processing?

**Key Questions to Consider**:
- How do these mechanisms differ from the language models we explored in Module 1?
- What advantages might these approaches provide for handling complex language tasks?
- How do these elements prepare us to understand the full transformer architecture?
- What questions do you still have about how tokens and positions work in transformers?

**Next Steps Preview**: In the next session, we'll explore the core innovation of transformer models—attention mechanisms—and how they allow tokens to interact with each other to create context-aware representations. We'll also discover how feed-forward networks function within the transformer architecture.

---

## Additional Resources

### Visualizations
- Tokenization process illustrations
- Power law distribution graphs
- Position encoding heatmaps
- Token and position combination diagrams

### Research References
- Vaswani, A., et al. (2017). Attention Is All You Need.
- Sennrich, R., Haddow, B., & Birch, A. (2016). Neural Machine Translation of Rare Words with Subword Units.
- Devlin, J., et al. (2019). BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.
- Press, O., & Wolf, L. (2017). Using the Output Embedding to Improve Language Models.

### Interactive Tools
- Tokenization visualization tools
- Position encoding demonstrations
- BPE algorithm interactive examples
