# Session 1.2: Building Your First N-gram Predictor

## Primary Question:
How can we build the simplest form of text predictor without using machine learning models, and what are its limitations?

Through hands-on coding and interactive exploration, you'll build a simple n-gram predictor and experience firsthand why more sophisticated approaches are needed.

## Part 1: Understanding N-grams Conceptually
- Interactive: First, try to predict the next word in these sentences:
  * "The cat sat on the ___"
  * "I drink coffee every ___"
  * "The sun rises in the ___"
- Discuss: What information did you use to make these predictions?
- Now, explain in your own words what you think an n-gram might be
- Ask the AI to confirm or clarify your understanding
- Work together to define what a bigram predictor should do

## Part 2: Designing Before Coding
- Draw or describe (without code) how you'd approach building a word predictor
- Key questions to answer together:
  * What information do we need to track?
  * How would we store that information?
  * How would we use it to make predictions?
- Ask the AI for feedback on your design approach
- Identify potential challenges before writing any code

## Part 3: Building Incrementally
### Step 1: Basic Tokenization
- First challenge: How do we split text into words?
- Try it yourself first, then discuss with the AI
- Test with simple sentences
- Discuss: What issues might we face with punctuation, contractions, or special characters?

### Step 2: Counting Words
- How would you count word occurrences?
- Try to implement a simple counter
- Check your understanding: What data structure would work best?
- **Checkpoint**: Explain why you chose this data structure

### Step 3: Tracking Word Pairs
- Design: How would you store information about which words follow which?
- Draw or describe your data structure
- Implement and test with a small example
- Verify: Does your structure capture what we need?
- **Checkpoint**: Demonstrate tracking bigrams with "The cat sat on the mat"

### Step 4: Making Predictions
- Given a word, how would you predict the next one?
- Consider: How do we handle multiple possible next words?
- Implement probability calculation
- Test with examples and discuss results
- **Checkpoint**: Calculate P(mat|the) from your training data

## Part 4: Understanding Evaluation
- Before coding evaluation, discuss:
  * What makes a prediction "good" or "bad"?
  * How would you measure success?
  * What does accuracy mean in this context?
- Design an evaluation approach together
- Implement and test it
- Interpret the results: Why might accuracy be low? Is that expected?
- Key concepts to understand:
  * **Training vs. Test Data**: Why we need separate datasets
  * **Evaluation Metrics**: Accuracy and its limitations
  * **Baseline Performance**: What's a reasonable expectation?

## Part 5: Exploring Limitations Through Experimentation
- Test with different types of text
- Try to break your predictor:
  * What happens with words it hasn't seen?
  * What about rare word combinations?
  * How does it handle longer texts?
- Compare bigrams vs trigrams
- Document the limitations you discover

## Part 6: Understanding Core ML Concepts Through Examples
### Concept Exploration Exercises
1. **The Curse of Dimensionality**
   - Task: Create trigram and 4-gram predictors
   - Observe: How does accuracy change? Why does performance degrade?
   - Discover: The exponential growth problem

2. **Sparsity Problem**
   - Task: Count how many unique bigrams vs. possible bigrams exist
   - Calculate: What percentage of possible combinations appear in data?
   - Understand: Why most theoretical combinations never occur

3. **Context Window Limitations**
   - Task: Try to predict words that clearly depend on earlier context
   - Example: "In ancient Rome, unlike modern times, people would..."
   - Realize: Why fixed-size context fails for long-range dependencies

### Checkpoint Discussion
Before moving to reflection, verify understanding of:
- Why larger n leads to worse performance (curse of dimensionality)
- How sparsity affects prediction quality
- Why context windows need to be flexible
- The trade-off between pattern memorization and generalization

## Part 7: Reflection and Bridge to Neural Networks
- Summarize what you've learned about n-gram predictors
- List the key limitations you experienced firsthand
- Consider: What specific problems would you want a better solution to solve?
- Predict: How might embeddings or neural networks address these issues?

## Key Concepts to Master

### Basic (Essential Understanding):
1. **Tokenization**: How text is broken into processable units
2. **Conditional Probability**: P(next_word | previous_words) and its calculation
3. **Training vs. Inference**: How the model learns patterns and makes predictions
4. **Model Evaluation**: What accuracy means and its limitations
5. **Out-of-Distribution Problem**: Why unseen words break the predictor

### Medium (Through Examples):
1. **Curse of Dimensionality**: Why larger n-grams become impractical
2. **Sparsity**: Why most word combinations never appear in training
3. **Context Window Limitations**: Fixed vs. variable context understanding
4. **Data Structure Efficiency**: Using dictionaries and nested structures
5. **Frequency-Based Prediction**: Why common patterns dominate

### Advanced (For Further Exploration):
1. **Smoothing Techniques**: Handling zero probabilities
2. **Backoff Models**: Gracefully degrading to shorter n-grams
3. **Perplexity**: A better metric for language models
4. **Language Modeling Fundamentals**: Theoretical foundations
5. **Statistical vs. Semantic**: Why embeddings capture meaning

## Knowledge Checkpoints Throughout the Session

### After Part 1 (Understanding N-grams):
- Quick quiz: "Explain in your own words what a bigram is"
- Practice: "Given 'The cat sat on the mat', list all the bigrams"
- Check: Can student identify the prediction task correctly?

### After Part 3 (Building Incrementally):
- Code review: Have student explain each function's purpose
- Test understanding: "What would happen if we encounter a new word?"
- Verify: Can student trace through the code logic?

### After Part 4 (Evaluation):
- Concept check: "Why might 30% accuracy be expected for this task?"
- Application: "How would you calculate accuracy for your predictor?"
- Understanding: "Why do we need separate training and test data?"

### After Part 6 (Core ML Concepts):
- Synthesis: "Explain why larger n-grams don't necessarily perform better"
- Connection: "How does sparsity relate to the need for embeddings?"
- Forward thinking: "What limitations would you want to overcome?"

## AI Assistant Guidelines
Throughout this session, the AI should:
- Ask probing questions before providing answers
- Encourage students to try things themselves first
- Help students discover limitations through experimentation
- Guide without giving away solutions too quickly
- Check for understanding at each step
- Ensure key ML concepts are understood through hands-on experience

## Code Template to Get Started:
```python
# Start with this skeleton and build up gradually
class SimpleBigramPredictor:
    def __init__(self):
        # What do we need to store?
        pass
    
    def train(self, text):
        # How do we process the training data?
        pass
    
    def predict(self, word):
        # How do we make predictions?
        pass
    
    def evaluate(self, test_text):
        # How do we measure performance?
        pass

# The journey begins with understanding, not implementing
```

By the end of this session, you should have:
- A working n-gram predictor you built step-by-step
- A clear understanding of its limitations through personal experience
- Insights into why we need more sophisticated approaches
- Questions that naturally lead to the next concepts in machine learning
