# Session 1.2: Building Your First N-gram Predictor

## Primary Question
How can we build the simplest form of text predictor without using machine learning models, and what are its limitations?

Through hands-on coding and interactive exploration, you'll build a simple n-gram predictor and experience firsthand why more sophisticated approaches are needed.

## Knowledge Points Overview
This session covers the following key knowledge points:
1. N-grams as a conceptual foundation for text prediction
2. Design principles for statistical language models
3. Tokenization and text preprocessing
4. Statistical tracking and storage structures
5. Prediction implementation and probability calculation
6. Model evaluation techniques
7. Fundamental limitations and machine learning concepts

---

## Knowledge Point 1: N-grams as a Conceptual Foundation

**Probing Question**: Before we dive into implementation, how do you think humans predict the next word in a sentence? What information are we using?

### Understanding N-grams Conceptually
- **Interactive Activity**: Try to predict the next word in these sentences:
  * "The cat sat on the ___"
  * "I drink coffee every ___"
  * "The sun rises in the ___"

- **Discussion Points**:
  * What information did you use to make these predictions?
  * How many previous words did you consider?
  * Are some words more predictable than others? Why?

- **Key Concept**: N-grams are sequences of n adjacent words in text
  * Unigrams: single words ("cat", "sat", "on")
  * Bigrams: pairs of words ("the cat", "cat sat", "sat on")
  * Trigrams: triplets of words ("the cat sat", "cat sat on")

- **Visual Reference**: [Optional] Examine a visual representation of n-grams and their frequencies from Jurafsky & Martin's "Speech and Language Processing" textbook.

### Understanding Check ✓
- What is an n-gram, in your own words?
- Given the sentence "The cat sat on the mat", list all the bigrams.
- Why might we want to track sequences of words rather than individual words?

**Navigation Options**:
1. Proceed to Knowledge Point 2
2. Explore more examples of n-grams
3. See visualization of n-gram frequency distributions
4. Request practice exercises

---

## Knowledge Point 2: Design Principles for Statistical Language Models

**Probing Question**: If you were designing a system to predict the next word in a text, what information would you need to track, and how would you organize it?

### Designing Before Coding
- **Conceptual Activity**: Draw or describe (without code) your approach to building a word predictor
  * What information do we need to track?
  * How would we store that information?
  * How would we use it to make predictions?

- **Key Decisions**:
  * How to represent words (tokens)
  * How to count and store frequency information
  * How to calculate and use probabilities
  * How to handle unseen words or combinations

- **Design Principle**: A bigram model predicts the next word based on the current word
  * P(next_word | current_word) = Count(current_word, next_word) / Count(current_word)

- **Visual Reference**: [Optional] Look at a system diagram showing the components of a statistical language model from statistical NLP literature.

### Understanding Check ✓
- Why is it important to design before coding?
- What challenges do you anticipate in implementing a bigram predictor?
- How would your design change if you wanted to build a trigram predictor instead?

**Navigation Options**:
1. Proceed to Knowledge Point 3
2. Explore more design considerations
3. See example architecture diagrams
4. Request practice exercises

---

## Knowledge Point 3: Tokenization and Text Preprocessing

**Probing Question**: What challenges might we face when converting raw text into processable word units (tokens)?

### Basic Tokenization
- **Implementation Challenge**: How do we split text into words?
  * Simple approach: `text.split()`
  * Considerations: punctuation, capitalization, special characters

- **Code Implementation**:
```python
def tokenize(text):
    # Simple tokenization
    return text.lower().split()

# Test with a simple sentence
sentence = "The cat sat on the mat."
tokens = tokenize(sentence)
print(tokens)
```

- **Discussion Points**:
  * What issues might we face with punctuation?
  * How should we handle contractions (e.g., "don't")?
  * What about special characters or numbers?

- **Advanced Tokenization**:
  * Regular expressions for more sophisticated tokenization
  * Handling edge cases
  * Language-specific considerations

- **Visual Reference**: [Optional] Examine a diagram showing tokenization challenges from NLP literature.

### Understanding Check ✓
- What is tokenization and why is it important?
- What are two challenges in tokenization that our simple approach doesn't handle well?
- How might tokenization affect our prediction quality?

**Navigation Options**:
1. Proceed to Knowledge Point 4
2. Explore more about tokenization techniques
3. See examples of tokenization edge cases
4. Request practice exercises

---

## Knowledge Point 4: Statistical Tracking and Storage Structures

**Probing Question**: What data structures would be most effective for counting and storing word occurrences and their relationships?

### Counting Words and Tracking Word Pairs
- **Implementation Challenges**:
  * How do we count individual word occurrences?
  * How do we track which words follow which?
  * What data structure is most appropriate?

- **Code Implementation**:
```python
def build_language_model(tokens):
    # Count individual words
    word_counts = {}
    for token in tokens:
        word_counts[token] = word_counts.get(token, 0) + 1
    
    # Track word pairs (bigrams)
    bigram_counts = {}
    for i in range(len(tokens) - 1):
        current_word = tokens[i]
        next_word = tokens[i+1]
        
        if current_word not in bigram_counts:
            bigram_counts[current_word] = {}
        
        bigram_counts[current_word][next_word] = bigram_counts[current_word].get(next_word, 0) + 1
    
    return word_counts, bigram_counts

# Test with our tokens
word_counts, bigram_counts = build_language_model(tokens)
print("Word counts:", word_counts)
print("Bigram counts:", bigram_counts)
```

- **Data Structure Analysis**:
  * Why nested dictionaries are effective for this task
  * Time and space complexity considerations
  * Alternatives and their trade-offs

- **Checkpoint**: Demonstrate tracking bigrams with "The cat sat on the mat"

- **Visual Reference**: [Optional] Look at a visualization of the nested dictionary structure from computational linguistics resources.

### Understanding Check ✓
- Why did we choose dictionaries for storing our counts?
- What information does our nested dictionary structure capture?
- How would our data structures change for a trigram model?

**Navigation Options**:
1. Proceed to Knowledge Point 5
2. Explore alternative data structures
3. See visualization of the nested dictionary
4. Request practice exercises

---

## Knowledge Point 5: Prediction Implementation and Probability Calculation

**Probing Question**: Once we have counts of words and their co-occurrences, how can we use this information to make probabilistic predictions?

### Making Predictions
- **Implementation Challenge**: Given a word, how do we predict the next one?
  * Consider multiple possible next words
  * Calculate probabilities for each candidate
  * Select based on highest probability

- **Code Implementation**:
```python
def predict_next_word(word, word_counts, bigram_counts):
    if word not in bigram_counts:
        return None  # We've never seen this word before
    
    # Get all words that have followed our input word
    followers = bigram_counts[word]
    
    # Find the most frequent follower
    best_count = 0
    best_word = None
    
    for follower, count in followers.items():
        if count > best_count:
            best_count = count
            best_word = follower
    
    # Calculate probability
    probability = best_count / word_counts[word]
    
    return best_word, probability

# Test with our model
prediction, prob = predict_next_word("the", word_counts, bigram_counts)
print(f"After 'the', predict '{prediction}' with probability {prob:.2f}")
```

- **Probability Calculation**:
  * P(next_word | current_word) = Count(current_word, next_word) / Count(current_word)
  * What this formula means intuitively
  * Handling zero probabilities (unseen combinations)

- **Checkpoint**: Calculate P(mat|the) from your training data

- **Visual Reference**: [Optional] Examine a diagram showing probability calculations for next-word prediction from introductory NLP textbooks.

### Understanding Check ✓
- What does the probability value tell us about our prediction?
- What happens if we encounter a word we've never seen before?
- How could we improve our prediction method beyond just picking the most frequent follower?

**Navigation Options**:
1. Proceed to Knowledge Point 6
2. Explore more sophisticated prediction strategies
3. See visualization of probability calculations
4. Request practice exercises

---

## Knowledge Point 6: Model Evaluation Techniques

**Probing Question**: How can we measure how "good" our text predictor is, and what would constitute a reasonable baseline for success?

### Understanding Evaluation
- **Discussion Points**:
  * What makes a prediction "good" or "bad"?
  * How would you measure success?
  * What does accuracy mean in this context?

- **Evaluation Concepts**:
  * Training vs. Test Data: Why we need separate datasets
  * Accuracy: Percentage of correct predictions
  * Perplexity: A better metric for language models (advanced)
  * Baseline Performance: What's a reasonable expectation?

- **Code Implementation**:
```python
def evaluate_predictor(test_tokens, word_counts, bigram_counts):
    correct_predictions = 0
    total_predictions = 0
    
    for i in range(len(test_tokens) - 1):
        current_word = test_tokens[i]
        actual_next_word = test_tokens[i+1]
        
        prediction, _ = predict_next_word(current_word, word_counts, bigram_counts)
        
        if prediction == actual_next_word:
            correct_predictions += 1
        
        total_predictions += 1
    
    accuracy = correct_predictions / total_predictions
    return accuracy

# Assuming we have test_tokens from a separate test set
# accuracy = evaluate_predictor(test_tokens, word_counts, bigram_counts)
# print(f"Model accuracy: {accuracy:.2f}")
```

- **Interpretation**:
  * Why might accuracy be low? Is that expected?
  * How would vocabulary size affect accuracy?
  * What would be a "good" score for a simple bigram model?

- **Visual Reference**: [Optional] Look at a graph showing typical accuracy ranges for n-gram models of different sizes from NLP literature.

### Understanding Check ✓
- Why do we need separate training and test data?
- Why might 30% accuracy be considered reasonable for this task?
- How would you explain model performance to a non-technical person?

**Navigation Options**:
1. Proceed to Knowledge Point 7
2. Explore more evaluation metrics
3. See visualization of n-gram model performance
4. Request practice exercises

---

## Knowledge Point 7: Fundamental Limitations and Machine Learning Concepts

**Probing Question**: Why might statistical n-gram models falter as we increase the context size, and what machine learning concepts explain these limitations?

### Exploring Limitations Through Experimentation
- **Experimental Activities**:
  * Compare bigrams vs. trigrams vs. 4-grams
  * Test with different types of text
  * Try to "break" your predictor

- **Core ML Concepts Through Examples**:
  1. **The Curse of Dimensionality**
     * As n increases, possible combinations explode exponentially
     * Most combinations never appear in training data
     * Demonstration: Count unique bigrams vs. possible bigrams

  2. **Sparsity Problem**
     * Most theoretical combinations never occur
     * Calculate: What percentage of possible combinations appear?
     * Impact on prediction quality

  3. **Context Window Limitations**
     * Fixed-size context misses long-range dependencies
     * Example: "In ancient Rome, unlike modern times, people would..."
     * Why understanding requires flexible context

- **Visual Reference**: [Optional] Examine visualizations of the curse of dimensionality and sparsity from machine learning literature.

### Understanding Check ✓
- Explain why larger n-grams don't necessarily perform better.
- How does sparsity relate to the need for more sophisticated models?
- What limitations would you want a better solution to address?

**Navigation Options**:
1. Proceed to Reflection
2. Explore more about these limitations
3. See visualizations of these ML concepts
4. Request practice exercises

---

## Reflection and Bridge to Neural Networks

**Integration Activity**: Summarize what you've learned about n-gram predictors and their limitations. Consider:
- The key components of your implemented predictor
- The fundamental limitations you experienced firsthand
- How these limitations connect to the need for more sophisticated approaches

**Key Questions to Consider**:
- What specific problems would you want a better solution to solve?
- How might word embeddings help address the sparsity problem?
- Why might neural networks offer advantages over statistical counting?

**Complete Code Model**:
```python
class SimpleBigramPredictor:
    def __init__(self):
        self.word_counts = {}
        self.bigram_counts = {}
    
    def tokenize(self, text):
        return text.lower().split()
    
    def train(self, text):
        tokens = self.tokenize(text)
        
        # Count individual words
        for token in tokens:
            self.word_counts[token] = self.word_counts.get(token, 0) + 1
        
        # Track word pairs (bigrams)
        for i in range(len(tokens) - 1):
            current_word = tokens[i]
            next_word = tokens[i+1]
            
            if current_word not in self.bigram_counts:
                self.bigram_counts[current_word] = {}
            
            self.bigram_counts[current_word][next_word] = self.bigram_counts[current_word].get(next_word, 0) + 1
    
    def predict(self, word):
        if word not in self.bigram_counts:
            return None  # We've never seen this word before
        
        # Find the most frequent follower
        best_count = 0
        best_word = None
        
        for follower, count in self.bigram_counts[word].items():
            if count > best_count:
                best_count = count
                best_word = follower
        
        # Calculate probability
        probability = best_count / self.word_counts[word]
        
        return best_word, probability
    
    def evaluate(self, test_text):
        tokens = self.tokenize(test_text)
        correct_predictions = 0
        total_predictions = 0
        
        for i in range(len(tokens) - 1):
            current_word = tokens[i]
            actual_next_word = tokens[i+1]
            
            prediction = self.predict(current_word)
            
            if prediction and prediction[0] == actual_next_word:
                correct_predictions += 1
            
            total_predictions += 1
        
        accuracy = correct_predictions / total_predictions
        return accuracy

# Example usage
predictor = SimpleBigramPredictor()
predictor.train("the cat sat on the mat the dog chased the cat the cat ran up the tree")
print(predictor.predict("the"))
print(predictor.predict("cat"))
```

**Next Steps Preview**: In the next session, we'll explore how neural language models and word embeddings overcome many of the limitations we've discovered in our n-gram predictor.

---

## Additional Resources

### Visualizations and Interactive Tools
- [Google Ngram Viewer](https://books.google.com/ngrams) - Visualize word frequency across Google Books corpus with interactive graphs showing how phrases have occurred over time
- [EarlyPrint N-gram Browser](https://earlyprint.org/lab/tool_ngram_browser.html) - Explore historical text with options for unigrams, bigrams, and trigrams
- [Content Harmony N-Gram Analyzer](https://www.contentharmony.com/tools/n-grams/) - Extract and analyze n-grams from text with frequency counts and visualization
- [Python Visualization with NLTK](https://www.nltk.org/) - Tools for natural language processing including n-gram analysis
- [Voyant Tools](https://voyant-tools.org/) - Web-based text analysis with visualization capabilities

### Programming Tools
- [NLTK N-gram Package](https://www.nltk.org/api/nltk.lm.html) - Python library for language modeling including n-grams
- [Python N-gram Implementation](https://www.analyticsvidhya.com/blog/2021/09/what-are-n-grams-and-how-to-implement-them-in-python/) - Tutorial on implementing n-grams in Python with code examples
- [Python Tutor](http://pythontutor.com/visualize.html) - Visualize Python code execution including dictionary structures

### Research References
- Manning, C. & Schütze, H. (1999). [Foundations of Statistical Natural Language Processing](https://nlp.stanford.edu/fsnlp/)
- Jurafsky, D. & Martin, J.H. [Speech and Language Processing](https://web.stanford.edu/~jurafsky/slp3/) (Ch. 3: N-gram Language Models)
- Bengio, Y., et al. (2003). [A Neural Probabilistic Language Model](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)
