# Session 1.5: Advanced Word Embeddings and Applications

## Primary Question
How can we train word embeddings more efficiently than full language models, and what semantic properties do these embeddings capture?

Through exploring specialized embedding techniques and their applications, you'll discover how word vectors can encode meaning and relationships, setting the foundation for modern language models.

## Knowledge Points Overview
This session covers the following key knowledge points:
1. Word2Vec and specialized embedding approaches
2. Training word embeddings with context
3. Semantic properties of word embeddings
4. Applications of word embeddings
5. Limitations of static embeddings

---

## Knowledge Point 1: Word2Vec and Specialized Embedding Approaches

**Probing Question**: Is there a more efficient way to learn meaningful word representations than training a full neural language model?

### Beyond Full Language Modeling
- **Motivation for Specialized Approaches**:
  * Neural language models are computationally expensive
  * Prediction is complex and slow to train
  * We might only need good word representations

- **Introduction to Word2Vec**:
  * Mikolov et al.'s breakthrough approach (2013)
  * Focus only on learning word representations
  * Simpler architecture than Bengio's language model

- **Two Model Architectures**:
  * Skip-gram: Predict context words from center word
  * CBOW (Continuous Bag of Words): Predict center word from context words
  * Comparison of both approaches

- **Conceptual Shift**:
  * From language modeling (next word prediction) to representation learning
  * Simpler prediction tasks designed specifically for embedding quality
  * Dramatic training efficiency improvements

- **Visual Reference**: Examine the architecture diagrams from the original "Efficient Estimation of Word Representations in Vector Space" (Mikolov et al., 2013).

### Understanding Check ✓
- What's the key difference between Word2Vec and neural language models?
- How do Skip-gram and CBOW differ in their approach?
- Why might specialized embedding approaches be preferable to full language modeling?

**Navigation Options**:
1. Proceed to Knowledge Point 2
2. Explore Word2Vec architectures in more detail
3. See visualization of model differences
4. Request practice exercises

---

## Knowledge Point 2: Training Word Embeddings with Context

**Probing Question**: How do specialized techniques like Word2Vec actually learn from context, and what training optimizations make them efficient?

### Efficient Learning Strategies
- **The Context Window Concept**:
  * Sliding window of words around target
  * Example: "the quick brown fox jumps" with window size 2
  * Target-context pairs generation

- **Training Objectives**:
  * Maximize probability of context given target (Skip-gram)
  * Maximize probability of target given context (CBOW)
  * Both require softmax over vocabulary (potentially costly)

- **Optimization Techniques**:
  * Negative sampling: Train on positive examples and few negative examples
  * Hierarchical softmax: Efficient approximation of full softmax
  * Subsampling frequent words: Reduce training examples with common words

- **Training Process Walkthrough**:
  * Example text passage
  * Window sliding and pair generation
  * Positive and negative example selection
  * Model update step

- **Visual Reference**: Examine training visualization from "The Illustrated Word2Vec" (Alammar, 2019) showing pair generation and training.

### Understanding Check ✓
- How does negative sampling make training more efficient?
- Why might subsampling frequent words improve embedding quality?
- What's the significance of the context window size in training?

**Navigation Options**:
1. Proceed to Knowledge Point 3
2. Explore training optimizations in more detail
3. See visualization of training process
4. Request practice exercises

---

## Knowledge Point 3: Semantic Properties of Word Embeddings

**Probing Question**: What kinds of meaningful patterns and relationships can word embeddings capture in their vector space?

### Discovering Vector Space Properties
- **Vector Arithmetic and Analogies**:
  * Famous example: king - man + woman ≈ queen
  * Country-capital relationships: france - paris ≈ italy - rome
  * Semantic and syntactic patterns

- **Types of Relationships Captured**:
  * Semantic similarity (dog ~ puppy)
  * Analogical relationships (king:queen::man:woman)
  * Hierarchical relationships (animal > dog > poodle)
  * Conceptual properties (capitals, genders, tenses)

- **Interactive Exploration**: Explore the "Word Analogy" tool in the TensorFlow Embedding Projector (https://projector.tensorflow.org/)

- **Mathematical Interpretation**:
  * Word vectors encode feature dimensions
  * Difference vectors capture relationships
  * Consistent patterns across vector space

- **Visual Reference**: Examine visualizations of word analogies from "Linguistic Regularities in Continuous Space Word Representations" (Mikolov et al., 2013).

### Understanding Check ✓
- How does vector arithmetic reflect semantic relationships?
- What does the success of these analogies tell us about the structure of the embedding space?
- What kinds of knowledge might be encoded in the dimensions of these vectors?

**Navigation Options**:
1. Proceed to Knowledge Point 4
2. Explore more semantic properties
3. See visualization of vector arithmetic
4. Request practice exercises

---

## Knowledge Point 4: Applications of Word Embeddings

**Probing Question**: How can these word vector representations be used in practical natural language processing applications?

### Practical Uses of Word Embeddings
- **Document Classification**:
  * Representing documents as combinations of word vectors
  * Improved classification through semantic understanding
  * Handling synonyms and related concepts

- **Information Retrieval**:
  * Semantic search beyond exact word matching
  * Query expansion using similar words
  * Improving search relevance

- **Machine Translation**:
  * Cross-lingual word embeddings
  * Finding translation equivalents in vector space
  * Alignment of multilingual word vectors

- **Named Entity Recognition**:
  * Using vector similarity to identify entity types
  * Generalization to unseen entities
  * Improvement over discrete feature approaches

- **Real-World Example Walkthrough**:
  * Converting documents to vector representations
  * Processing with simple classifiers
  * Performance improvements from embeddings

- **Visual Reference**: Examine application diagrams from "Distributed Representations of Words and Phrases and their Compositionality" (Mikolov et al., 2013).

### Understanding Check ✓
- What types of applications benefit most from word embeddings?
- How do embeddings improve these applications compared to traditional approaches?
- What practical challenges might arise when applying embeddings?

**Navigation Options**:
1. Proceed to Knowledge Point 5
2. Explore applications in more detail
3. See visualization of application examples
4. Request practice exercises

---

## Knowledge Point 5: Limitations of Static Embeddings

**Probing Question**: What fundamental limitations remain with these word embedding approaches, and how might they be addressed?

### Embedding Challenges and Future Directions
- **Word Sense Disambiguation Problem**:
  * Single vector for words with multiple meanings
  * Example: "bank" (financial vs. riverside)
  * Why this is problematic for downstream applications

- **Context-Dependent Meanings**:
  * "Red" in "red wine" vs. "red flag"
  * Need for representations that change with context
  * Limitation of static (fixed) embeddings

- **Out-of-Vocabulary Words**:
  * Handling words not seen during training
  * Approaches: subword embeddings, character-level models
  * Example solutions: FastText, BPE tokenization

- **Domain-Specific Language**:
  * General vs. specialized vocabulary
  * Transfer learning and domain adaptation
  * When to train custom embeddings

- **Bridge to Future Approaches**:
  * From static to contextual embeddings
  * The need for variable context
  * Preview of next module on transformer models

- **Visual Reference**: Examine comparison visualizations of static vs. contextual embeddings from recent NLP literature.

### Understanding Check ✓
- What is the fundamental limitation of having one vector per word?
- How might contextual representations address these limitations?
- Which applications would be most affected by these limitations?

**Navigation Options**:
1. Proceed to Reflection
2. Explore limitations in more detail
3. See visualization of embedding limitations
4. Request practice exercises

---

## Reflection and Bridge to Next Module

**Integration Activity**: Summarize the progression from n-gram models to neural language models to specialized word embeddings, highlighting the strengths and limitations of each approach.

**Key Questions to Consider**:
- How has our approach to representing words evolved from Session 1.2 through 1.5?
- What do you find most surprising or interesting about the properties of word embeddings?
- What future directions might address the limitations we've identified?
- How might these concepts connect to modern language models?

**Next Steps Preview**: In Module 2, we'll explore the transformer architecture that powers modern LLMs, starting with tokenization approaches that address some of the limitations we've discovered, and learn how position information and attention mechanisms revolutionized language modeling.

---

## Additional Resources

### Interactive Tools
- TensorFlow Embedding Projector (https://projector.tensorflow.org/)
- Interactive Word2Vec Explorer (https://ronxin.github.io/wevi/)
- Word Vector Analogy Demo (https://people.cs.umass.edu/~miyyer/analogy.html)

### Implementations
- Gensim Word2Vec Tutorial (https://radimrehurek.com/gensim/models/word2vec.html)
- FastText Implementation (https://fasttext.cc/)
- Hugging Face Tokenizers Library (https://github.com/huggingface/tokenizers)

### Research References
- Mikolov, T., et al. (2013). "Efficient Estimation of Word Representations in Vector Space." ICLR Workshop.
- Mikolov, T., et al. (2013). "Distributed Representations of Words and Phrases and their Compositionality." NeurIPS.
- Mikolov, T., et al. (2013). "Linguistic Regularities in Continuous Space Word Representations." NAACL.
- Pennington, J., et al. (2014). "GloVe: Global Vectors for Word Representation." EMNLP.
- Bojanowski, P., et al. (2017). "Enriching Word Vectors with Subword Information." TACL.
- Alammar, J. (2019). "The Illustrated Word2Vec." https://jalammar.github.io/illustrated-word2vec/
