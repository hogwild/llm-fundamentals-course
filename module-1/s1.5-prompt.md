# Session 1.5: Advanced Word Embeddings and Applications

## Primary Question
How can we train word embeddings more efficiently than full language models, what semantic properties do these embeddings capture, and what are the trade-offs between supervised and unsupervised approaches?

Through exploring specialized embedding techniques and their applications, you'll discover how word vectors can encode meaning and relationships, setting the foundation for modern language models.

## Knowledge Points Overview
This session covers the following key knowledge points:
1. Supervised vs. unsupervised approaches to word embeddings
2. Word2Vec and specialized embedding techniques
3. Training word embeddings with context
4. Semantic properties of word embeddings
5. Applications of word embeddings
6. Limitations of static embeddings and bridge to next module

---

## Knowledge Point 1: Supervised vs. Unsupervised Approaches to Word Embeddings

**Probing Question**: What are the key differences between supervised and unsupervised approaches to learning word embeddings, and what are their respective advantages?

### Understanding Different Learning Paradigms
- **Supervised Word Embeddings (Neural Language Models)**:
  * Language modeling objective (predicting next word)
  * Learned as part of a larger task
  * More computationally expensive
  * Example: Bengio's Neural Language Model (from Session 1.4)

- **Unsupervised Word Embeddings**:
  * Focused solely on learning good representations
  * Specialized objective functions
  * More computationally efficient
  * Example: Word2Vec, GloVe

- **Computational Considerations**:
  * Training time comparison: hours vs. days/weeks
  * Hardware requirements
  * When free text data is abundant, main difference is computational cost
  * For smaller/specialized domains, quality considerations become more important

- **Bridge to Future Concepts**:
  * Foreshadowing: For advanced capabilities like instruction following, supervised learning becomes essential
  * Connection to Module 2: How supervised learning enables more complex behaviors

- **Visual Reference**: Examine side-by-side comparison of training approaches from "Distributed Representations of Words and Phrases and their Compositionality" (Mikolov et al., 2013, NeurIPS).

### Understanding Check ✓
- What are the key differences between supervised and unsupervised approaches to word embeddings?
- When might you prefer one approach over the other?
- How do these approaches foreshadow different techniques we'll see in modern language models?

**Navigation Options**:
1. Proceed to Knowledge Point 2
2. Explore paradigm differences in more detail
3. See visualization of approach comparisons
4. Request practice exercises

---

## Knowledge Point 2: Word2Vec and Specialized Embedding Techniques

**Probing Question**: How do specialized approaches like Word2Vec learn meaningful word representations more efficiently than full neural language models?

### Beyond Full Language Modeling
- **Introduction to Word2Vec**:
  * Mikolov et al.'s breakthrough approach (2013)
  * Focus only on learning word representations
  * Simpler architecture than Bengio's language model

- **Two Model Architectures**:
  * Skip-gram: Predict context words from center word
  * CBOW (Continuous Bag of Words): Predict center word from context words
  * Comparison of both approaches

- **Conceptual Shift**:
  * From language modeling (next word prediction) to representation learning
  * Simpler prediction tasks designed specifically for embedding quality
  * Dramatic training efficiency improvements

- **Visual Reference**: Examine the architecture diagrams from the original "Efficient Estimation of Word Representations in Vector Space" (Mikolov et al., 2013, https://arxiv.org/abs/1301.3781).

### Understanding Check ✓
- What's the key difference between Word2Vec and neural language models?
- How do Skip-gram and CBOW differ in their approach?
- Why might specialized embedding approaches be preferable to full language modeling?

**Navigation Options**:
1. Proceed to Knowledge Point 3
2. Explore Word2Vec architectures in more detail
3. See visualization of model differences
4. Request practice exercises

---

## Knowledge Point 3: Training Word Embeddings with Context

**Probing Question**: How do specialized techniques like Word2Vec actually learn from context, and what training optimizations make them efficient?

### Efficient Learning Strategies
- **The Context Window Concept**:
  * Sliding window of words around target
  * Example: "the quick brown fox jumps" with window size 2
  * Target-context pairs generation

- **Training Objectives**:
  * Maximize probability of context given target (Skip-gram)
  * Maximize probability of target given context (CBOW)
  * Both require softmax over vocabulary (potentially costly)

- **Optimization Techniques**:
  * Negative sampling: Train on positive examples and few negative examples
  * Hierarchical softmax: Efficient approximation of full softmax
  * Subsampling frequent words: Reduce training examples with common words

- **Training Process Walkthrough**:
  * Example text passage
  * Window sliding and pair generation
  * Positive and negative example selection
  * Model update step

- **Visual Reference**: Examine training visualization from "The Illustrated Word2Vec" (Alammar, 2019, https://jalammar.github.io/illustrated-word2vec/) showing pair generation and training.

### Understanding Check ✓
- How does negative sampling make training more efficient?
- Why might subsampling frequent words improve embedding quality?
- What's the significance of the context window size in training?

**Navigation Options**:
1. Proceed to Knowledge Point 4
2. Explore training optimizations in more detail
3. See visualization of training process
4. Request practice exercises

---

## Knowledge Point 4: Semantic Properties of Word Embeddings

**Probing Question**: What kinds of meaningful patterns and relationships can word embeddings capture in their vector space?

### Discovering Vector Space Properties
- **Vector Arithmetic and Analogies**:
  * Famous example: king - man + woman ≈ queen
  * Country-capital relationships: france - paris ≈ italy - rome
  * Semantic and syntactic patterns

- **Types of Relationships Captured**:
  * Semantic similarity (dog ~ puppy)
  * Analogical relationships (king:queen::man:woman)
  * Hierarchical relationships (animal > dog > poodle)
  * Conceptual properties (capitals, genders, tenses)

- **Interactive Exploration**: Explore the "Word Analogy" tool in the TensorFlow Embedding Projector (https://projector.tensorflow.org/) which allows visualization of high-dimensional data

- **Mathematical Interpretation**:
  * Word vectors encode feature dimensions
  * Difference vectors capture relationships
  * Consistent patterns across vector space

- **Visual Reference**: Examine visualizations of word analogies from "Linguistic Regularities in Continuous Space Word Representations" (Mikolov et al., 2013, published in NAACL proceedings).

### Understanding Check ✓
- How does vector arithmetic reflect semantic relationships?
- What does the success of these analogies tell us about the structure of the embedding space?
- What kinds of knowledge might be encoded in the dimensions of these vectors?

**Navigation Options**:
1. Proceed to Knowledge Point 5
2. Explore more semantic properties
3. See visualization of vector arithmetic
4. Request practice exercises

---

## Knowledge Point 5: Applications of Word Embeddings

**Probing Question**: How can these word vector representations be used in practical natural language processing applications?

### Practical Uses of Word Embeddings
- **Document Classification**:
  * Representing documents as combinations of word vectors
  * Improved classification through semantic understanding
  * Handling synonyms and related concepts

- **Information Retrieval**:
  * Semantic search beyond exact word matching
  * Query expansion using similar words
  * Improving search relevance

- **Machine Translation**:
  * Cross-lingual word embeddings
  * Finding translation equivalents in vector space
  * Alignment of multilingual word vectors

- **Named Entity Recognition**:
  * Using vector similarity to identify entity types
  * Generalization to unseen entities
  * Improvement over discrete feature approaches

- **Real-World Example Walkthrough**:
  * Converting documents to vector representations
  * Processing with simple classifiers
  * Performance improvements from embeddings

- **Visual Reference**: Examine application diagrams from "Distributed Representations of Words and Phrases and their Compositionality" (Mikolov et al., 2013, published in NeurIPS).

### Understanding Check ✓
- What types of applications benefit most from word embeddings?
- How do embeddings improve these applications compared to traditional approaches?
- What practical challenges might arise when applying embeddings?

**Navigation Options**:
1. Proceed to Knowledge Point 6
2. Explore applications in more detail
3. See visualization of application examples
4. Request practice exercises

---

## Knowledge Point 6: Limitations of Static Embeddings and Bridge to Next Module

**Probing Question**: What fundamental limitations remain with these word embedding approaches, and how do they connect to the need for more advanced approaches in Module 2?

### Embedding Challenges and Future Directions
- **Word Sense Disambiguation Problem**:
  * Single vector for words with multiple meanings
  * Example: "bank" (financial vs. riverside)
  * Why this is problematic for downstream applications

- **Context-Dependent Meanings**:
  * "Red" in "red wine" vs. "red flag"
  * Need for representations that change with context
  * Limitation of static (fixed) embeddings

- **Out-of-Vocabulary Words**:
  * Handling words not seen during training
  * Approaches: subword embeddings, character-level models
  * Example solutions: FastText, BPE tokenization

- **From Unsupervised to Supervised Learning**:
  * Limitations of purely unsupervised approaches
  * Needing to integrate task-specific supervision
  * Connection to instruction following capabilities

- **Bridge to Module 2**:
  * Preview of tokenization approaches (addressing OOV issues)
  * Introduction to contextual embeddings (addressing word sense issues)
  * The role of attention in finding relevant context
  * How modern LLMs use both supervised and unsupervised techniques

- **Visual Reference**: Examine comparison visualizations of static vs. contextual embeddings from recent NLP literature.

### Understanding Check ✓
- What is the fundamental limitation of having one vector per word?
- How might contextual representations address these limitations?
- How does the need for instruction following connect to supervised learning?

**Navigation Options**:
1. Proceed to Reflection
2. Explore limitations in more detail
3. See visualization of embedding limitations
4. Request practice exercises

---

## Additional Resources

### Interactive Tools
- TensorFlow Embedding Projector (https://projector.tensorflow.org/) - Interactive visualization of high-dimensional data including word embeddings
- Interactive Word2Vec Explorer (WEVI) (https://ronxin.github.io/wevi/) - Step-by-step visualization of Word2Vec training
- Word Vector Analogy Demo (https://people.cs.umass.edu/~miyyer/analogy.html) - Demonstration of word analogy capabilities

### Implementations
- Gensim Word2Vec Tutorial (https://radimrehurek.com/gensim/models/word2vec.html)
- FastText Implementation (https://fasttext.cc/)
- Hugging Face Tokenizers Library (https://github.com/huggingface/tokenizers)

### Research References
- Mikolov, T., et al. (2013). "Efficient Estimation of Word Representations in Vector Space." ICLR Workshop. https://arxiv.org/abs/1301.3781
- Mikolov, T., et al. (2013). "Distributed Representations of Words and Phrases and their Compositionality." NeurIPS.
- Mikolov, T., et al. (2013). "Linguistic Regularities in Continuous Space Word Representations." NAACL.
- Pennington, J., et al. (2014). "GloVe: Global Vectors for Word Representation." EMNLP. https://nlp.stanford.edu/pubs/glove.pdf
- Bojanowski, P., et al. (2017). "Enriching Word Vectors with Subword Information." TACL.
- Alammar, J. (2019). "The Illustrated Word2Vec." https://jalammar.github.io/illustrated-word2vec/
