# Session 1.4: Neural Language Model Prediction and Training

## Primary Question
How do neural language models make predictions and learn from data, leveraging the vector representations we explored in Session 1.3?

Through examining neural language model architecture and training processes, you'll understand how these models make predictions and improve over time, addressing fundamental limitations of n-gram approaches.

## Knowledge Points Overview
This session covers the following key knowledge points:
1. Neural language model output layer and softmax
2. Loss functions and error measurement
3. Gradient descent and model training
4. Bengio's neural language model architecture
5. Advantages over n-gram models

---

## Knowledge Point 1: Neural Language Model Output Layer and Softmax

**Probing Question**: How does a neural network convert its internal calculations into meaningful word predictions?

### Making Predictions Across Vocabulary
- **The Output Layer Challenge**:
  * Need to predict one word out of thousands in vocabulary
  * Each word needs a score/probability
  * Scores must be comparable across all words

- **Introduction to Softmax Function**:
  * Purpose: Convert raw scores to probabilities
  * Mathematical definition (in simple terms)
  * Properties: outputs sum to 1, preserves ranking, emphasizes high values

- **From Network Output to Word Prediction**:
  * Raw scores for each word in vocabulary
  * Apply softmax to get probability distribution
  * Select highest probability word or sample from distribution

- **Interactive Demonstration**: Visualize how softmax transforms scores into probabilities
  * Example: Raw scores [2.0, 1.0, 0.1] → Probabilities [0.67, 0.24, 0.09]
  * See how changing one score affects all probabilities

- **Visual Reference**: Examine a visualization of the softmax transformation from "The Illustrated Transformer" (Alammar, 2018, https://jalammar.github.io/illustrated-transformer/).

### Understanding Check ✓
- What is the purpose of the softmax function in language models?
- Why do we need to convert raw scores to probabilities?
- How does this enable the model to make predictions across the entire vocabulary?

**Navigation Options**:
1. Proceed to Knowledge Point 2
2. Explore softmax function in more detail
3. See visualization of softmax transformation
4. Request practice exercises

---

## Knowledge Point 2: Loss Functions and Error Measurement

**Probing Question**: How do we measure if a language model's predictions are good or bad, and what exactly are we trying to optimize?

### Measuring Prediction Quality
- **Introduction to Loss Functions**:
  * Purpose: Quantify prediction error
  * Desired properties: lower when prediction is better
  * Guide the learning process

- **Cross-Entropy Loss for Language Modeling**:
  * Measuring difference between predicted and actual probabilities
  * Why it's suitable for word prediction tasks
  * Simple intuition: penalizes wrong predictions, rewards correct ones

- **From Word-Level to Sequence-Level Evaluation**:
  * Perplexity: standard evaluation metric for language models
  * Relationship to cross-entropy loss
  * Intuitive meaning: model's confusion level when predicting

- **Conceptual Example**:
  * Correct next word: "cat"
  * Model predicts: {"cat": 0.3, "dog": 0.2, "hat": 0.1, ...}
  * Loss calculation focuses on probability assigned to "cat"

- **Visual Reference**: Examine a visualization of cross-entropy loss from "Deep Learning" (Goodfellow et al., 2016, MIT Press).

### Understanding Check ✓
- What is a loss function and why is it necessary for training?
- How does cross-entropy loss measure the quality of word predictions?
- Why might we care more about the probability of the correct word than the probabilities of incorrect words?

**Navigation Options**:
1. Proceed to Knowledge Point 3
2. Explore loss functions in more detail
3. See visualization of cross-entropy calculation
4. Request practice exercises

---

## Knowledge Point 3: Gradient Descent and Model Training

**Probing Question**: Once we know how wrong our predictions are, how do we actually improve the model?

### Learning from Mistakes
- **The Learning Challenge**:
  * Starting with random vectors and weights
  * Need to adjust to reduce prediction errors
  * Millions of parameters to tune simultaneously

- **Introduction to Gradient Descent**:
  * Gradient: direction of steepest increase in loss
  * Idea: move parameters in opposite direction to decrease loss
  * Learning rate: how big each adjustment step should be

- **The Training Process**:
  * Forward pass: make predictions
  * Calculate loss (error)
  * Backward pass: compute gradients
  * Update parameters
  * Repeat millions of times

- **Interactive Exploration**: Visualize gradient descent with UCLA's "Gradient Descent Visualizer" (https://uclaacm.github.io/gradient-descent-visualiser/) which shows how parameters update during training

- **Visual Reference**: Examine animations of gradient descent from Google's Machine Learning Crash Course.

### Understanding Check ✓
- What is gradient descent in simple terms?
- How does the learning process actually improve the model's predictions?
- Why do we need many iterations of this process to train a model?

**Navigation Options**:
1. Proceed to Knowledge Point 4
2. Explore gradient descent in more detail
3. See animation of training process
4. Request practice exercises

---

## Knowledge Point 4: Bengio's Neural Language Model Architecture

**Probing Question**: How do all these components combine into a complete neural language model?

### The First Neural Language Model
- **Historical Context**:
  * Bengio et al.'s 2003 seminal paper
  * First successful neural approach to language modeling
  * Foundation for modern language models

- **Complete Architecture**:
  * Input: n-1 previous words (as indices)
  * Embedding layer: convert indices to vectors
  * Hidden layer: process combined vectors
  * Output layer: predict probability for each word in vocabulary

- **Comparison to N-gram Approach**:
  * Both use fixed context window
  * N-gram: count statistics of sequences
  * Neural LM: learn representations and patterns

- **Step-by-Step Walkthrough**:
  * Input phrase: "the cat sat on"
  * Goal: predict probability of next word
  * Follow data through complete model architecture

- **Visual Reference**: Examine Figure 1 from "A Neural Probabilistic Language Model" (Bengio et al., 2003, https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf) showing the complete architecture.

### Understanding Check ✓
- How does Bengio's model architecture extend the n-gram approach?
- What are the main components of this neural language model?
- How does information flow through the model during prediction?

**Navigation Options**:
1. Proceed to Knowledge Point 5
2. Explore Bengio's architecture in more detail
3. See visualization of complete model
4. Request practice exercises

---

## Knowledge Point 5: Advantages Over N-gram Models

**Probing Question**: What specific limitations of n-gram models do neural language models overcome, and why?

### Comparing Model Capabilities
- **Generalization to Unseen Sequences**:
  * N-gram: cannot handle unseen word combinations
  * Neural LM: can generalize through similar word vectors
  * Concrete example: "The cat chased the mouse" → "The dog chased the squirrel"

- **Space Efficiency**:
  * N-gram: storing counts for all possible sequences
  * Neural LM: storing vectors and weights
  * Quantitative comparison for vocabulary of 10,000 words

- **Handling of Sparse Data**:
  * N-gram: sparse counts for rare combinations
  * Neural LM: sharing of statistical strength through word similarities
  * Example with rare words demonstration

- **Empirical Performance Improvements**:
  * Lower perplexity on test datasets
  * Better handling of out-of-vocabulary words
  * Improved performance with limited training data

- **Visual Reference**: Examine performance comparison charts from "A Neural Probabilistic Language Model" (Bengio et al., 2003, https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf).

### Understanding Check ✓
- What specific n-gram limitations are addressed by neural language models?
- How does the vector representation enable better generalization?
- Which advantages would be most important in practical applications?

**Navigation Options**:
1. Proceed to Code Implementation
2. Explore advantages in more detail
3. See performance comparison visualizations
4. Request practice exercises

---

## Complete Code Implementation: Simple Neural Language Model

Below is a PyTorch implementation of a simplified version of Bengio's neural language model. This code demonstrates the core concepts we've discussed in this session, allowing you to see how these components work together in practice.

```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from collections import Counter
from torch.utils.data import Dataset, DataLoader

# Define a simple neural language model based on Bengio's architecture
class SimpleNeuralLM(nn.Module):
    def __init__(self, vocab_size, embedding_dim, context_size, hidden_dim):
        super(SimpleNeuralLM, self).__init__()
        
        # Embedding layer: converts word indices to dense vectors
        self.embeddings = nn.Embedding(vocab_size, embedding_dim)
        
        # Hidden layer: processes combined context vectors
        self.hidden = nn.Linear(context_size * embedding_dim, hidden_dim)
        self.activation = nn.Tanh()
        
        # Output layer: produces scores for each word in vocabulary
        self.output = nn.Linear(hidden_dim, vocab_size)
        
        # Softmax: converts scores to probabilities
        self.softmax = nn.LogSoftmax(dim=1)
    
    def forward(self, x):
        # x shape: [batch_size, context_size]
        
        # Convert word indices to embeddings
        embeds = self.embeddings(x)  # [batch_size, context_size, embedding_dim]
        
        # Flatten the embeddings for the hidden layer
        embeds = embeds.view(embeds.shape[0], -1)  # [batch_size, context_size * embedding_dim]
        
        # Process through hidden layer
        hidden = self.activation(self.hidden(embeds))  # [batch_size, hidden_dim]
        
        # Generate scores for each word in vocabulary
        output = self.output(hidden)  # [batch_size, vocab_size]
        
        # Convert scores to probabilities
        log_probs = self.softmax(output)  # [batch_size, vocab_size]
        
        return log_probs

# Dataset for training our language model
class LanguageModelDataset(Dataset):
    def __init__(self, text, context_size):
        self.data = []
        self.context_size = context_size
        
        # Create vocabulary (word to index mapping)
        words = text.lower().split()
        word_counts = Counter(words)
        self.vocab = {word: i+1 for i, (word, _) in enumerate(word_counts.most_common())}
        self.vocab['<UNK>'] = 0  # Unknown token
        
        # Create context-target pairs
        for i in range(context_size, len(words)):
            context = words[i-context_size:i]
            target = words[i]
            
            # Convert words to indices
            context_ids = [self.vocab.get(w, 0) for w in context]
            target_id = self.vocab.get(target, 0)
            
            self.data.append((context_ids, target_id))
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        context, target = self.data[idx]
        return torch.tensor(context), torch.tensor(target)
    
    def get_vocab_size(self):
        return len(self.vocab)

# Training function
def train_model(model, dataloader, epochs=5, learning_rate=0.01):
    # Define loss function and optimizer
    criterion = nn.NLLLoss()
    optimizer = optim.SGD(model.parameters(), lr=learning_rate)
    
    # Training loop
    for epoch in range(epochs):
        total_loss = 0
        for contexts, targets in dataloader:
            # Reset gradients
            optimizer.zero_grad()
            
            # Forward pass: compute predictions
            log_probs = model(contexts)
            
            # Compute loss
            loss = criterion(log_probs, targets)
            total_loss += loss.item()
            
            # Backward pass: compute gradients
            loss.backward()
            
            # Update parameters
            optimizer.step()
        
        # Print progress
        print(f"Epoch {epoch+1}, Loss: {total_loss/len(dataloader):.4f}")

# Evaluation function
def evaluate_model(model, dataloader):
    model.eval()
    total_loss = 0
    criterion = nn.NLLLoss()
    
    with torch.no_grad():
        for contexts, targets in dataloader:
            log_probs = model(contexts)
            loss = criterion(log_probs, targets)
            total_loss += loss.item()
    
    # Calculate perplexity
    avg_loss = total_loss / len(dataloader)
    perplexity = np.exp(avg_loss)
    
    print(f"Evaluation - Loss: {avg_loss:.4f}, Perplexity: {perplexity:.2f}")
    return perplexity

# Prediction function
def predict_next_word(model, context, dataset, top_k=3):
    # Convert context words to indices
    context_ids = [dataset.vocab.get(w.lower(), 0) for w in context]
    
    # Ensure context has the right size
    if len(context_ids) < model.context_size:
        # Pad with UNK if context is too short
        context_ids = [0] * (model.context_size - len(context_ids)) + context_ids
    else:
        # Take the last N words if context is too long
        context_ids = context_ids[-model.context_size:]
    
    # Convert to tensor
    context_tensor = torch.tensor([context_ids])
    
    # Get model predictions
    model.eval()
    with torch.no_grad():
        log_probs = model(context_tensor)
        probs = torch.exp(log_probs)
    
    # Get top-k predictions
    top_probs, top_indices = torch.topk(probs, top_k)
    
    # Convert indices back to words
    idx_to_word = {idx: word for word, idx in dataset.vocab.items()}
    predictions = [(idx_to_word[idx.item()], prob.item()) for idx, prob in zip(top_indices[0], top_probs[0])]
    
    return predictions

# Usage example
def run_example():
    # Sample text (in practice, you'd use a much larger corpus)
    text = """
    The quick brown fox jumps over the lazy dog.
    A watched pot never boils.
    All that glitters is not gold.
    The early bird catches the worm.
    You can't judge a book by its cover.
    Better late than never, but never late is better.
    """
    
    # Parameters
    context_size = 2
    embedding_dim = 16
    hidden_dim = 32
    batch_size = 4
    
    # Create dataset and dataloader
    dataset = LanguageModelDataset(text, context_size)
    vocab_size = dataset.get_vocab_size()
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
    
    # Initialize model
    model = SimpleNeuralLM(vocab_size, embedding_dim, context_size, hidden_dim)
    
    # Train model
    train_model(model, dataloader, epochs=100)
    
    # Evaluate model
    evaluate_model(model, dataloader)
    
    # Make some predictions
    test_contexts = [
        ["the", "quick"],
        ["over", "the"],
        ["bird", "catches"]
    ]
    
    for context in test_contexts:
        predictions = predict_next_word(model, context, dataset)
        print(f"Context: '{' '.join(context)}'")
        for word, prob in predictions:
            print(f"  → '{word}' (probability: {prob:.4f})")

if __name__ == "__main__":
    run_example()
```

### Code Walkthrough

This implementation demonstrates the key components we've discussed:

1. **Model Architecture** (`SimpleNeuralLM` class):
   - Embedding layer: Converts word indices to vectors
   - Hidden layer with Tanh activation: Processes concatenated vectors 
   - Output layer: Generates a score for each word in vocabulary
   - Softmax: Converts scores to probabilities

2. **Dataset Creation** (`LanguageModelDataset` class):
   - Tokenization: Splits text into words
   - Vocabulary building: Maps words to indices
   - Context-target pair generation: Creates training examples

3. **Training Process** (`train_model` function):
   - Loss computation: Uses negative log likelihood loss
   - Gradient descent: Updates parameters to minimize error
   - Iterative improvement over multiple epochs

4. **Evaluation** (`evaluate_model` function):
   - Loss computation on unseen data
   - Perplexity calculation: Standard language model evaluation metric

5. **Prediction** (`predict_next_word` function):
   - Context processing: Converts words to indices
   - Forward pass: Generates probabilities for all words
   - Top-k selection: Identifies most likely next words

### Comparing to N-gram Models

This neural implementation differs from the n-gram model in Session 1.2 in several key ways:

1. **Representation**: Words are dense vectors instead of discrete counts
2. **Generalization**: Can handle unseen combinations through vector similarity
3. **Parameters**: Learns embedding values and network weights instead of just counting
4. **Training**: Uses gradient descent instead of direct counting
5. **Space Efficiency**: Stores only embedding and weight matrices instead of all possible n-grams

### Limitations of This Implementation

For simplicity, this implementation:
- Uses a small corpus (in practice, you'd use millions of sentences)
- Has a limited vocabulary
- Uses a small context window
- Has relatively small embedding and hidden dimensions
- Doesn't include advanced techniques like regularization or more complex architectures

### Exploration Tasks

1. Try increasing the context window size. How does this affect predictions?
2. Experiment with different embedding and hidden dimensions. What changes?
3. Test the model on different types of text. How does domain affect performance?
4. Modify the architecture to include more hidden layers. Does this improve results?
5. Implement early stopping to prevent overfitting.

---

## Reflection and Bridge to Next Session

**Integration Activity**: Summarize how neural language models represent, predict, and learn language patterns, highlighting the specific improvements over n-gram approaches.

**Key Questions to Consider**:
- How does the combination of vector representations and neural networks fundamentally change what's possible in language modeling?
- Which aspect of neural language models do you think contributes most to their improved performance?
- What limitations might still exist in this approach?

**Next Steps Preview**: In Session 1.5, we'll explore specialized embedding techniques like Word2Vec that focus specifically on learning meaningful word representations, and discover the fascinating semantic properties these representations can capture.

---

## Additional Resources

### Visualizations
- Softmax transformation visualizations
- Gradient descent animations
- Neural language model architecture diagrams
- Performance comparison charts

### Interactive Tools
- UCLA Gradient Descent Visualizer (https://uclaacm.github.io/gradient-descent-visualiser/)
- Interactive Neural Network Playground (https://playground.tensorflow.org/)
- Loss Function Visualizers

### Research References
- Bengio, Y., et al. (2003). "A Neural Probabilistic Language Model." Journal of Machine Learning Research, 3, 1137-1155. https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf
- Goodfellow, I., et al. (2016). "Deep Learning." MIT Press.
- Alammar, J. (2018). "The Illustrated Transformer." https://jalammar.github.io/illustrated-transformer/
- Google Machine Learning Crash Course: https://developers.google.com/machine-learning/crash-course
