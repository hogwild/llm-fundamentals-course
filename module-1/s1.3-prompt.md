# Session 1.3: Neural Language Models and Word Embeddings

## Primary Question
How do neural approaches to language modeling overcome the limitations of n-grams, and how do they learn meaningful representations of words?

Through hands-on exploration and interactive examples, you'll discover how neural language models learn word embeddings while predicting next words, and how these embeddings capture semantic meaning.

## Knowledge Points Overview
This session covers the following key knowledge points:
1. Contextual word meaning and n-gram limitations
2. Words as vectors: The fundamental shift
3. Multilayer perceptrons: The building blocks of neural networks
4. Optimization through gradient descent
5. Neural language model architecture
6. Learning process for word embeddings
7. Word2Vec: Unsupervised embedding approaches
8. Semantic properties of word embeddings
9. Self-supervised learning as a bridge to modern LLMs## Knowledge Point 4: Optimization Through Gradient Descent

**Probing Question**: How do neural networks actually learn from their errors, and what mathematical process allows them to gradually improve their predictions?

### The Learning Algorithm
- **Gradient Descent Fundamentals**:
  * The concept of a loss landscape
  * Finding the direction of steepest descent
  * Taking steps proportional to the negative gradient
  * The role of learning rate

- **Interactive Visualization**: Explore the [UCLA Gradient Descent Visualizer](https://uclaacm.github.io/gradient-descent-visualiser/) to see how parameters update during training

- **Variants of Gradient Descent**:
  * Batch gradient descent: Using all training examples
  * Stochastic gradient descent: Using one example at a time
  * Mini-batch gradient descent: Using small batches
  * Trade-offs between these approaches

- **Challenges in Optimization**:
  * Local minima vs. global minima
  * Saddle points
  * Vanishing and exploding gradients
  * Learning rate selection

- **Visual Reference**: [Optional] See Google Developers' visualization of [gradient descent iterations](https://developers.google.com/machine-learning/crash-course/linear-regression/gradient-descent) showing how loss decreases through training

### Understanding Check ✓
- What is the gradient and why is it important for learning?
- How does the learning rate affect convergence?
- What are the advantages of mini-batch gradient descent over batch or stochastic approaches?

**Navigation Options**:
1. Proceed to Knowledge Point 5
2. Explore gradient descent variants in more detail
3. See animation of gradient descent optimization
4. Request practice exercises## Knowledge Point 3: Multilayer Perceptrons: The Building Blocks of Neural Networks

**Probing Question**: What fundamental computational structures allow neural networks to model non-linear relationships that simple n-gram models cannot?

### Understanding MLPs
- **Key Concept**: Multilayer perceptrons as universal function approximators
  * From single neurons to layered networks
  * How multiple layers enable complex pattern recognition
  * Why MLPs can model non-linear relationships

- **MLP Architecture**:
  * Input layer: Represents features (word vectors)
  * Hidden layer(s): Where transformations occur
  * Output layer: Produces predictions
  * Activation functions: Creating non-linearity

- **Non-Linearity and Activation Functions**:
  * Why we need non-linear activation functions
  * Common functions: Sigmoid, tanh, ReLU
  * How activation functions enable complex pattern learning

- **Interactive Exploration**: Experiment with an [Interactive Multilayer Perceptron Visualization](https://chokkan.github.io/deeplearning/demo-mlp.html) that shows how changing parameters affects network behavior

- **Visual Reference**: [Optional] Examine the diagram of biological vs. artificial neurons from DataCamp's MLP tutorial which illustrates the biological inspiration for neural networks

### Understanding Check ✓
- How do multilayer perceptrons differ from single perceptrons?
- Why are non-linear activation functions crucial in neural networks?
- How do multiple layers enable MLPs to learn complex patterns?

**Navigation Options**:
1. Proceed to Knowledge Point 4
2. Explore MLP architectures in more detail
3. See animation of how MLPs transform data
4. Request practice exercises# Session 1.3: Neural Language Models and Word Embeddings

## Primary Question
How do neural approaches to language modeling overcome the limitations of n-grams, and how do they learn meaningful representations of words?

Through hands-on exploration and interactive examples, you'll discover how neural language models learn word embeddings while predicting next words, and how these embeddings capture semantic meaning.

## Knowledge Points Overview
This session covers the following key knowledge points:
1. Contextual word meaning and n-gram limitations
2. Words as vectors: The fundamental shift
3. Multilayer perceptrons: The building blocks of neural networks
4. Optimization through gradient descent
5. Neural language model architecture
6. Learning process for word embeddings
7. Word2Vec: Unsupervised embedding approaches
8. Semantic properties of word embeddings
9. Self-supervised learning as a bridge to modern LLMs

---

## Knowledge Point 1: Contextual Word Meaning and N-gram Limitations

**Probing Question**: Why might statistical counting of word sequences (n-grams) struggle with understanding the meaning of words in different contexts?

### Revisiting N-gram Limitations
- **Interactive Activity**: Try predicting what comes next in these sentences:
  * "The bank is by the river..."
  * "She deposited money in the bank..."
  * "Despite having similar symptoms, the treatment for these two conditions is completely..."

- **Discussion Points**:
  * Why does the word "bank" have different meanings in different contexts?
  * How would a simple n-gram model handle these ambiguities?
  * What information beyond simple word co-occurrence might be needed?

- **N-gram Challenge**: Create a simple bigram prediction for both contexts of "bank"
  * What patterns would your model from Session 1.2 capture?
  * Why does this approach fail to distinguish between different meanings?

- **Visual Reference**: [Optional] See Figure 1 in "Statistical Language Models Based on Neural Networks" (Mikolov, 2012) showing how meaning is lost in n-gram models.

### Understanding Check ✓
- What are two fundamental limitations of n-gram models when dealing with word meaning?
- Why is context crucial for understanding word meaning?
- How do these limitations affect practical applications like search or translation?

**Navigation Options**:
1. Proceed to Knowledge Point 2
2. Explore more examples of context-dependent meanings
3. See visualization of ambiguity problems
4. Request practice exercises

---

## Knowledge Point 2: Words as Vectors: The Fundamental Shift

**Probing Question**: What if we could represent words not just as discrete symbols, but as points in a multidimensional space where similar words are closer together?

### The Vector Representation Revolution
- **Key Concept**: Words as vectors of numbers (not just discrete symbols)
  * From symbolic to continuous representation
  * Similarity between words as distance in vector space
  * Why this is a fundamental shift in approach

- **Interactive Exploration**: Examine word vectors in the [TensorFlow Embedding Projector](https://projector.tensorflow.org/)
  * Observe how similar words cluster together
  * Explore relationships between words in vector space
  * See how context affects positioning

- **Conceptual Foundation**:
  * Each word represented as a vector of N numbers (e.g., 100-300 dimensions)
  * Each dimension potentially capturing a semantic or syntactic feature
  * Similar words have similar vectors (closer in space)

- **Visual Reference**: [Optional] Look at word embedding visualization from [The Illustrated Word2Vec](https://jalammar.github.io/illustrated-word2vec/) showing words in 2D space.

### Understanding Check ✓
- How does representing words as vectors differ fundamentally from n-gram approaches?
- Why might vector representations better capture meaning?
- What might these vector dimensions represent semantically?

**Navigation Options**:
1. Proceed to Knowledge Point 3
2. Explore embedding visualizations
3. See comparison of symbolic vs. vector representations
4. Request practice exercises

---

## Knowledge Point 5: Neural Language Model Architecture

**Probing Question**: How can neural networks use vector representations of words to make better predictions than n-gram models?

### Building a Neural Predictor
- **Architectural Overview**:
  * Input layer: Word vectors for context words
  * Hidden layer(s): Combine and transform vectors
  * Output layer: Probability distribution over vocabulary

- **Step-by-Step Walkthrough**:
  1. Convert input words to vector representations (embeddings)
  2. Process through MLP architecture
  3. Output probabilities for next word
  4. Calculate loss and update weights via gradient descent

- **Bengio's Original Architecture**:
  * Visual Reference: Figure 1 from the seminal paper [A Neural Probabilistic Language Model](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)
  * Historical importance and key innovations
  * How it addressed n-gram limitations

- **Simplified Code Example**:

```python
# Minimal neural language model
import torch
import torch.nn as nn

class SimpleNeuralLM(nn.Module):
    def __init__(self, vocab_size, embedding_dim, context_size):
        super(SimpleNeuralLM, self).__init__()
        # Embedding layer: converts word indices to vectors
        self.embeddings = nn.Embedding(vocab_size, embedding_dim)
        # Hidden layer
        self.linear1 = nn.Linear(context_size * embedding_dim, 100)
        # Output layer
        self.linear2 = nn.Linear(100, vocab_size)
    
    def forward(self, inputs):
        # Get embeddings for input words
        embeds = self.embeddings(inputs).view(inputs.shape[0], -1)
        # Process through neural network
        hidden = torch.tanh(self.linear1(embeds))
        output = self.linear2(hidden)
        return output
```

- **Interactive Tool**: [Optional] Explore a neural network visualization at [TensorFlow Playground](https://playground.tensorflow.org/)

### Understanding Check ✓
- What is the role of the embedding layer in a neural language model?
- How does this architecture differ from traditional n-gram models?
- Why can this approach better handle sparse data?

**Navigation Options**:
1. Proceed to Knowledge Point 6
2. Explore neural network architecture in more detail
3. See interactive neural network visualization
4. Request practice exercises

---

## Knowledge Point 6: Learning Process for Word Embeddings

**Probing Question**: How do neural language models learn meaningful vector representations just by trying to predict the next word?

### The Dual Learning Process
- **Key Insight**: The model learns two things simultaneously:
  1. How to predict the next word (the stated task)
  2. How to represent words as vectors (embeddings)

- **Learning Mechanism**:
  * Start with random vectors for each word
  * When prediction is wrong, update both:
    * The neural network weights (via gradient descent)
    * The word vectors themselves
  * Words in similar contexts get similar updates

- **Visual Exploration**: Watch this process unfold in the [Word Vectors Visualization with t-SNE](https://www.kaggle.com/code/jeffd23/visualizing-word-vectors-with-t-sne)

- **Simplified Learning Process**:
  1. Make prediction using current word vectors
  2. Calculate error (difference from correct answer)
  3. Update vectors using gradient descent
  4. Repeat millions of times across corpus

- **Intuitive Example**:
  * If "dog" and "cat" often appear in similar contexts...
  * Their vectors will receive similar updates
  * Eventually, their vectors become similar

- **Visual Reference**: [Optional] Examine gradient descent visualization from [Distill.pub](https://distill.pub/2017/momentum/) showing how parameters evolve during training

### Understanding Check ✓
- How do word vectors become meaningful through the training process?
- Why do words used in similar contexts end up with similar vectors?
- How is this different from simply counting co-occurrences?

**Navigation Options**:
1. Proceed to Knowledge Point 7
2. Explore the learning process in more detail
3. See animation of embedding learning
4. Request practice exercises

---

## Knowledge Point 7: Word2Vec: Unsupervised Embedding Approaches

**Probing Question**: Could we learn word representations without the task of predicting the next word, focusing directly on capturing relationships between words?

### Beyond Language Modeling
- **Word2Vec Approaches**:
  * **Skip-gram**: Predict context words from center word
  * **CBOW (Continuous Bag of Words)**: Predict center word from context words
  * Key difference from neural language models: no next-word prediction

- **Interactive Comparison**:
  * Visual Exploration: [Interactive Word2Vec Visualization (WEVI)](https://ronxin.github.io/wevi/)
  * Compare supervised (language model) vs. unsupervised (Word2Vec) approaches

- **Advantages of Direct Embedding Learning**:
  * Faster training
  * Focus on relationships rather than prediction
  * Captures different types of semantic relationships

- **Simplified Code Example**:

```python
# Simplified Word2Vec model (Skip-gram)
class SimpleWord2Vec(nn.Module):
    def __init__(self, vocab_size, embedding_dim):
        super(SimpleWord2Vec, self).__init__()
        # Embeddings for center word
        self.center_embeddings = nn.Embedding(vocab_size, embedding_dim)
        # Embeddings for context words 
        self.context_embeddings = nn.Embedding(vocab_size, embedding_dim)
    
    def forward(self, center, context):
        # Get embeddings
        center_embed = self.center_embeddings(center)
        context_embed = self.context_embeddings(context)
        # Compute similarity score
        score = torch.sum(center_embed * context_embed, dim=1)
        return torch.sigmoid(score)
```

- **Visual Reference**: [Optional] Examine the architecture diagram from the original [Word2Vec paper](https://arxiv.org/pdf/1301.3781.pdf)

### Understanding Check ✓
- What's the key difference between Word2Vec and neural language models?
- How does the Skip-gram model differ from CBOW?
- Why might we prefer direct embedding learning over language modeling?

**Navigation Options**:
1. Proceed to Knowledge Point 8
2. Explore Word2Vec in more detail
3. See comparison of embedding approaches
4. Request practice exercises

---

## Knowledge Point 8: Semantic Properties of Word Embeddings

**Probing Question**: What kinds of meaning and relationships can be captured in these word vectors, and how can we use them?

### Exploring Vector Properties
- **Vector Arithmetic**: Discover semantic operations in embedding space
  * king - man + woman = ? (queen)
  * paris - france + italy = ? (rome)
  * Visual Demonstration: [Embedding Projector Vector Math](https://projector.tensorflow.org/)

- **Types of Relationships Captured**:
  * Semantic similarity (dog ~ puppy)
  * Analogical relationships (king:queen::man:woman)
  * Hierarchical relationships (animal > dog > poodle)
  * Conceptual properties (capitals, genders, tenses)

- **Practical Applications**:
  * Document classification
  * Information retrieval
  * Machine translation
  * Named entity recognition

- **Limitations Discussion**:
  * Words with multiple meanings (polysemy)
  * Rare words and out-of-vocabulary words
  * Cultural and historical biases

- **Interactive Exploration**: Try semantic operations in [TensorFlow Embedding Projector](https://projector.tensorflow.org/)

### Understanding Check ✓
- How can vector arithmetic reveal semantic relationships?
- What types of relationships do embeddings capture well? Poorly?
- How might these properties be useful in practical applications?

**Navigation Options**:
1. Proceed to Knowledge Point 9
2. Explore more semantic properties
3. See examples of embedding applications
4. Request practice exercises

---

## Knowledge Point 9: Self-Supervised Learning as a Bridge to Modern LLMs

**Probing Question**: How do word embeddings and neural language models connect to modern large language models like the one you're interacting with now?

### Evolution of Language Models
- **Key Concept**: Self-supervised learning
  * The text itself provides its own supervision signal
  * No human labels needed
  * Scale enables emergent capabilities

- **Evolution Path**:
  1. N-gram models (statistical, fixed context)
  2. Neural language models with word embeddings (fixed context)
  3. Recurrent neural networks (variable context)
  4. Transformer models (parallel processing with attention)

- **Conceptual Advances**:
  * From static to contextual embeddings
  * From limited to extended context windows
  * From separate embeddings to end-to-end learning

- **Visual Reference**: [Optional] Look at the evolution diagram from [The Illustrated Transformer](https://jalammar.github.io/illustrated-transformer/)

- **Bridge to the Future**:
  * Contextual word representations (words have different vectors in different contexts)
  * Attention mechanisms (dynamic context weighting)
  * Scaling relationships (capabilities emerge with size)

### Understanding Check ✓
- How has the approach to word representation evolved from static embeddings to modern LLMs?
- Why is self-supervision so important for language model development?
- What fundamental limitations of traditional embeddings do modern LLMs address?

**Navigation Options**:
1. Proceed to Reflection
2. Explore self-supervised learning in more detail
3. See visualization of model evolution
4. Request practice exercises

---

## Reflection and Bridge to Next Sessions

**Integration Activity**: Summarize the progression from n-gram models to neural language models to word embeddings, highlighting how each approach builds on and improves upon the previous one.

**Key Questions to Consider**:
- How did the shift from discrete symbols to continuous vectors fundamentally change language modeling?
- What do you find most surprising about how word meanings can be captured in vector space?
- How might these approaches be further improved in future models?
- What limitations might still exist even with the most advanced embedding approaches?

**Next Steps Preview**: In the next sessions, we'll explore how modern transformer architectures take these concepts even further, using mechanisms like attention to create contextual representations that change based on the surrounding words.

---

## Additional Resources

### Interactive Tools and Visualizations
- [TensorFlow Embedding Projector](https://projector.tensorflow.org/) - Interactive exploration of word embeddings
- [The Illustrated Word2Vec](https://jalammar.github.io/illustrated-word2vec/) - Visual explanation of Word2Vec
- [Interactive Word2Vec Visualization (WEVI)](https://ronxin.github.io/wevi/) - Step-by-step visualization of Word2Vec training
- [TensorFlow Playground](https://playground.tensorflow.org/) - Interactive neural network visualization
- [UCLA Gradient Descent Visualizer](https://uclaacm.github.io/gradient-descent-visualiser/) - Visual tool showing how gradient descent works
- [Interactive Multilayer Perceptron Demo](https://chokkan.github.io/deeplearning/demo-mlp.html) - Visualization of MLP behavior with parameter adjustment
- [Google's Gradient Descent Visualization](https://developers.google.com/machine-learning/crash-course/linear-regression/gradient-descent) - Shows how loss decreases through training
- [Distill.pub Momentum Visualization](https://distill.pub/2017/momentum/) - Interactive visualization of gradient descent optimization

### Code Implementations
- [Gensim Word2Vec Tutorial](https://radimrehurek.com/gensim/models/word2vec.html) - Practical implementation of Word2Vec
- [PyTorch Word Embeddings Tutorial](https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html) - Building embeddings in PyTorch
- [TensorFlow Embedding Tutorial](https://www.tensorflow.org/tutorials/text/word_embeddings) - TensorFlow approach to word embeddings
- [MLXtend Multilayer Perceptron Guide](https://rasbt.github.io/mlxtend/user_guide/classifier/MultiLayerPerceptron/) - Clear implementation of MLP with visualization

### Research Papers
- Bengio, Y., et al. (2003). [A Neural Probabilistic Language Model](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)
- Mikolov, T., et al. (2013). [Efficient Estimation of Word Representations in Vector Space](https://arxiv.org/pdf/1301.3781.pdf)
- Pennington, J., et al. (2014). [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf)
- Rumelhart, D.E., et al. (1986). [Learning representations by back-propagating errors](https://www.nature.com/articles/323533a0) - Foundational paper on backpropagation
