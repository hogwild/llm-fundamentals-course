# Session 1.3: From N-grams to Neural Representations

## Primary Question
How can neural approaches represent words as mathematical objects that capture meaning, addressing the fundamental limitations of n-gram models?

Through exploring vector representations and neural network basics, you'll discover how language can be transformed into forms that machines can process to find deeper patterns than simple word counting.

## Knowledge Points Overview
This session covers the following key knowledge points:
1. Contextual word meaning and n-gram limitations revisited
2. Words as vectors: The fundamental shift
3. Neural network basics for language processing
4. Input representation in neural language models

---

## Knowledge Point 1: Contextual Word Meaning and N-gram Limitations Revisited

**Probing Question**: What specific limitations of n-gram models prevent them from capturing the meaning of words in different contexts?

### Core Concepts (For Everyone)

#### Beyond Statistical Counting
- **Interactive Activity**: Analyze these sentences:
  * "The bank is by the river."
  * "I deposited money in the bank."
  * "The bank approved my loan."
  
- **Discussion Points**:
  * How does the meaning of "bank" change in each sentence?
  * How would an n-gram model struggle to understand these different meanings?
  * What would a more powerful approach need to capture?

- **Everyday Analogy**: Think about how we understand meaning through context. The word "bright" means something different when describing a student, a light, or a color. We naturally understand these differences because we consider the whole context, not just the previous word or two. Traditional n-gram models can't do this - they only see a small window of words without understanding how meaning changes in different situations.

- **Core Limitation**: N-grams treat words as discrete symbols without inherent meaning
  * Words are either identical or completely different
  * No concept of similarity or relatedness
  * No way to generalize across similar contexts

### Understanding Check ✓
- What are two fundamental limitations of n-gram models when dealing with word meaning?
- Why does treating words as completely separate symbols create problems?
- How do humans understand different meanings of the same word in different contexts?

---

## Knowledge Point 2: Words as Vectors: The Fundamental Shift

**Probing Question**: What if words could be represented as points in a multidimensional space, where similar words are closer together?

### Core Concepts (For Everyone)

#### The Vector Representation Revolution
- **Everyday Analogy**: Imagine a library where books are placed on shelves based on their topics. Similar books are close together - cooking books near other cooking books, science fiction near other science fiction. This arrangement lets you find related books easily. Vector representations do something similar for words - they arrange words in an invisible "meaning space" so that words with similar meanings are neighbors.

- **Key Insight**: Instead of treating words as separate, unrelated symbols, we can represent them as points in space
  * Each word becomes a list of numbers (its coordinates in this space)
  * Words with similar meanings have similar coordinates
  * We can measure how similar words are by how close they are in this space

- **Interactive Exploration**: Visualize a simple 2D word map
  * Words like "king" and "queen" would be close to each other
  * Words like "dog" and "cat" would be near each other
  * Words like "cold" would be far from "hot"

![Word Vector Space](/api/placeholder/600/300)

- **Why This Matters**: This approach allows us to:
  * Capture relationships between words 
  * Understand similarities and differences
  * Make better predictions by generalizing to words with similar meanings

### Hands-On Implementation (For CS Students)

Let's implement a simple demonstration of word vectors using PyTorch:

```python
import torch
import numpy as np
import matplotlib.pyplot as plt

# Create a simple embedding layer
vocab_size = 10  # Simplified vocabulary
embedding_dim = 2  # 2D for easy visualization
embedding = torch.nn.Embedding(vocab_size, embedding_dim)

# Create a simple vocabulary for demonstration
vocabulary = ["king", "queen", "man", "woman", "prince", "princess", "boy", "girl", "uncle", "aunt"]
word_to_idx = {word: idx for idx, word in enumerate(vocabulary)}

# Get the vector for each word
word_vectors = {}
for word in vocabulary:
    idx = word_to_idx[word]
    # Convert to tensor and get embedding
    idx_tensor = torch.tensor([idx])
    vector = embedding(idx_tensor).detach().numpy()[0]
    word_vectors[word] = vector

# Plot the vectors in 2D space
plt.figure(figsize=(10, 8))
for word, vector in word_vectors.items():
    x, y = vector
    plt.scatter(x, y)
    plt.text(x+0.01, y+0.01, word, fontsize=12)

plt.title("Word Vectors in 2D Space")
plt.xlabel("Dimension 1")
plt.ylabel("Dimension 2")
plt.grid(True)
plt.axis('equal')
plt.show()

# Calculate similarities between words
def cosine_similarity(vec1, vec2):
    dot = np.dot(vec1, vec2)
    norm1 = np.linalg.norm(vec1)
    norm2 = np.linalg.norm(vec2)
    return dot / (norm1 * norm2)

# Check similarities between some word pairs
word_pairs = [("king", "queen"), ("king", "man"), ("king", "boy"), ("woman", "girl")]
for word1, word2 in word_pairs:
    sim = cosine_similarity(word_vectors[word1], word_vectors[word2])
    print(f"Similarity between '{word1}' and '{word2}': {sim:.3f}")
```

Note: This example creates random embeddings. In practice, these vectors would be learned from data to capture meaningful relationships.

### Advanced Theory (For the Curious)

#### Distributed Representation Theory

The theoretical foundation for word vectors comes from the concept of "distributed representations" first proposed by Geoffrey Hinton. In this framework:

- Each concept is represented by a pattern of activity across many units
- Each unit participates in representing many different concepts
- This creates an efficient encoding that captures semantic relationships

Unlike one-hot encodings (where each word activates exactly one unit), distributed representations allow the model to generalize across similar patterns. This is analogous to how the brain represents concepts using patterns of neural activations rather than single "grandmother cells."

When we represent words as vectors in a continuous space, we're creating a semantic topology where:
- Distance corresponds to semantic similarity
- Directions can represent relationships
- Regions can represent conceptual categories

This geometric interpretation of meaning allows us to perform "word algebra" operations like king - man + woman = queen, which demonstrate the model's understanding of semantic relationships.

### Understanding Check ✓
- What is a word vector, in your own words?
- How does representing words as vectors differ fundamentally from n-gram approaches?
- Why might vector representations better capture word meaning and relationships?

---

## Knowledge Point 3: Neural Network Basics for Language Processing

**Probing Question**: What are neural networks, and how can they help us process language in ways that overcome n-gram limitations?

### Core Concepts (For Everyone)

#### Introduction to Neural Networks
- **Everyday Analogy**: Think of a neural network like a team of people passing notes. The first group receives information and writes notes highlighting important patterns. They pass these notes to the next group, who find higher-level patterns in those notes, and so on. At each step, the team learns which patterns are important by being told how well they did and adjusting their note-taking. Neural networks work similarly, passing information through layers that learn to recognize increasingly complex patterns.

- **Key Concept**: Neural networks learn patterns from examples rather than following explicit rules
  * They consist of layers of interconnected units
  * Each unit combines inputs and produces an output
  * The connections between units have adjustable strengths (weights)
  * The network learns by adjusting these weights based on examples

- **Why Neural Networks for Language**:
  * They can learn patterns directly from data
  * They can work with word vectors (continuous representations)
  * They can discover complex relationships between words
  * They improve with more examples

![Simple Neural Network](/api/placeholder/600/300)

### Hands-On Implementation (For CS Students)

Let's implement a simple neural network for language processing using PyTorch:

```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np

# Define a simple neural network
class SimpleNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(SimpleNN, self).__init__()
        # First layer (input to hidden)
        self.layer1 = nn.Linear(input_size, hidden_size)
        # Activation function
        self.activation = nn.ReLU()
        # Second layer (hidden to output)
        self.layer2 = nn.Linear(hidden_size, output_size)
    
    def forward(self, x):
        # Pass input through first layer
        x = self.layer1(x)
        # Apply activation function
        x = self.activation(x)
        # Pass through second layer
        x = self.layer2(x)
        return x

# Create a toy dataset (binary classification task)
# Pretend these are word vectors for different words
vectors = [
    [0.2, 0.8],  # Positive example 1
    [0.3, 0.7],  # Positive example 2
    [0.1, 0.9],  # Positive example 3
    [0.8, 0.2],  # Negative example 1
    [0.7, 0.3],  # Negative example 2
    [0.9, 0.1]   # Negative example 3
]
labels = [1, 1, 1, 0, 0, 0]  # 1 for positive, 0 for negative

# Convert to PyTorch tensors
X = torch.tensor(vectors, dtype=torch.float32)
y = torch.tensor(labels, dtype=torch.float32).view(-1, 1)

# Create and train the neural network
input_size = 2  # Dimension of our vectors
hidden_size = 3  # Size of hidden layer
output_size = 1  # Binary classification (1 output)

# Initialize the model
model = SimpleNN(input_size, hidden_size, output_size)

# Define loss function and optimizer
criterion = nn.BCEWithLogitsLoss()  # Binary cross entropy loss
optimizer = optim.SGD(model.parameters(), lr=0.1)

# Training loop
for epoch in range(1000):
    # Forward pass
    outputs = model(X)
    loss = criterion(outputs, y)
    
    # Backward pass and optimize
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
    
    if (epoch+1) % 100 == 0:
        print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')

# Test the model
with torch.no_grad():
    test_vectors = torch.tensor([
        [0.25, 0.75],  # Should be positive
        [0.85, 0.15]   # Should be negative
    ], dtype=torch.float32)
    
    predictions = torch.sigmoid(model(test_vectors))
    
    for i, pred in enumerate(predictions):
        print(f"Vector {test_vectors[i]}: Prediction = {pred.item():.3f}, Class = {'Positive' if pred > 0.5 else 'Negative'}")
```

### Advanced Theory (For the Curious)

#### Universal Approximation Theorem

The mathematical power of neural networks comes from the Universal Approximation Theorem, which states that a feedforward neural network with a single hidden layer containing a finite number of neurons can approximate any continuous function on compact subsets of ℝⁿ, under mild assumptions on the activation function.

For language processing, this means neural networks can theoretically learn any mapping from input word representations to output predictions, given sufficient capacity and training data.

The learning process mathematically involves:

1. **Forward Propagation**: Computing predictions
   $$h = f(Wx + b)$$
   $$\hat{y} = g(Vh + c)$$
   Where W, V are weight matrices and b, c are bias vectors

2. **Error Calculation**: Measuring prediction error
   $$E = L(\hat{y}, y)$$
   Where L is a loss function like cross-entropy

3. **Backward Propagation**: Computing gradients and updating weights
   $$W \leftarrow W - \alpha \frac{\partial E}{\partial W}$$
   $$V \leftarrow V - \alpha \frac{\partial E}{\partial V}$$
   Where α is the learning rate

This process allows neural networks to adaptively learn complex patterns from data without explicit programming.

### Understanding Check ✓
- What is an artificial neural network, in simple terms?
- How do neural networks learn from examples?
- Why are neural networks better suited than simple statistical methods for processing word vectors?

---

## Knowledge Point 4: Input Representation in Neural Language Models

**Probing Question**: How do we actually convert words into a form that neural networks can process?

### Core Concepts (For Everyone)

#### Preparing Words for Neural Processing
- **Everyday Analogy**: Imagine translating between languages using a dictionary. First, you need to look up each word to find its translation. Similarly, to process words with neural networks, we first need to "look up" each word to find its vector representation. This lookup process converts words from their text form into numerical vectors that the network can process.

- **From Words to Numbers**:
  * We create a vocabulary of all the words we want to handle
  * We assign each word a unique ID number
  * We create a "lookup table" (embedding matrix) that maps IDs to vector representations
  * When processing text, we convert each word to its ID, then look up its vector

- **Processing and Prediction**:
  * After looking up vectors for input words, the neural network processes them
  * The network transforms these vectors through its layers
  * The final layer produces scores for each possible next word
  * These scores are converted to probabilities, and the word with highest probability is predicted

- **Why This Matters**:
  * Neural networks can only process numbers, not text directly
  * The embedding lookup gives each word a rich, meaningful representation
  * These representations capture similarities and relationships between words
  * Words with similar meanings get similar vectors

![Embedding Lookup Process](/api/placeholder/600/300)

### Hands-On Implementation (For CS Students)

Let's implement a simple embedding lookup and a complete forward pass in a neural language model:

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt

# Create a vocabulary and word-to-index mapping
vocabulary = ["the", "cat", "sat", "on", "mat", "dog", "chased", "mouse"]
word_to_idx = {word: idx for idx, word in enumerate(vocabulary)}

# Create a simple embedding layer
vocab_size = len(vocabulary)
embedding_dim = 3  # 3D vectors for each word
embedding = nn.Embedding(vocab_size, embedding_dim)

# Function to get embeddings for a sentence
def get_sentence_embeddings(sentence, embedding_layer, word_to_idx):
    # Tokenize (split into words)
    tokens = sentence.lower().split()
    # Convert words to indices
    indices = [word_to_idx.get(word, word_to_idx.get("the")) for word in tokens]  # Default to "the" if word not found
    # Convert to tensor
    indices_tensor = torch.tensor(indices)
    # Get embeddings
    embeddings = embedding_layer(indices_tensor)
    return tokens, embeddings

# Simple language model that makes predictions
class SimpleLanguageModel(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim):
        super(SimpleLanguageModel, self).__init__()
        # Embedding layer
        self.embedding = nn.Embedding(vocab_size, embedding_dim)
        # Hidden layer
        self.hidden = nn.Linear(embedding_dim, hidden_dim)
        self.activation = nn.ReLU()
        # Output layer
        self.output = nn.Linear(hidden_dim, vocab_size)
    
    def forward(self, x):
        # Get embeddings
        embeds = self.embedding(x)
        # Process through hidden layer
        hidden = self.activation(self.hidden(embeds))
        # Get output scores for each word in vocabulary
        logits = self.output(hidden)
        # Convert scores to probabilities using softmax
        probs = F.softmax(logits, dim=-1)
        return probs
    
    def predict_next_word(self, word_idx, word_to_idx, idx_to_word):
        # Convert word to tensor
        input_tensor = torch.tensor([word_idx])
        # Get probabilities
        probs = self.forward(input_tensor)
        # Get the most likely word
        _, predicted_idx = torch.max(probs, dim=1)
        # Convert back to word
        predicted_word = idx_to_word[predicted_idx.item()]
        return predicted_word, probs[0][predicted_idx].item()

# Create the model
idx_to_word = {idx: word for word, idx in word_to_idx.items()}
model = SimpleLanguageModel(vocab_size, embedding_dim=3, hidden_dim=5)

# Test the model on a few words
test_words = ["the", "cat", "dog"]
print("Model predictions (before training):")
for word in test_words:
    if word in word_to_idx:
        word_idx = word_to_idx[word]
        pred_word, confidence = model.predict_next_word(word_idx, word_to_idx, idx_to_word)
        print(f"After '{word}', model predicts '{pred_word}' with {confidence:.2f} confidence")
    else:
        print(f"Word '{word}' not in vocabulary")

# Notice that these predictions are random because the model is untrained!
print("\nIn the next session, we'll learn how to train this model using loss functions and optimization.")
```

### Advanced Theory (For the Curious)

#### Mathematical Representation of the Forward Pass

The forward pass in a neural language model can be represented mathematically as:

1. **Embedding Lookup**:
   $e_i = E[i]$
   Where $e_i$ is the embedding vector for the word with index $i$, and $E$ is the embedding matrix.

2. **Hidden Layer Computation**:
   $h = f(W_h e + b_h)$
   Where $W_h$ is the hidden layer weight matrix, $b_h$ is the bias vector, and $f$ is the activation function.

3. **Output Layer Computation**:
   $z = W_o h + b_o$
   Where $W_o$ is the output layer weight matrix and $b_o$ is the output bias vector, producing raw scores ($z$) for each word.

4. **Softmax Transformation**:
   $p(w_j) = \frac{e^{z_j}}{\sum_{k=1}^{V} e^{z_k}}$
   Where $p(w_j)$ is the probability of word $j$ being the next word, and $V$ is the vocabulary size.

5. **Word Selection**:
   $\hat{w} = \arg\max_j p(w_j)$
   The predicted word is the one with the highest probability.

This completes the forward pass of the model, but the model is initially random. In Session 1.4, we'll explore how to train this model through loss functions and optimization techniques.

### Understanding Check ✓
- What is the purpose of an embedding layer in a neural language model?
- How does the model convert its internal calculations to word predictions?
- Why do we need to convert words to vectors before neural network processing?

---

## Reflection and Bridge to Next Session

**Integration Activity**: Summarize the key differences between n-gram representations and neural representations of language, highlighting how the shift to vector representations addresses fundamental limitations.

**Key Questions to Consider**:
- How does the shift from discrete to continuous representation fundamentally change what's possible in language modeling?
- Why is it important for words to have "relationships" to each other in their representation?
- How do these new representation techniques set the stage for more advanced language models?

**Next Steps Preview**: In Session 1.4, we'll explore how neural language models actually make predictions and learn from data, building on the representation techniques covered here.

---

## Additional Resources

### Visual Resources
- Word vector spaces in 2D and 3D
- Embedding lookup process illustrations

### Key References
- Bengio, Y., et al. (2003). "A Neural Probabilistic Language Model." Journal of Machine Learning Research, 3, 1137-1155.
- Mikolov, T., et al. (2013). "Efficient Estimation of Word Representations in Vector Space." ICLR Workshop.
