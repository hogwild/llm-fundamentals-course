# Module 1: Foundations of Word Prediction and Embeddings

This module introduces the core concepts of language modeling through the lens of next-word prediction, which serves as the fundamental mechanism behind language models like the one you're interacting with now.

## Learning Objectives

By the end of this module, you should be able to:
- Understand how next-word prediction serves as the foundation for LLMs
- Experience how this simple concept enables complex capabilities like instruction following
- Build a simple n-gram predictor and understand its limitations
- Recognize why more sophisticated approaches like word embeddings are necessary

## Sessions

### [Session 1.1: Understanding Next-Word Prediction](./session-1.1/)
Explore how prediction serves as the foundational mechanism behind language models, and how this simple concept enables complex capabilities like answering questions and following instructions.

### [Session 1.2: Building Your First N-gram Predictor](./session-1.2/)
Experience firsthand the process of building a simple statistical language model, and discover through hands-on coding why more sophisticated approaches are needed.

## Key Concepts

- **Next-token prediction**: The core predictive task that underlies all language models
- **Instruction following**: How prediction enables AI to follow complex instructions
- **N-gram models**: Simple statistical approaches based on token frequency
- **Tokenization**: How text is broken into processable units
- **Training vs. Inference**: How models learn patterns and make predictions
- **Limitations of simple models**: Why we need more sophisticated approaches
